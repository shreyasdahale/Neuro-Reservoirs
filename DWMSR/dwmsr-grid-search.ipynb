{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:57.648569Z",
     "iopub.status.busy": "2025-06-28T18:12:57.648231Z",
     "iopub.status.idle": "2025-06-28T18:12:58.570908Z",
     "shell.execute_reply": "2025-06-28T18:12:58.569956Z",
     "shell.execute_reply.started": "2025-06-28T18:12:57.648487Z"
    },
    "papermill": {
     "duration": 1.654633,
     "end_time": "2025-06-02T12:10:39.772629",
     "exception": false,
     "start_time": "2025-06-02T12:10:38.117996",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "from skopt.utils import use_named_args\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.572294Z",
     "iopub.status.busy": "2025-06-28T18:12:58.571909Z",
     "iopub.status.idle": "2025-06-28T18:12:58.579072Z",
     "shell.execute_reply": "2025-06-28T18:12:58.578219Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.572273Z"
    },
    "papermill": {
     "duration": 0.012055,
     "end_time": "2025-06-02T12:10:39.787851",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.775796",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x*(rho - z) - y\n",
    "    dzdt = x*y - beta*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    num_steps = int(tmax / dt)\n",
    "    t_vals = np.linspace(0, tmax, num_steps+1)\n",
    "    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.580488Z",
     "iopub.status.busy": "2025-06-28T18:12:58.580165Z",
     "iopub.status.idle": "2025-06-28T18:12:58.602001Z",
     "shell.execute_reply": "2025-06-28T18:12:58.600978Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.580461Z"
    },
    "papermill": {
     "duration": 0.012858,
     "end_time": "2025-06-02T12:10:39.803577",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.790719",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n",
    "    \"\"\"\n",
    "    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (N, dim)\n",
    "        True trajectory over time.\n",
    "    y_pred : ndarray of shape (N, dim)\n",
    "        Model's predicted trajectory over time (closed-loop).\n",
    "    t_vals : ndarray of shape (N,)\n",
    "        Time values corresponding to the trajectory steps.\n",
    "    threshold : float, optional\n",
    "        The error threshold, default is 0.4 as in your snippet.\n",
    "    lambda_max : float, optional\n",
    "        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    T_VPT : float\n",
    "        Valid prediction time. The earliest time at which normalized error surpasses threshold\n",
    "        (or the last time if never surpassed).\n",
    "    T_lambda : float\n",
    "        Lyapunov time = 1 / lambda_max\n",
    "    ratio : float\n",
    "        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n",
    "    \"\"\"\n",
    "    # 1) Average of y_true\n",
    "    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n",
    "    \n",
    "    # 2) Time-averaged norm^2 of (y_true - y_mean)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n",
    "    \n",
    "    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n",
    "    diff = y_true - y_pred\n",
    "    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n",
    "    delta_gamma = err_sq / denom      # shape (N,)\n",
    "    \n",
    "    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n",
    "    idx_exceed = np.where(delta_gamma > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = t_vals[-1]\n",
    "    else:\n",
    "        T_VPT = t_vals[idx_exceed[0]]\n",
    "    \n",
    "    # 5) Compute T_lambda and ratio\n",
    "    T_lambda = 1.0 / lambda_max\n",
    "\n",
    "    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n",
    "\n",
    "    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n",
    "    ratio = T_VPT / T_lambda\n",
    "\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.604891Z",
     "iopub.status.busy": "2025-06-28T18:12:58.604348Z",
     "iopub.status.idle": "2025-06-28T18:12:58.623352Z",
     "shell.execute_reply": "2025-06-28T18:12:58.622405Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.604865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons for Teacher-forced Single-step Forecasting\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets)**2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * variance))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.625362Z",
     "iopub.status.busy": "2025-06-28T18:12:58.624436Z",
     "iopub.status.idle": "2025-06-28T18:12:58.641521Z",
     "shell.execute_reply": "2025-06-28T18:12:58.640528Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.625328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.642973Z",
     "iopub.status.busy": "2025-06-28T18:12:58.642690Z",
     "iopub.status.idle": "2025-06-28T18:12:58.675861Z",
     "shell.execute_reply": "2025-06-28T18:12:58.674817Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.642954Z"
    },
    "papermill": {
     "duration": 0.104497,
     "end_time": "2025-06-02T12:10:39.910805",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.806308",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Diffusion-Wavelet Multi-Scale Reservoir (DW-MSR)\n",
    "===============================================\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "Notation recap\n",
    "-----------------------------------------------------------------------\n",
    "* Base graph         G = (V,E), |V| = n0\n",
    "* Laplacian          L = D − A\n",
    "* Diffusion kernel   P_s = exp(− 2**s · τ0 · L)      for s = 0 … S\n",
    "* State vector       x_t = [x_t^{(0)} ; … ; x_t^{(S)}] ∈ R^{N}\n",
    "                      where   N = (S+1) · n0\n",
    "* Update (per scale) see equations in the methodology.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import expm\n",
    "from sklearn.linear_model import Ridge\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "#                          Helper functions                             #\n",
    "# --------------------------------------------------------------------- #\n",
    "def _build_laplacian(adj: sparse.spmatrix) -> sparse.spmatrix:\n",
    "    \"\"\"Combinatorial Laplacian L = D − A   (sparse CSR).\"\"\"\n",
    "    deg = np.asarray(adj.sum(axis=1)).ravel()\n",
    "    D = sparse.diags(deg, format=\"csr\")\n",
    "    return D - adj\n",
    "\n",
    "\n",
    "def _default_sequence(val: float, length: int) -> list[float]:\n",
    "    \"\"\"Repeat *val* 'length' times, return as list.\"\"\"\n",
    "    return [val] * length\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "#                     Diffusion-Wavelet Reservoir ESN                   #\n",
    "# --------------------------------------------------------------------- #\n",
    "class DiffusionWaveletReservoirESN:\n",
    "    \"\"\"\n",
    "    DW-MSR Echo-State Network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adj                   : scipy.sparse matrix (shape [n0,n0])\n",
    "        Symmetric, unweighted or weighted adjacency of the base graph.\n",
    "    num_scales            : int,   S ≥ 0   (# coarse levels)\n",
    "    tau0                  : float, base diffusion time   (τ₀)\n",
    "    betas                 : Sequence[float] length S,   funnel strengths β_s\n",
    "    alphas                : Sequence[float] length S+1, leak per scale α_s\n",
    "    input_scale           : float, scale of random W_in entries\n",
    "    ridge_alpha           : float, ℓ₂ penalty in ridge read-out\n",
    "    detail_features       : bool,  include Δ_s = x^{(s-1)}−x^{(s)} in Φ(x)?\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * Reservoir size  N = (S+1) * n0\n",
    "    * P_s are pre-computed once with sparse expm; they share sparsity of *adj*.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def __init__(\n",
    "        self,\n",
    "        adj: sparse.spmatrix,\n",
    "        num_scales: int = 2,\n",
    "        tau0: float = 0.1,\n",
    "        betas: Sequence[float] | None = None,\n",
    "        alphas: Sequence[float] | None = None,\n",
    "        input_scale: float = 0.5,\n",
    "        ridge_alpha: float = 1e-8,\n",
    "        detail_features: bool = True,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        # -------- basics -------------------------------------------------\n",
    "        if adj.shape[0] != adj.shape[1]:\n",
    "            raise ValueError(\"adjacency must be square\")\n",
    "        if not sparse.isspmatrix(adj):\n",
    "            adj = sparse.csr_matrix(adj)\n",
    "        self.n0 = adj.shape[0]\n",
    "        self.S = int(num_scales)\n",
    "        if self.S < 0:\n",
    "            raise ValueError(\"num_scales must be ≥ 0\")\n",
    "\n",
    "        self.N = (self.S + 1) * self.n0\n",
    "        self.tau0 = float(tau0)\n",
    "        self.input_scale = input_scale\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.detail_features = detail_features\n",
    "        self.seed = seed\n",
    "\n",
    "        # -------- leak & funnel parameters ------------------------------\n",
    "        self.betas = list(betas) if betas is not None else _default_sequence(0.5, self.S)\n",
    "        if len(self.betas) != self.S:\n",
    "            raise ValueError(\"betas must have length S\")\n",
    "\n",
    "        self.alphas = (\n",
    "            list(alphas)\n",
    "            if alphas is not None\n",
    "            else [0.5] + _default_sequence(1.0, self.S)  # finer quicker, coarse slow\n",
    "        )\n",
    "        if len(self.alphas) != self.S + 1:\n",
    "            raise ValueError(\"alphas must have length S+1\")\n",
    "\n",
    "        # -------- internal matrices -------------------------------------\n",
    "        self.Ps: list[sparse.spmatrix] = []\n",
    "        self.Vs: list[np.ndarray] = []  # just β_s I, store scalars\n",
    "        self._precompute_operators(adj)\n",
    "\n",
    "        self.W_in: np.ndarray | None = None      # set in fit_readout\n",
    "        self.W_out: np.ndarray | None = None\n",
    "\n",
    "        # state block list for convenience (each block length n0)\n",
    "        self.x_blocks = [np.zeros(self.n0, dtype=np.float32) for _ in range(self.S + 1)]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                    Pre-computation of diffusion kernels            #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _precompute_operators(self, adj: sparse.spmatrix):\n",
    "        \"\"\"Compute P_s and store funnel scalars β_s.\"\"\"\n",
    "        L = _build_laplacian(adj).tocsr()\n",
    "        # largest eigenvalue bound (Gershgorin): max row sum of |L|\n",
    "        lam_max = L.max(axis=1).toarray().ravel().max() + 1e-9\n",
    "        if self.tau0 * (2 ** self.S) * lam_max > 50:\n",
    "            print(\n",
    "                \"Warning: very large diffusion times may cause underflow in expm; \"\n",
    "                \"consider reducing tau0.\"\n",
    "            )\n",
    "\n",
    "        for s in range(self.S + 1):\n",
    "            tau_s = (2 ** s) * self.tau0\n",
    "            Ps = expm((-tau_s) * L)  # still sparse CSR\n",
    "            self.Ps.append(Ps)\n",
    "\n",
    "        self.Vs = self.betas  # just scalars\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                            Core update                             #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _single_step(self, u_t: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update all scales in causal order (fine → coarse) per eq. (1).\n",
    "        \"\"\"\n",
    "        new_blocks = []\n",
    "\n",
    "        # scale 0 (fine)\n",
    "        z0 = self.Ps[0].dot(self.x_blocks[0]) + self.W_in.dot(u_t)\n",
    "        x0_new = np.tanh(z0)\n",
    "        x0_next = (1.0 - self.alphas[0]) * self.x_blocks[0] + self.alphas[0] * x0_new\n",
    "        new_blocks.append(x0_next)\n",
    "\n",
    "        # coarser scales\n",
    "        for s in range(1, self.S + 1):\n",
    "            z = self.Ps[s].dot(self.x_blocks[s]) + self.Vs[s - 1] * new_blocks[s - 1]\n",
    "            xs_new = np.tanh(z)\n",
    "            xs_next = (1.0 - self.alphas[s]) * self.x_blocks[s] + self.alphas[s] * xs_new\n",
    "            new_blocks.append(xs_next)\n",
    "\n",
    "        # commit\n",
    "        self.x_blocks = new_blocks\n",
    "\n",
    "    def reset_state(self):\n",
    "        for blk in self.x_blocks:\n",
    "            blk.fill(0.0)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                        Read-out training                            #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def fit_readout(self, inputs: np.ndarray, targets: np.ndarray, discard: int = 100):\n",
    "        \"\"\"\n",
    "        Teacher-forcing to train W_out (ridge).\n",
    "\n",
    "        inputs  shape [T, d_in]\n",
    "        targets shape [T, d_out]\n",
    "        \"\"\"\n",
    "        T, d_in = inputs.shape\n",
    "        if T <= discard + 1:\n",
    "            raise ValueError(\"Not enough data for training\")\n",
    "\n",
    "        # random W_in\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        self.W_in = (\n",
    "            rng.uniform(-1.0, 1.0, size=(self.n0, d_in)) * self.input_scale\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        # roll through data\n",
    "        self.reset_state()\n",
    "        states, details = [], []\n",
    "        for t in range(T):\n",
    "            self._single_step(inputs[t])\n",
    "            if t >= discard:\n",
    "                flat_state = np.concatenate(self.x_blocks)\n",
    "                states.append(flat_state)\n",
    "\n",
    "                if self.detail_features and self.S > 0:\n",
    "                    # Δ_s = x^{(s-1)} − x^{(s)}\n",
    "                    delta_list = [\n",
    "                        self.x_blocks[s - 1] - self.x_blocks[s] for s in range(1, self.S + 1)\n",
    "                    ]\n",
    "                    details.append(np.concatenate(delta_list))\n",
    "\n",
    "        X_main = np.asarray(states, dtype=np.float32)  # [T-d, N]\n",
    "        Y = targets[discard:]\n",
    "\n",
    "        # feature map Φ\n",
    "        if self.detail_features and self.S > 0:\n",
    "            X_det = np.asarray(details, dtype=np.float32)  # same rows\n",
    "            feats = np.concatenate(\n",
    "                [X_main, X_det, np.ones((X_main.shape[0], 1), dtype=np.float32)], axis=1\n",
    "            )\n",
    "        else:\n",
    "            feats = np.concatenate(\n",
    "                [X_main, np.ones((X_main.shape[0], 1), dtype=np.float32)], axis=1\n",
    "            )\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(feats, Y)\n",
    "        self.W_out = reg.coef_.astype(np.float32)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #                       Autoregressive rollout                        #\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def predict_autoregressive(\n",
    "        self, initial_input: np.ndarray, n_steps: int\n",
    "    ) -> np.ndarray:\n",
    "        if self.W_out is None:\n",
    "            raise RuntimeError(\"fit_readout() must be called first\")\n",
    "\n",
    "        d_in = initial_input.shape[0]\n",
    "        d_out = self.W_out.shape[0]\n",
    "        preds = np.empty((n_steps, d_out), dtype=np.float32)\n",
    "\n",
    "        #self.reset_state()\n",
    "        current_u = initial_input.astype(np.float32).copy()\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            self._single_step(current_u)\n",
    "\n",
    "            flat_state = np.concatenate(self.x_blocks)\n",
    "            if self.detail_features and self.S > 0:\n",
    "                delta_list = [\n",
    "                    self.x_blocks[s - 1] - self.x_blocks[s] for s in range(1, self.S + 1)\n",
    "                ]\n",
    "                feat_vec = np.concatenate([flat_state, *delta_list, [1.0]])\n",
    "            else:\n",
    "                feat_vec = np.concatenate([flat_state, [1.0]])\n",
    "\n",
    "            y_t = (self.W_out @ feat_vec).astype(np.float32)\n",
    "            preds[t] = y_t\n",
    "            current_u = y_t[:d_in]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.677749Z",
     "iopub.status.busy": "2025-06-28T18:12:58.677206Z",
     "iopub.status.idle": "2025-06-28T18:12:58.700313Z",
     "shell.execute_reply": "2025-06-28T18:12:58.699339Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.677724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n0      = 100               # number of nodes\n",
    "p_edge  = 4 / n0            # expected degree ≈ 4\n",
    "\n",
    "rng  = np.random.default_rng(123)\n",
    "rows = rng.choice(n0, size=int(p_edge * n0 * (n0 - 1) // 2))\n",
    "cols = rng.choice(n0, size=rows.size)\n",
    "mask = rows != cols         # avoid self-loops\n",
    "rows, cols = rows[mask], cols[mask]\n",
    "\n",
    "# build upper triangle, then symmetrise\n",
    "adj = sparse.csr_matrix((np.ones_like(rows), (rows, cols)), shape=(n0, n0))\n",
    "adj = adj + adj.T\n",
    "adj[adj > 0] = 1.0          # make unweighted (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.702387Z",
     "iopub.status.busy": "2025-06-28T18:12:58.701415Z",
     "iopub.status.idle": "2025-06-28T18:12:58.719345Z",
     "shell.execute_reply": "2025-06-28T18:12:58.718200Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.702351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "betas = [list(pair) for pair in itertools.product(values, repeat=2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.720771Z",
     "iopub.status.busy": "2025-06-28T18:12:58.720382Z",
     "iopub.status.idle": "2025-06-28T18:12:58.737033Z",
     "shell.execute_reply": "2025-06-28T18:12:58.736050Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.720718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"tau0\":[0.01,0.035,0.06,0.085],\n",
    "    \"input_scale\":[0.2,0.45,0.7,0.9],\n",
    "    \"betas\":betas,\n",
    "    \"alphas\":[[0.3,0.1,0.1]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:12:58.738871Z",
     "iopub.status.busy": "2025-06-28T18:12:58.738462Z",
     "iopub.status.idle": "2025-06-28T18:12:58.755054Z",
     "shell.execute_reply": "2025-06-28T18:12:58.754057Z",
     "shell.execute_reply.started": "2025-06-28T18:12:58.738839Z"
    },
    "papermill": {
     "duration": 0.016065,
     "end_time": "2025-06-02T12:10:39.943473",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.927408",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    # Precompute param combinations\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    horizons = [200,400,600,800,1000]\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    # tqdm adds a progress bar for better visualization\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores = []\n",
    "        horizon_nrmse_all ={\n",
    "            200:[],\n",
    "            400:[],\n",
    "            600:[],\n",
    "            800:[],\n",
    "            1000:[]\n",
    "        }\n",
    "        \n",
    "        # Run all 20 seeds\n",
    "        for initial_state in [[1.0,1.0,1.0],[1.0,2.0,3.0],[2.0,1.5,4.0]]:\n",
    "            tmax = 250\n",
    "            dt   = 0.02\n",
    "            t_vals, rossler_traj = generate_lorenz_data(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "            \n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            rossler_traj = rossler_traj[washout:]\n",
    "            \n",
    "            # normalize\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(rossler_traj)\n",
    "            rossler_traj = scaler.transform(rossler_traj)\n",
    "            \n",
    "            T_data = len(rossler_traj)\n",
    "            for train_frac in [0.3,0.35,0.4]:\n",
    "                train_end = int(train_frac*(T_data-1))\n",
    "                train_input  = rossler_traj[:train_end]\n",
    "                train_target = rossler_traj[1:train_end+1]\n",
    "                test_input   = rossler_traj[train_end:-1]\n",
    "                test_target  = rossler_traj[train_end+1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "                for seed in np.arange(1,6):\n",
    "                    model = model_class(adj,**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "                    T_VPT_s, _, _ = compute_valid_prediction_time(test_target,preds,t_vals,0.4,0.9,dt)\n",
    "                    seed_scores.append(T_VPT_s)\n",
    "                    horizon_nrmse = evaluate_nrmse(preds, test_target, horizons)\n",
    "                    for h in horizons:\n",
    "                        horizon_nrmse_all[h].append(horizon_nrmse[h])\n",
    "        mean_score = float(np.mean(seed_scores))\n",
    "        std_dev    = float(np.std(seed_scores))\n",
    "        mean_nrmse_dict = {str(h): float(np.mean(horizon_nrmse_all[h])) for h in horizons}\n",
    "        std_nrmse_dict  = {str(h): float(np.std(horizon_nrmse_all[h]))  for h in horizons}\n",
    "        \n",
    "        # print(f\"Params: {params} → Avg T_VPT={mean_score:.3f}, \"\n",
    "        #       f\"Std Dev={std_dev:.3f} → {status}\")\n",
    "\n",
    "        results.append({\n",
    "            \"params\":      params,\n",
    "            \"seed_scores\": seed_scores,\n",
    "            \"mean_T_VPT\":  mean_score,\n",
    "            \"std_dev\":     std_dev,\n",
    "            \"mean_NRMSEs\": mean_nrmse_dict,\n",
    "            \"std_NRMSEs\": std_nrmse_dict\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T18:23:21.210011Z",
     "iopub.status.busy": "2025-06-28T18:23:21.209685Z",
     "iopub.status.idle": "2025-06-28T18:23:21.215508Z",
     "shell.execute_reply": "2025-06-28T18:23:21.214365Z",
     "shell.execute_reply.started": "2025-06-28T18:23:21.209990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1, 0.1, 0.1].json\n"
     ]
    }
   ],
   "source": [
    "output_path=str(grid[\"alphas\"][0])+\".json\"\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-28T18:23:48.648Z",
     "iopub.execute_input": "2025-06-28T18:23:45.392254Z",
     "iopub.status.busy": "2025-06-28T18:23:45.391904Z"
    },
    "papermill": {
     "duration": 0.025218,
     "end_time": "2025-06-02T15:55:28.978478",
     "exception": false,
     "start_time": "2025-06-02T15:55:28.95326",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Initial grid search for DWMSR with 400 combinations ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:   0%|          | 0/400 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "run_grid_search(DiffusionWaveletReservoirESN, grid, \"DWMSR\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13496.185164,
   "end_time": "2025-06-02T15:55:29.637237",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-02T12:10:33.452073",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
