{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d531c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1315979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from models.CR import CR3D\n",
    "from models.CRJ import CRJ3D\n",
    "from models.MCI import MCI3D\n",
    "from models.ESN import ESN3D\n",
    "from models.SAR import SAR3D\n",
    "from models.SparseESN import SparseESN3D\n",
    "from models.SW import SW3DSegregated, SW3DRandom\n",
    "from models.HFR import HFRRes3D\n",
    "from models.MC import MicrocolumnRes3D\n",
    "from models.SwirlGatedMultiCycle import MPPRN\n",
    "from models.DWMSR import DWMSR3D\n",
    "from metrics.metrics import mse_dimwise, nrmse_dimwise, compute_valid_prediction_time, compute_attractor_deviation, compute_relative_psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd0f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x*(rho - z) - y\n",
    "    dzdt = x*y - beta*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    num_steps = int(tmax / dt) + 1 # +1 to include t=0\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf0908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons\n",
    "    for teacher-forced single-step forecasting or autoregressive rollout.\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets) ** 2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        variance[variance == 0] = 1e-8  # avoid divide-by-zero\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * np.sum(variance)))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b169699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n",
    "    \"\"\"\n",
    "    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (N, dim)\n",
    "        True trajectory over time.\n",
    "    y_pred : ndarray of shape (N, dim)\n",
    "        Model's predicted trajectory over time (closed-loop).\n",
    "    t_vals : ndarray of shape (N,)\n",
    "        Time values corresponding to the trajectory steps.\n",
    "    threshold : float, optional\n",
    "        The error threshold, default is 0.4 as in your snippet.\n",
    "    lambda_max : float, optional\n",
    "        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    T_VPT : float\n",
    "        Valid prediction time. The earliest time at which normalized error surpasses threshold\n",
    "        (or the last time if never surpassed).\n",
    "    T_lambda : float\n",
    "        Lyapunov time = 1 / lambda_max\n",
    "    ratio : float\n",
    "        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n",
    "    \"\"\"\n",
    "    # 1) Average of y_true\n",
    "    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n",
    "    \n",
    "    # 2) Time-averaged norm^2 of (y_true - y_mean)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n",
    "    \n",
    "    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n",
    "    diff = y_true - y_pred\n",
    "    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n",
    "    delta_gamma = err_sq / denom      # shape (N,)\n",
    "    \n",
    "    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n",
    "    idx_exceed = np.where(delta_gamma > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = t_vals[-1]\n",
    "    else:\n",
    "        T_VPT = t_vals[idx_exceed[0]]\n",
    "    \n",
    "    # 5) Compute T_lambda and ratio\n",
    "    T_lambda = 1.0 / lambda_max\n",
    "\n",
    "    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n",
    "\n",
    "    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n",
    "    ratio = T_VPT / T_lambda\n",
    "\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ee15b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0c0201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 10501, from t=0..250 with dt=0.02.\n"
     ]
    }
   ],
   "source": [
    "initial_state  = [1.0, 1.0, 1.0]\n",
    "tmax = 250\n",
    "dt = 0.02\n",
    "t_vals, lorenz_traj = generate_lorenz_data(\n",
    "    initial_state=initial_state,\n",
    "    tmax=tmax,\n",
    "    dt=dt\n",
    ")\n",
    "\n",
    "washout = 2000\n",
    "t_vals = t_vals[washout:]\n",
    "lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(lorenz_traj)\n",
    "lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "T_data = len(lorenz_traj)\n",
    "train_frac = 0.8\n",
    "train_end = int(train_frac * (T_data - 1))\n",
    "train_input = lorenz_traj[:train_end]\n",
    "train_target = lorenz_traj[1:train_end + 1]\n",
    "test_input = lorenz_traj[train_end:-1]\n",
    "test_target = lorenz_traj[train_end + 1:]\n",
    "n_test_steps = len(test_input)\n",
    "initial_in = test_input[0]\n",
    "\n",
    "T_data = len(lorenz_traj)\n",
    "print(f\"Data length: {T_data}, from t=0..{tmax} with dt={dt}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304eabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sph_res = MPPRN(\n",
    "#     reservoir_size        = 300,      # total neurons  (must be multiple of 3)\n",
    "#     cycle_weight          = 0.97,      # r  weight on each edge of the three base cycles\n",
    "#     bridge_weight         = 0.2,     # s  weight on the three inter-cycle “bridge” edges\n",
    "#     input_scale           = 0.7,      # |W_in| multiplier   (was 0.5 by default)\n",
    "#     leak_rate             = 0.8,      # α  leaky-integrator coefficient (1.0 ⇒ no leak)\n",
    "#     ridge_alpha           = 1e-6,     #  penalty for the read-out ridge regression\n",
    "#     swirl_beta            = 12.5,      # β  controls the steepness of the swirl gate\n",
    "#     swirl_frequency       = None,     # ω  spatial frequency (defaults to 2π / (N/3))\n",
    "#     swirl_sigmoid         = True,     # if False, uses raw sin; if True, uses σ(β sin(…))\n",
    "#     seed                  = 42,       # seed \n",
    "#     use_polynomial_readout= True      # augment read-out with square andd bias\n",
    "#     )\n",
    "# sph_res.fit_readout(train_input, train_target, discard=1000)\n",
    "# sph_preds = sph_res.predict_autoregressive(initial_input=initial_in, n_steps=n_test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "212e7650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted Erdős–Rényi random graph\n",
    "from scipy import sparse\n",
    "\n",
    "n0      = 300               # number of nodes\n",
    "p_edge  = 4 / n0            # expected degree ≈ 4\n",
    "\n",
    "rng  = np.random.default_rng(123)\n",
    "rows = rng.choice(n0, size=int(p_edge * n0 * (n0 - 1) // 2))\n",
    "cols = rng.choice(n0, size=rows.size)\n",
    "mask = rows != cols         # avoid self-loops\n",
    "rows, cols = rows[mask], cols[mask]\n",
    "\n",
    "\n",
    "# build upper triangle, then symmetrise\n",
    "adj = sparse.csr_matrix((np.ones_like(rows), (rows, cols)), shape=(n0, n0))\n",
    "adj = adj + adj.T\n",
    "adj[adj > 0] = 1.0          # make unweighted (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581f9e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DWMSR3D.__init__() got an unexpected keyword argument 'frequency_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sph_res = \u001b[43mDWMSR3D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m        \u001b[49m\u001b[43madj\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# (required) sparse CSR adjacency\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_mode\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgold\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# 'random' or 'fixed'\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_scales\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# S – number of coarse levels\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtau0\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# base diffusion time τ₀\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# funnel strengths β₁…β_S\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# leak rates α₀…α_S  (len = S+1)\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_scale\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# scaling of random W_in\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mridge_alpha\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# ℓ₂ penalty in ridge read-out\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdetail_features\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# include Δ_s features?\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m123\u001b[39;49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# RNG seed for W_in & warnings\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m sph_res.fit_readout(train_input, train_target, discard=\u001b[32m1000\u001b[39m)\n\u001b[32m     14\u001b[39m sph_preds = sph_res.predict_autoregressive(initial_input=initial_in, n_steps=n_test_steps)\n",
      "\u001b[31mTypeError\u001b[39m: DWMSR3D.__init__() got an unexpected keyword argument 'frequency_mode'"
     ]
    }
   ],
   "source": [
    "sph_res = DWMSR3D(\n",
    "        adj              = adj,                 # (required) sparse CSR adjacency\n",
    "        num_scales       = 2,                   # S – number of coarse levels\n",
    "        tau0             = 0.01,                # base diffusion time τ₀\n",
    "        betas            = [0.5, 0.4],          # funnel strengths β₁…β_S\n",
    "        alphas           = [0.6, 0.7, 1.0],     # leak rates α₀…α_S  (len = S+1)\n",
    "        input_scale      = 0.1,                 # scaling of random W_in\n",
    "        ridge_alpha      = 1e-4,                # ℓ₂ penalty in ridge read-out\n",
    "        detail_features  = True,                # include Δ_s features?\n",
    "        seed             = 123                  # RNG seed for W_in & warnings\n",
    "    )\n",
    "sph_res.fit_readout(train_input, train_target, discard=1000)\n",
    "sph_preds = sph_res.predict_autoregressive(initial_input=initial_in, n_steps=n_test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6077d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, sph_ratio = compute_valid_prediction_time(\n",
    "    y_true=test_target,\n",
    "    y_pred=sph_preds,\n",
    "    t_vals=t_vals,\n",
    "    threshold=0.4,\n",
    "    lambda_max=0.9,\n",
    "    dt=dt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49e15353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.4660000000000015)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sph_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378f914e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPPRN NRMSE: {1000: np.float64(1.3461871583353822)}\n"
     ]
    }
   ],
   "source": [
    "sph_nrmse = evaluate_nrmse(sph_preds, test_target, horizons=[1000])\n",
    "print(f\"MPPRN NRMSE: {sph_nrmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a7e646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
