{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85917e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10dce793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a77d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x*(rho - z) - y\n",
    "    dzdt = x*y - beta*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    num_steps = int(tmax / dt) + 1 # +1 to include t=0\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af6e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n",
    "    \"\"\"\n",
    "    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (N, dim)\n",
    "        True trajectory over time.\n",
    "    y_pred : ndarray of shape (N, dim)\n",
    "        Model's predicted trajectory over time (closed-loop).\n",
    "    t_vals : ndarray of shape (N,)\n",
    "        Time values corresponding to the trajectory steps.\n",
    "    threshold : float, optional\n",
    "        The error threshold, default is 0.4 as in your snippet.\n",
    "    lambda_max : float, optional\n",
    "        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    T_VPT : float\n",
    "        Valid prediction time. The earliest time at which normalized error surpasses threshold\n",
    "        (or the last time if never surpassed).\n",
    "    T_lambda : float\n",
    "        Lyapunov time = 1 / lambda_max\n",
    "    ratio : float\n",
    "        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n",
    "    \"\"\"\n",
    "    # 1) Average of y_true\n",
    "    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n",
    "    \n",
    "    # 2) Time-averaged norm^2 of (y_true - y_mean)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n",
    "    \n",
    "    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n",
    "    diff = y_true - y_pred\n",
    "    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n",
    "    delta_gamma = err_sq / denom      # shape (N,)\n",
    "    \n",
    "    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n",
    "    idx_exceed = np.where(delta_gamma > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = t_vals[-1]\n",
    "    else:\n",
    "        T_VPT = t_vals[idx_exceed[0]]\n",
    "    \n",
    "    # 5) Compute T_lambda and ratio\n",
    "    T_lambda = 1.0 / lambda_max\n",
    "\n",
    "    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n",
    "\n",
    "    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n",
    "    ratio = T_VPT / T_lambda\n",
    "\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92024a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_vpt(name, preds, test_target, time_test, dt):\n",
    "    T_VPT, T_lambda, ratio = compute_valid_prediction_time(\n",
    "        test_target, preds, time_test, threshold=0.4, lambda_max=0.9, dt=dt\n",
    "    )\n",
    "    # print(f\"{name:20s} => T_VPT={T_VPT:.3f},  T_lambda={T_lambda:.3f}, ratio={ratio:.3f}\")\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a639e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse_dimwise(pred, truth):\n",
    "    length = min(len(pred), len(truth))\n",
    "    pred = pred[:length]\n",
    "    truth = truth[:length]\n",
    "    mse = np.mean((pred - truth) ** 2, axis=0)\n",
    "    std = np.std(truth, axis=0)\n",
    "    std[std == 0] = 1e-8\n",
    "    nrmse = np.sqrt(mse) / std\n",
    "    return np.linalg.norm(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f3c4c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 10501, from t=0..250 with dt=0.02.\n",
      "Train size: 8400  Test size: 2100\n"
     ]
    }
   ],
   "source": [
    "# 1) Generate Lorenz data\n",
    "tmax = 250\n",
    "dt   = 0.02\n",
    "t_vals, lorenz_traj = generate_lorenz_data(\n",
    "    initial_state=[1.0,1.0,1.0],\n",
    "    tmax=tmax,\n",
    "    dt=dt\n",
    ")\n",
    "\n",
    "washout = 2000\n",
    "t_vals = t_vals[washout:]\n",
    "lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(lorenz_traj)\n",
    "lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "T_data = len(lorenz_traj)\n",
    "print(f\"Data length: {T_data}, from t=0..{tmax} with dt={dt}.\")\n",
    "\n",
    "n_test_steps = 2100\n",
    "\n",
    "# train/test split\n",
    "train_frac = 0.8\n",
    "train_end = int(train_frac*(T_data-1))\n",
    "train_input  = lorenz_traj[:train_end]\n",
    "train_target = lorenz_traj[1:train_end+1]\n",
    "test_input   = lorenz_traj[train_end:train_end+n_test_steps]\n",
    "test_target  = lorenz_traj[train_end+1:train_end+n_test_steps+1]\n",
    "print(f\"Train size: {len(train_input)}  Test size: {len(test_input)}\")\n",
    "\n",
    "initial_in = test_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6ebb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_spectral_radius(W, target_radius=0.95):\n",
    "    \"\"\"\n",
    "    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(W)\n",
    "    radius = np.max(np.abs(eigvals))\n",
    "    if radius == 0:\n",
    "        return W\n",
    "    return (W / radius) * target_radius\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"\n",
    "    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n",
    "    We'll use this for both training and prediction.\n",
    "    \"\"\"\n",
    "    x_sq = x**2\n",
    "    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1\n",
    "\n",
    "class HFRRes3D:\n",
    "    \"\"\"\n",
    "    Hierarchical Fractal Reservoir (HFR) for 3D chaotic systems.\n",
    "    \n",
    "    This novel reservoir architecture partitions the chaotic attractor at multiple\n",
    "    hierarchical scales, combining them in a fractal-like adjacency structure.\n",
    "    The method is model-free, relying solely on the observed trajectory in R^3,\n",
    "    and does not require knowledge of any system parameters such as sigma, rho, beta\n",
    "    for Lorenz63. \n",
    "    \n",
    "    Key Idea:\n",
    "     1) Define multiple 'scales' of partition of the data's bounding region.\n",
    "     2) Each scale is subdivided into a certain number of cells (regions).\n",
    "     3) Each cell at level l has links to both:\n",
    "        - other cells at the same level (horizontal adjacency),\n",
    "        - 'child' cells at the finer level l+1 (vertical adjacency).\n",
    "     4) We gather all cells across levels => a multi-level fractal graph => adjacency => W.\n",
    "     5) We build a typical ESN from this adjacency, feed data with W_in, run leaky tanh updates,\n",
    "        then do a polynomial readout for 3D next-step prediction.\n",
    "\n",
    "    This approach is suitable for chaotic systems whose attractors often exhibit fractal\n",
    "    self-similarity, thus capturing multi-scale structures in a single reservoir.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_levels=3,             # number of hierarchical levels\n",
    "                 cells_per_level=None,   # list of number of cells at each level, e.g. [8, 32, 128]\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_levels       : int, number of hierarchical scales\n",
    "        cells_per_level: list[int], the number of partitions/cells at each level\n",
    "                         if None, we auto-generate e.g. 2^(level+2)\n",
    "        spectral_radius: final scaling for adjacency\n",
    "        input_scale    : random input scale W_in\n",
    "        leaking_rate   : ESN leaky alpha\n",
    "        ridge_alpha    : readout ridge penalty\n",
    "        seed           : random seed\n",
    "        \"\"\"\n",
    "        self.n_levels        = n_levels\n",
    "        self.cells_per_level = cells_per_level\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale     = input_scale\n",
    "        self.leaking_rate    = leaking_rate\n",
    "        self.ridge_alpha     = ridge_alpha\n",
    "        self.seed            = seed\n",
    "\n",
    "        if self.cells_per_level is None:\n",
    "            # default scheme e.g. 8, 16, 32 for 3 levels\n",
    "            self.cells_per_level = [8*(2**i) for i in range(n_levels)]\n",
    "\n",
    "        # We'll store adjacency W, input W_in, readout W_out, reservoir state x\n",
    "        self.W     = None\n",
    "        self.W_in  = None\n",
    "        self.W_out = None\n",
    "        self.x     = None\n",
    "        self.n_levels = len(self.cells_per_level)\n",
    "\n",
    "        # We'll define a total number of nodes = sum(cells_per_level)\n",
    "        self.n_nodes = sum(self.cells_per_level)\n",
    "\n",
    "    def _build_partitions(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build hierarchical partitions for each level.\n",
    "        We'll store the bounding box for data_3d, then for each level l in [0..n_levels-1]\n",
    "        run e.g. k-means with K = cells_per_level[l], each point gets a label => we track transitions.\n",
    "\n",
    "        Return: \n",
    "          partitions => list of arrays, partitions[l] => shape (N, ) cluster assignment in [0..cells_per_level[l]-1]\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        N = len(data_3d)\n",
    "        partitions = []\n",
    "\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            # cluster\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.seed+10*level, n_init='auto')\n",
    "            kmeans.fit(data_3d)\n",
    "            labels = kmeans.predict(data_3d)\n",
    "            partitions.append(labels)\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def _build_hierarchical_adjacency(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build a block adjacency with cross-level links, then scale spectral radius.\n",
    "        Steps:\n",
    "          1) Build partitions for each level => partitions[l] in [0..cells_per_level[l]-1]\n",
    "          2) For each level l, build a transition matrix T_l of shape (cells_per_level[l], cells_per_level[l]).\n",
    "          3) Link scale l to scale l+1 by figuring out which cluster i at scale l maps to which cluster j at scale l+1\n",
    "             for each sample t => link i-> j if data_3d[t] is in i at scale l and j at scale l+1.\n",
    "          4) Combine all transitions in one big adjacency W in R^(n_nodes x n_nodes).\n",
    "          5) row-normalize W => scale largest eigenvalue => spectral_radius\n",
    "        \"\"\"\n",
    "        partitions = self._build_partitions(data_3d)\n",
    "        N = len(data_3d)\n",
    "\n",
    "        # offsets for each level => to index big W\n",
    "        offsets = []\n",
    "        running = 0\n",
    "        for level in range(self.n_levels):\n",
    "            offsets.append(running)\n",
    "            running += self.cells_per_level[level]\n",
    "\n",
    "        # total nodes\n",
    "        n_tot = self.n_nodes\n",
    "        # initialize adjacency\n",
    "        A = np.zeros((n_tot, n_tot))\n",
    "\n",
    "        # 1) horizontal adjacency in each level\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            labels = partitions[level]\n",
    "            # T_l => shape (k, k)\n",
    "            T_l = np.zeros((k, k))\n",
    "            for t in range(N-1):\n",
    "                i = labels[t]\n",
    "                j = labels[t+1]\n",
    "                T_l[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = T_l.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            T_l /= row_sum\n",
    "            # place T_l into big A\n",
    "            off = offsets[level]\n",
    "            A[off:off+k, off:off+k] = T_l\n",
    "\n",
    "        # 2) vertical adjacency between scale l and l+1\n",
    "        for level in range(self.n_levels-1):\n",
    "            k_l   = self.cells_per_level[level]\n",
    "            k_lp1 = self.cells_per_level[level+1]\n",
    "            labels_l   = partitions[level]\n",
    "            labels_lp1 = partitions[level+1]\n",
    "            # we define adjacency from i in [0..k_l-1] to j in [0..k_lp1-1] if the same sample t belongs to i at level l and j at l+1\n",
    "            # Count how many times\n",
    "            Xvert1 = np.zeros((k_l, k_lp1))\n",
    "            for t in range(N):\n",
    "                i = labels_l[t]\n",
    "                j = labels_lp1[t]\n",
    "                Xvert1[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = Xvert1.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            Xvert = Xvert1/row_sum\n",
    "            # place in big A\n",
    "            off_l   = offsets[level]\n",
    "            off_lp1 = offsets[level+1]\n",
    "            A[off_l:off_l+k_l, off_lp1:off_lp1+k_lp1] = Xvert\n",
    "            # tentative idea, we could also define adjacency from l+1 -> l (parent link), if desired\n",
    "            # we do the same for the 'child -> parent' link or skip it if we only want forward adjacency\n",
    "            # For now, let's do symmetrical\n",
    "            Yvert = Xvert1.T\n",
    "            col_sum = Yvert.sum(axis=1, keepdims=True)\n",
    "            col_sum[col_sum==0.0] = 1.0\n",
    "            Yvert /= col_sum\n",
    "            A[off_lp1:off_lp1+k_lp1, off_l:off_l+k_l] = Yvert\n",
    "\n",
    "        # now we have a big adjacency => row normalize again, then scale spectral radius\n",
    "        row_sum = A.sum(axis=1, keepdims=True)\n",
    "        row_sum[row_sum==0.0] = 1.0\n",
    "        A /= row_sum\n",
    "\n",
    "        A = scale_spectral_radius(A, self.spectral_radius)\n",
    "        return A\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Main training routine:\n",
    "          1) Build hierarchical adjacency from fractal partition => self.W\n",
    "          2) define W_in => shape(n_nodes, 3)\n",
    "          3) teacher forcing => polynomial readout => solve => self.W_out\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Build adjacency\n",
    "        W_big = self._build_hierarchical_adjacency(train_input)\n",
    "        self.W = W_big\n",
    "\n",
    "        # define W_in => shape(n_nodes,3)\n",
    "        self.n_nodes = W_big.shape[0]\n",
    "        self.W_in = (np.random.rand(self.n_nodes,3)-0.5)*2.0*self.input_scale\n",
    "\n",
    "        # define reservoir state\n",
    "        self.x = np.zeros(self.n_nodes)\n",
    "\n",
    "        # gather states => teacher forcing => polynomial => readout\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]\n",
    "        X_list= []\n",
    "        for s in states_use:\n",
    "            X_list.append( augment_state_with_squares(s) )\n",
    "        X_aug= np.array(X_list)\n",
    "\n",
    "        reg= Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        self.W_out= reg.coef_\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Teacher forcing => feed real 3D => gather states => shape => [T-discard, n_nodes].\n",
    "        returns (states_after_discard, states_discarded).\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states= []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states= np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.x is not None:\n",
    "            self.x.fill(0.0)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        x(t+1)= (1-alpha)x(t)+ alpha tanh( W*x(t)+ W_in*u(t) ).\n",
    "        \"\"\"\n",
    "        alpha= self.leaking_rate\n",
    "        pre_acts= self.W@self.x + self.W_in@u\n",
    "        x_new= np.tanh(pre_acts)\n",
    "        self.x= (1.0- alpha)*self.x+ alpha*x_new\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        fully autonomous => feed last predicted => next input\n",
    "        \"\"\"\n",
    "        preds= []\n",
    "        #self.reset_state()\n",
    "        current_in= np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            big_x= augment_state_with_squares(self.x)\n",
    "            out= self.W_out@big_x\n",
    "            preds.append(out)\n",
    "            current_in= out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51f3d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (g) Hierarchical Fractal Reservoir (HFR)\n",
    "# hfr_res = HFRRes3D(\n",
    "#     n_levels=3,             # number of hierarchical scales\n",
    "#     cells_per_level=[50, 100, 150],   #[32, 32, 32, 32, 32, 32, 32, 32, 64],   # number of partitions at each scale (sum => total reservoir size) \n",
    "#                                                         # this config gave 11 VPT: 32, 32, 32, 32, 32, 32, 32, 32, 32, 64, 64, 64\n",
    "#                                                         # this config gave 10.62 VPT: 32, 32, 32, 32, 32, 32, 32, 32, 32, 32\n",
    "#                                                         # clubbing two of them let to a drop to 10.0 VPT: 32, 32, 32, 32, 32, 32, 32, 32, 64 \n",
    "#                                                         # but funny part is that 320 itself is giving 12.06 VPT\n",
    "#     spectral_radius=0.95,\n",
    "#     input_scale=0.2,\n",
    "#     leaking_rate=0.1,\n",
    "#     ridge_alpha=1e-8,\n",
    "#     seed=50\n",
    "# )\n",
    "# hfr_res.fit_readout(train_input, train_target, discard=100)\n",
    "\n",
    "# hfr_preds = hfr_res.predict_autoregressive(initial_in, n_test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7e1dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_hfr    = mse_dimwise(hfr_preds,    test_target)\n",
    "# nrmse_hfr    = nrmse_dimwise(hfr_preds,    test_target)\n",
    "# print(report_vpt(\"HFR\",         hfr_preds, test_target, t_vals[train_end:train_end+n_test_steps], dt))\n",
    "# adev_hfr = compute_attractor_deviation(hfr_preds, test_target)\n",
    "# rel_psd_hfr = compute_relative_psd(test_target, hfr_preds)\n",
    "# print(\"PSD          :\",  rel_psd_hfr)\n",
    "# print(\"ADev          :\",  adev_hfr)\n",
    "# print(\"mse      :\",  mse_hfr)\n",
    "# print(\"nrmse    :\",  nrmse_hfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "791e4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4) partial trajectory\n",
    "# plot_len = min(500, n_test_steps)\n",
    "# time_test = np.linspace(train_end*dt, (train_end+plot_len)*dt, plot_len)\n",
    "\n",
    "# plt.figure(figsize=(15,6))\n",
    "# plt.plot(time_test, test_target[:plot_len,0], label='True x', linestyle='--')\n",
    "# plt.plot(time_test, hfr_preds[:plot_len,0], label='Pred x', linestyle='-')\n",
    "\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Normalized x')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1e98830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# ax.plot(test_target[:plot_len,0], test_target[:plot_len,1], test_target[:plot_len,2], label='True', linestyle='--')\n",
    "# ax.plot(hfr_preds[:plot_len,0], hfr_preds[:plot_len,1], hfr_preds[:plot_len,2], label='HFR')\n",
    "# ax.set_title('3D Phase Plot')\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37442da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3\n",
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(10, 141, step):  # max a = 140 to leave room for b and c\n",
    "#     for b in range(a + step, 151, step):\n",
    "#         c = 300 - a - b\n",
    "#         if c > b and c % step == 0 and c <= 300:\n",
    "#             result.append([a, b, c])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b39e7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4\n",
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(10, 211, step):  # max a = 210\n",
    "#     for b in range(a + step, 221, step):\n",
    "#         for c in range(b + step, 231, step):\n",
    "#             d = 300 - a - b - c\n",
    "#             if d > c and d % step == 0 and d <= 300:\n",
    "#                 result.append([a, b, c, d])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb42af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5\n",
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(10, 221, step):  # max a = 220 so sum of 5 increasing values doesn't exceed 300\n",
    "#     for b in range(a + step, 231, step):\n",
    "#         for c in range(b + step, 241, step):\n",
    "#             for d in range(c + step, 251, step):\n",
    "#                 e = 300 - a - b - c - d\n",
    "#                 if e > d and e % step == 0 and e <= 300:\n",
    "#                     result.append([a, b, c, d, e])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46c39dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 6\n",
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(10, 181, step):  # Max a = 180\n",
    "#     for b in range(a + step, 191, step):\n",
    "#         for c in range(b + step, 201, step):\n",
    "#             for d in range(c + step, 211, step):\n",
    "#                 for e in range(d + step, 221, step):\n",
    "#                     f = 300 - a - b - c - d - e\n",
    "#                     if f > e and f % step == 0 and f <= 300:\n",
    "#                         result.append([a, b, c, d, e, f])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88f38ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7\n",
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(10, 151, step):  # max a = 150\n",
    "#     for b in range(a + step, 161, step):\n",
    "#         for c in range(b + step, 171, step):\n",
    "#             for d in range(c + step, 181, step):\n",
    "#                 for e in range(d + step, 191, step):\n",
    "#                     for f in range(e + step, 201, step):\n",
    "#                         g = 300 - a - b - c - d - e - f\n",
    "#                         if g > f and g % step == 0 and g <= 300:\n",
    "#                             result.append([a, b, c, d, e, f, g])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b243f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 8 (but not feasible for the case of step=10)\n",
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(5, 121, step):  # Conservative bounds to leave room\n",
    "#     for b in range(a + step, 131, step):\n",
    "#         for c in range(b + step, 141, step):\n",
    "#             for d in range(c + step, 151, step):\n",
    "#                 for e in range(d + step, 161, step):\n",
    "#                     for f in range(e + step, 171, step):\n",
    "#                         for g in range(f + step, 181, step):\n",
    "#                             h = 300 - a - b - c - d - e - f - g\n",
    "#                             if h > g and h % step == 0 and h <= 300:\n",
    "#                                 result.append([a, b, c, d, e, f, g, h])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f85f898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(5, 101, step):\n",
    "#     for b in range(a + step, 111, step):\n",
    "#         for c in range(b + step, 121, step):\n",
    "#             for d in range(c + step, 131, step):\n",
    "#                 for e in range(d + step, 141, step):\n",
    "#                     for f in range(e + step, 151, step):\n",
    "#                         for g in range(f + step, 161, step):\n",
    "#                             for h in range(g + step, 171, step):\n",
    "#                                 i = 300 - a - b - c - d - e - f - g - h\n",
    "#                                 if i > h and i % step == 0 and i <= 300:\n",
    "#                                     result.append([a, b, c, d, e, f, g, h, i])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61795860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cells_per_level_grid = {\n",
    "#     # \"cells_per_level\": [[300], [100, 200], [50, 100, 150], [40, 60, 80, 120], [25, 35, 50, 70, 120], [20, 30, 50, 60, 65, 75], [15, 25, 30, 40, 50, 60, 80], [5, 10, 25, 30, 40, 50, 60, 80]],\n",
    "#     # height=5\n",
    "#     # first 5\n",
    "#     # \"cells_per_level\": [[25, 50, 65, 70, 90], [25, 45, 55, 65, 110], [10, 20, 30, 60, 180], [10, 20, 25, 55, 190], [25, 30, 45, 95, 105]],\n",
    "#     # next 5\n",
    "#     # \"cells_per_level\": [[10, 15, 55, 60, 160], [10, 20, 55, 100, 115], [30, 35, 45, 80, 110], [25, 30, 40, 60, 145], [10, 35, 60, 75, 120]],\n",
    "#     # height=8\n",
    "#     # first 5\n",
    "#     # \"cells_per_level\": [[5, 15, 25, 30, 35, 40, 45, 105], [10, 15, 20, 30, 35, 55, 60, 75], [5, 15, 20, 25, 35, 55, 70, 75], [5, 10, 25, 30, 35, 60, 65, 70], [5, 10, 20, 30, 35, 50, 60, 90]],\n",
    "#     # next 5\n",
    "#     \"cells_per_level\": [[5, 15, 20, 25, 30, 35, 75, 95], [5, 10, 15, 35, 40, 45, 65, 85], [5, 10, 15, 20, 25, 30, 95, 100], [5, 15, 25, 35, 40, 45, 55, 80], [10, 15, 25, 30, 40, 45, 55, 80]],\n",
    "#     # \"cells_per_level\": [[5, 15, 25, 30, 35, 40, 45, 105], [10, 15, 20, 30, 35, 55, 60, 75], [5, 15, 20, 25, 35, 55, 70, 75], [5, 10, 25, 30, 35, 60, 65, 70], [5, 10, 20, 30, 35, 50, 60, 90]],\n",
    "#     # \"cells_per_level\": [[5, 15, 20, 25, 35, 55, 70, 75], [5, 10, 20, 30, 35, 50, 60, 90], [10, 15, 20, 30, 35, 55, 60, 75], [5, 10, 15, 20, 25, 30, 95, 100], [5, 10, 15, 20, 25, 30, 95, 100]],\n",
    "#     \"spectral_radius\": [0.9, 0.92, 0.94, 0.95, 0.96, 0.98, 0.99],\n",
    "#     \"input_scale\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "#     \"leaking_rate\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9],\n",
    "#     \"ridge_alpha\": [1e-8],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0598ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_per_level_grid = {\n",
    "    \"cells_per_level\": [[5, 15, 20, 25, 30, 35, 75, 95]],\n",
    "    \"spectral_radius\": [0.9],\n",
    "    \"input_scale\": [0.1],\n",
    "    \"leaking_rate\": [0.9],\n",
    "    \"ridge_alpha\": [1e-8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad4cbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    # Precompute param combinations\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    # tqdm adds a progress bar for better visualization\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores = []\n",
    "        seed_scores_nrmse = []\n",
    "        \n",
    "        for seed in range(1, 11):\n",
    "            model = model_class(**params, seed=seed)\n",
    "            model.fit_readout(train_input, train_target, discard=100)\n",
    "            preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "            T_VPT_s, _, _ = report_vpt(\"model\", preds,\n",
    "                                       test_target,\n",
    "                                       t_vals[train_end:train_end+n_test_steps],\n",
    "                                       dt)\n",
    "            nrmse = nrmse_dimwise(preds, test_target)\n",
    "            seed_scores.append(T_VPT_s)\n",
    "            seed_scores_nrmse.append(nrmse)\n",
    "\n",
    "        mean_score = float(np.mean(seed_scores))\n",
    "        std_dev    = float(np.std(seed_scores))\n",
    "        nrmse_mean = float(np.mean(seed_scores_nrmse))\n",
    "        nrmse_std  = float(np.std(seed_scores_nrmse))\n",
    "        \n",
    "        # print(f\"Params: {params} → Avg T_VPT={mean_score:.3f}, \"\n",
    "        #       f\"Std Dev={std_dev:.3f} → {status}\")\n",
    "\n",
    "        results.append({\n",
    "            \"params\":      params,\n",
    "            \"seed_scores\": seed_scores,\n",
    "            \"seed_scores_nrmse\": seed_scores_nrmse,\n",
    "            \"mean_T_VPT\":  mean_score,\n",
    "            \"std_dev\":     std_dev,\n",
    "            \"nrmse_mean\":  nrmse_mean,\n",
    "            \"nrmse_std\":   nrmse_std,\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "039e60f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Initial grid search for HFR with 1 combinations ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search: 100%|██████████| 1/1 [00:08<00:00,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All results saved to `grid_search_results.json`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hfr_results = run_grid_search(HFRRes3D, cells_per_level_grid, \"HFR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 10, 10, 10, 260], [10, 10, 10, 20, 250], [10, 10, 10, 30, 240], [10, 10, 10, 40, 230], [10, 10, 10, 50, 220], [10, 10, 10, 60, 210], [10, 10, 10, 70, 200], [10, 10, 10, 80, 190], [10, 10, 10, 90, 180], [10, 10, 10, 100, 170], [10, 10, 10, 110, 160], [10, 10, 10, 120, 150], [10, 10, 10, 130, 140], [10, 10, 20, 20, 240], [10, 10, 20, 30, 230], [10, 10, 20, 40, 220], [10, 10, 20, 50, 210], [10, 10, 20, 60, 200], [10, 10, 20, 70, 190], [10, 10, 20, 80, 180], [10, 10, 20, 90, 170], [10, 10, 20, 100, 160], [10, 10, 20, 110, 150], [10, 10, 20, 120, 140], [10, 10, 20, 130, 130], [10, 10, 30, 30, 220], [10, 10, 30, 40, 210], [10, 10, 30, 50, 200], [10, 10, 30, 60, 190], [10, 10, 30, 70, 180], [10, 10, 30, 80, 170], [10, 10, 30, 90, 160], [10, 10, 30, 100, 150], [10, 10, 30, 110, 140], [10, 10, 30, 120, 130], [10, 10, 40, 40, 200], [10, 10, 40, 50, 190], [10, 10, 40, 60, 180], [10, 10, 40, 70, 170], [10, 10, 40, 80, 160], [10, 10, 40, 90, 150], [10, 10, 40, 100, 140], [10, 10, 40, 110, 130], [10, 10, 40, 120, 120], [10, 10, 50, 50, 180], [10, 10, 50, 60, 170], [10, 10, 50, 70, 160], [10, 10, 50, 80, 150], [10, 10, 50, 90, 140], [10, 10, 50, 100, 130], [10, 10, 50, 110, 120], [10, 10, 60, 60, 160], [10, 10, 60, 70, 150], [10, 10, 60, 80, 140], [10, 10, 60, 90, 130], [10, 10, 60, 100, 120], [10, 10, 60, 110, 110], [10, 10, 70, 70, 140], [10, 10, 70, 80, 130], [10, 10, 70, 90, 120], [10, 10, 70, 100, 110], [10, 10, 80, 80, 120], [10, 10, 80, 90, 110], [10, 10, 80, 100, 100], [10, 10, 90, 90, 100], [10, 20, 20, 20, 230], [10, 20, 20, 30, 220], [10, 20, 20, 40, 210], [10, 20, 20, 50, 200], [10, 20, 20, 60, 190], [10, 20, 20, 70, 180], [10, 20, 20, 80, 170], [10, 20, 20, 90, 160], [10, 20, 20, 100, 150], [10, 20, 20, 110, 140], [10, 20, 20, 120, 130], [10, 20, 30, 30, 210], [10, 20, 30, 40, 200], [10, 20, 30, 50, 190], [10, 20, 30, 60, 180], [10, 20, 30, 70, 170], [10, 20, 30, 80, 160], [10, 20, 30, 90, 150], [10, 20, 30, 100, 140], [10, 20, 30, 110, 130], [10, 20, 30, 120, 120], [10, 20, 40, 40, 190], [10, 20, 40, 50, 180], [10, 20, 40, 60, 170], [10, 20, 40, 70, 160], [10, 20, 40, 80, 150], [10, 20, 40, 90, 140], [10, 20, 40, 100, 130], [10, 20, 40, 110, 120], [10, 20, 50, 50, 170], [10, 20, 50, 60, 160], [10, 20, 50, 70, 150], [10, 20, 50, 80, 140], [10, 20, 50, 90, 130], [10, 20, 50, 100, 120], [10, 20, 50, 110, 110], [10, 20, 60, 60, 150], [10, 20, 60, 70, 140], [10, 20, 60, 80, 130], [10, 20, 60, 90, 120], [10, 20, 60, 100, 110], [10, 20, 70, 70, 130], [10, 20, 70, 80, 120], [10, 20, 70, 90, 110], [10, 20, 70, 100, 100], [10, 20, 80, 80, 110], [10, 20, 80, 90, 100], [10, 20, 90, 90, 90], [10, 30, 30, 30, 200], [10, 30, 30, 40, 190], [10, 30, 30, 50, 180], [10, 30, 30, 60, 170], [10, 30, 30, 70, 160], [10, 30, 30, 80, 150], [10, 30, 30, 90, 140], [10, 30, 30, 100, 130], [10, 30, 30, 110, 120], [10, 30, 40, 40, 180], [10, 30, 40, 50, 170], [10, 30, 40, 60, 160], [10, 30, 40, 70, 150], [10, 30, 40, 80, 140], [10, 30, 40, 90, 130], [10, 30, 40, 100, 120], [10, 30, 40, 110, 110], [10, 30, 50, 50, 160], [10, 30, 50, 60, 150], [10, 30, 50, 70, 140], [10, 30, 50, 80, 130], [10, 30, 50, 90, 120], [10, 30, 50, 100, 110], [10, 30, 60, 60, 140], [10, 30, 60, 70, 130], [10, 30, 60, 80, 120], [10, 30, 60, 90, 110], [10, 30, 60, 100, 100], [10, 30, 70, 70, 120], [10, 30, 70, 80, 110], [10, 30, 70, 90, 100], [10, 30, 80, 80, 100], [10, 30, 80, 90, 90], [10, 40, 40, 40, 170], [10, 40, 40, 50, 160], [10, 40, 40, 60, 150], [10, 40, 40, 70, 140], [10, 40, 40, 80, 130], [10, 40, 40, 90, 120], [10, 40, 40, 100, 110], [10, 40, 50, 50, 150], [10, 40, 50, 60, 140], [10, 40, 50, 70, 130], [10, 40, 50, 80, 120], [10, 40, 50, 90, 110], [10, 40, 50, 100, 100], [10, 40, 60, 60, 130], [10, 40, 60, 70, 120], [10, 40, 60, 80, 110], [10, 40, 60, 90, 100], [10, 40, 70, 70, 110], [10, 40, 70, 80, 100], [10, 40, 70, 90, 90], [10, 40, 80, 80, 90], [10, 50, 50, 50, 140], [10, 50, 50, 60, 130], [10, 50, 50, 70, 120], [10, 50, 50, 80, 110], [10, 50, 50, 90, 100], [10, 50, 60, 60, 120], [10, 50, 60, 70, 110], [10, 50, 60, 80, 100], [10, 50, 60, 90, 90], [10, 50, 70, 70, 100], [10, 50, 70, 80, 90], [10, 50, 80, 80, 80], [10, 60, 60, 60, 110], [10, 60, 60, 70, 100], [10, 60, 60, 80, 90], [10, 60, 70, 70, 90], [10, 60, 70, 80, 80], [10, 70, 70, 70, 80], [20, 20, 20, 20, 220], [20, 20, 20, 30, 210], [20, 20, 20, 40, 200], [20, 20, 20, 50, 190], [20, 20, 20, 60, 180], [20, 20, 20, 70, 170], [20, 20, 20, 80, 160], [20, 20, 20, 90, 150], [20, 20, 20, 100, 140], [20, 20, 20, 110, 130], [20, 20, 20, 120, 120], [20, 20, 30, 30, 200], [20, 20, 30, 40, 190], [20, 20, 30, 50, 180], [20, 20, 30, 60, 170], [20, 20, 30, 70, 160], [20, 20, 30, 80, 150], [20, 20, 30, 90, 140], [20, 20, 30, 100, 130], [20, 20, 30, 110, 120], [20, 20, 40, 40, 180], [20, 20, 40, 50, 170], [20, 20, 40, 60, 160], [20, 20, 40, 70, 150], [20, 20, 40, 80, 140], [20, 20, 40, 90, 130], [20, 20, 40, 100, 120], [20, 20, 40, 110, 110], [20, 20, 50, 50, 160], [20, 20, 50, 60, 150], [20, 20, 50, 70, 140], [20, 20, 50, 80, 130], [20, 20, 50, 90, 120], [20, 20, 50, 100, 110], [20, 20, 60, 60, 140], [20, 20, 60, 70, 130], [20, 20, 60, 80, 120], [20, 20, 60, 90, 110], [20, 20, 60, 100, 100], [20, 20, 70, 70, 120], [20, 20, 70, 80, 110], [20, 20, 70, 90, 100], [20, 20, 80, 80, 100], [20, 20, 80, 90, 90], [20, 30, 30, 30, 190], [20, 30, 30, 40, 180], [20, 30, 30, 50, 170], [20, 30, 30, 60, 160], [20, 30, 30, 70, 150], [20, 30, 30, 80, 140], [20, 30, 30, 90, 130], [20, 30, 30, 100, 120], [20, 30, 30, 110, 110], [20, 30, 40, 40, 170], [20, 30, 40, 50, 160], [20, 30, 40, 60, 150], [20, 30, 40, 70, 140], [20, 30, 40, 80, 130], [20, 30, 40, 90, 120], [20, 30, 40, 100, 110], [20, 30, 50, 50, 150], [20, 30, 50, 60, 140], [20, 30, 50, 70, 130], [20, 30, 50, 80, 120], [20, 30, 50, 90, 110], [20, 30, 50, 100, 100], [20, 30, 60, 60, 130], [20, 30, 60, 70, 120], [20, 30, 60, 80, 110], [20, 30, 60, 90, 100], [20, 30, 70, 70, 110], [20, 30, 70, 80, 100], [20, 30, 70, 90, 90], [20, 30, 80, 80, 90], [20, 40, 40, 40, 160], [20, 40, 40, 50, 150], [20, 40, 40, 60, 140], [20, 40, 40, 70, 130], [20, 40, 40, 80, 120], [20, 40, 40, 90, 110], [20, 40, 40, 100, 100], [20, 40, 50, 50, 140], [20, 40, 50, 60, 130], [20, 40, 50, 70, 120], [20, 40, 50, 80, 110], [20, 40, 50, 90, 100], [20, 40, 60, 60, 120], [20, 40, 60, 70, 110], [20, 40, 60, 80, 100], [20, 40, 60, 90, 90], [20, 40, 70, 70, 100], [20, 40, 70, 80, 90], [20, 40, 80, 80, 80], [20, 50, 50, 50, 130], [20, 50, 50, 60, 120], [20, 50, 50, 70, 110], [20, 50, 50, 80, 100], [20, 50, 50, 90, 90], [20, 50, 60, 60, 110], [20, 50, 60, 70, 100], [20, 50, 60, 80, 90], [20, 50, 70, 70, 90], [20, 50, 70, 80, 80], [20, 60, 60, 60, 100], [20, 60, 60, 70, 90], [20, 60, 60, 80, 80], [20, 60, 70, 70, 80], [20, 70, 70, 70, 70], [30, 30, 30, 30, 180], [30, 30, 30, 40, 170], [30, 30, 30, 50, 160], [30, 30, 30, 60, 150], [30, 30, 30, 70, 140], [30, 30, 30, 80, 130], [30, 30, 30, 90, 120], [30, 30, 30, 100, 110], [30, 30, 40, 40, 160], [30, 30, 40, 50, 150], [30, 30, 40, 60, 140], [30, 30, 40, 70, 130], [30, 30, 40, 80, 120], [30, 30, 40, 90, 110], [30, 30, 40, 100, 100], [30, 30, 50, 50, 140], [30, 30, 50, 60, 130], [30, 30, 50, 70, 120], [30, 30, 50, 80, 110], [30, 30, 50, 90, 100], [30, 30, 60, 60, 120], [30, 30, 60, 70, 110], [30, 30, 60, 80, 100], [30, 30, 60, 90, 90], [30, 30, 70, 70, 100], [30, 30, 70, 80, 90], [30, 30, 80, 80, 80], [30, 40, 40, 40, 150], [30, 40, 40, 50, 140], [30, 40, 40, 60, 130], [30, 40, 40, 70, 120], [30, 40, 40, 80, 110], [30, 40, 40, 90, 100], [30, 40, 50, 50, 130], [30, 40, 50, 60, 120], [30, 40, 50, 70, 110], [30, 40, 50, 80, 100], [30, 40, 50, 90, 90], [30, 40, 60, 60, 110], [30, 40, 60, 70, 100], [30, 40, 60, 80, 90], [30, 40, 70, 70, 90], [30, 40, 70, 80, 80], [30, 50, 50, 50, 120], [30, 50, 50, 60, 110], [30, 50, 50, 70, 100], [30, 50, 50, 80, 90], [30, 50, 60, 60, 100], [30, 50, 60, 70, 90], [30, 50, 60, 80, 80], [30, 50, 70, 70, 80], [30, 60, 60, 60, 90], [30, 60, 60, 70, 80], [30, 60, 70, 70, 70], [40, 40, 40, 40, 140], [40, 40, 40, 50, 130], [40, 40, 40, 60, 120], [40, 40, 40, 70, 110], [40, 40, 40, 80, 100], [40, 40, 40, 90, 90], [40, 40, 50, 50, 120], [40, 40, 50, 60, 110], [40, 40, 50, 70, 100], [40, 40, 50, 80, 90], [40, 40, 60, 60, 100], [40, 40, 60, 70, 90], [40, 40, 60, 80, 80], [40, 40, 70, 70, 80], [40, 50, 50, 50, 110], [40, 50, 50, 60, 100], [40, 50, 50, 70, 90], [40, 50, 50, 80, 80], [40, 50, 60, 60, 90], [40, 50, 60, 70, 80], [40, 50, 70, 70, 70], [40, 60, 60, 60, 80], [40, 60, 60, 70, 70], [50, 50, 50, 50, 100], [50, 50, 50, 60, 90], [50, 50, 50, 70, 80], [50, 50, 60, 60, 80], [50, 50, 60, 70, 70], [50, 60, 60, 60, 70], [60, 60, 60, 60, 60]]\n",
      "Total unique combinations: 377\n"
     ]
    }
   ],
   "source": [
    "# # 5 allowing for duplicates\n",
    "# result = []\n",
    "# for a in range(10, 241, 10):  # a ≠ 0, a ≤ 240\n",
    "#     for b in range(a, (300 - a) // 4 + 1, 10):\n",
    "#         for c in range(b, (300 - a - b) // 3 + 1, 10):\n",
    "#             for d in range(c, (300 - a - b - c) // 2 + 1, 10):\n",
    "#                 e = 300 - a - b - c - d\n",
    "#                 if e >= d and e % 10 == 0:\n",
    "#                     result.append([a, b, c, d, e])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
