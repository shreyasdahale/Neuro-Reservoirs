{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5f7a6f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T12:10:38.123217Z",
     "iopub.status.busy": "2025-06-02T12:10:38.122783Z",
     "iopub.status.idle": "2025-06-02T12:10:39.770953Z",
     "shell.execute_reply": "2025-06-02T12:10:39.770123Z"
    },
    "papermill": {
     "duration": 1.654633,
     "end_time": "2025-06-02T12:10:39.772629",
     "exception": false,
     "start_time": "2025-06-02T12:10:38.117996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from __future__ import annotations\n",
    "from scipy.sparse.linalg import expm\n",
    "from sklearn.linear_model import Ridge\n",
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d4813d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:10:39.779847Z",
     "iopub.status.busy": "2025-06-02T12:10:39.779422Z",
     "iopub.status.idle": "2025-06-02T12:10:39.786327Z",
     "shell.execute_reply": "2025-06-02T12:10:39.785458Z"
    },
    "papermill": {
     "duration": 0.012055,
     "end_time": "2025-06-02T12:10:39.787851",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.775796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x*(rho - z) - y\n",
    "    dzdt = x*y - beta*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    num_steps = int(tmax / dt) + 1 # +1 to include t=0\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chen_deriv(state, t, a=35.0, b=3.0, c=28.0):\n",
    "    \"\"\"\n",
    "    Computes derivatives [dx/dt, dy/dt, dz/dt] for Chen system:\n",
    "      dx/dt = a*(y - x)\n",
    "      dy/dt = (c - a)*x + c*y - x*z\n",
    "      dz/dt = x*y - b*z\n",
    "    \"\"\"\n",
    "    x, y, z = state\n",
    "    dxdt = a*(y - x)\n",
    "    dydt = (c - a)*x + c*y - x*z\n",
    "    dzdt = x*y - b*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_chen_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=50.0,\n",
    "    dt=0.01,\n",
    "    a=35.0,\n",
    "    b=3.0,\n",
    "    c=28.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Integrates Chen's system from 'initial_state' up to time 'tmax' with step size 'dt'.\n",
    "    Returns:\n",
    "      t_vals: time array of length T\n",
    "      sol   : array shape [T, 3], the trajectory [x(t), y(t), z(t)]\n",
    "    \"\"\"\n",
    "    num_steps = int(tmax / dt)\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(chen_deriv, initial_state, t_vals, args=(a, b, c))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52925d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max = {\n",
    "        \"Mackey\": 0.006100,\n",
    "        \"Lorenz\": 0.905600,\n",
    "        \"Rossler\": 0.071400,\n",
    "        \"Chen\": 0.829600,\n",
    "        \"Chua\": 0.428400\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e369869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:10:39.794593Z",
     "iopub.status.busy": "2025-06-02T12:10:39.794263Z",
     "iopub.status.idle": "2025-06-02T12:10:39.802048Z",
     "shell.execute_reply": "2025-06-02T12:10:39.801063Z"
    },
    "papermill": {
     "duration": 0.012858,
     "end_time": "2025-06-02T12:10:39.803577",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.790719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n",
    "    \"\"\"\n",
    "    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (N, dim)\n",
    "        True trajectory over time.\n",
    "    y_pred : ndarray of shape (N, dim)\n",
    "        Model's predicted trajectory over time (closed-loop).\n",
    "    t_vals : ndarray of shape (N,)\n",
    "        Time values corresponding to the trajectory steps.\n",
    "    threshold : float, optional\n",
    "        The error threshold, default is 0.4 as in your snippet.\n",
    "    lambda_max : float, optional\n",
    "        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    T_VPT : float\n",
    "        Valid prediction time. The earliest time at which normalized error surpasses threshold\n",
    "        (or the last time if never surpassed).\n",
    "    T_lambda : float\n",
    "        Lyapunov time = 1 / lambda_max\n",
    "    ratio : float\n",
    "        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n",
    "    \"\"\"\n",
    "    # 1) Average of y_true\n",
    "    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n",
    "    \n",
    "    # 2) Time-averaged norm^2 of (y_true - y_mean)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n",
    "    \n",
    "    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n",
    "    diff = y_true - y_pred\n",
    "    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n",
    "    delta_gamma = err_sq / denom      # shape (N,)\n",
    "    \n",
    "    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n",
    "    idx_exceed = np.where(delta_gamma > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = t_vals[-1]\n",
    "    else:\n",
    "        T_VPT = t_vals[idx_exceed[0]]\n",
    "    \n",
    "    # 5) Compute T_lambda and ratio\n",
    "    T_lambda = 1.0 / lambda_max\n",
    "\n",
    "    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n",
    "\n",
    "    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n",
    "    ratio = T_VPT / T_lambda\n",
    "\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54001937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36590a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons for Teacher-forced Single-step Forecasting\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "\n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets)**2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        variance[variance == 0] = 1e-8 \n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * variance))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321d47c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:10:39.810566Z",
     "iopub.status.busy": "2025-06-02T12:10:39.810231Z",
     "iopub.status.idle": "2025-06-02T12:10:39.909131Z",
     "shell.execute_reply": "2025-06-02T12:10:39.908170Z"
    },
    "papermill": {
     "duration": 0.104497,
     "end_time": "2025-06-02T12:10:39.910805",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.806308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from numpy.linalg import eigvals\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def scale_spectral_radius(W, desired_radius=0.95):\n",
    "    \"\"\"Affine-scale W so that its spectral radius equals desired_radius.\"\"\"\n",
    "    eigs = eigvals(W)\n",
    "    radius = np.max(np.abs(eigs))\n",
    "    if radius == 0:\n",
    "        raise ValueError(\"Spectral radius of W is zero.\")\n",
    "    return W * (desired_radius / radius)\n",
    "\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"Return [x, x², 1] (same convention as in CycleReservoir3D).\"\"\"\n",
    "    return np.concatenate([x, x**2, [1.0]])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Hemispherically–Hierarchical Lobe Reservoir\n",
    "# ---------------------------------------------------------------------\n",
    "class HHLobeReservoir:\n",
    "    \"\"\"\n",
    "    Hemispherically-Hierarchical Lobe Reservoir (HH-LR)\n",
    "\n",
    "    • 8 anatomical modules  —  L/R × {F, P, T, O}\n",
    "    • Intra-module: Watts–Strogatz small-world graphs (ring + rewire)\n",
    "    • Intra-hemisphere shortcuts: distance-modulated (exp decay)\n",
    "    • Inter-hemisphere bridges: sparse homotopic callosal links\n",
    "    • Lobe cardinalities follow MRI volume ratio 4 : 3 : 2 : 1\n",
    "    \"\"\"\n",
    "\n",
    "    # bookkeeping ------------------------------------------------------\n",
    "    _LOBES  = ['F', 'P', 'T', 'O']\n",
    "    _HEMIS  = ['L', 'R']\n",
    "    _CENTROIDS = {                             # rough 2-D montage coords\n",
    "        ('L', 'F'): (-1.0,  1.0), ('L', 'P'): (-1.0,  0.0),\n",
    "        ('L', 'T'): (-1.0, -1.0), ('L', 'O'): (-1.0, -2.0),\n",
    "        ('R', 'F'): ( 1.0,  1.0), ('R', 'P'): ( 1.0,  0.0),\n",
    "        ('R', 'T'): ( 1.0, -1.0), ('R', 'O'): ( 1.0, -2.0),\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def __init__(self,\n",
    "                 reservoir_size=800,\n",
    "                 input_dim=32,\n",
    "                 spectral_radius=0.9,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 # small-world / shortcut hyper-parameters\n",
    "                 k_ring=8,\n",
    "                 p_rewire_frontal=0.30,\n",
    "                 p_rewire_other=0.10,\n",
    "                 P_lat=0.04,\n",
    "                 sigma=5.0,\n",
    "                 P_call=0.01,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        reservoir_size : total number of neurons (N)\n",
    "        input_dim      : dimension of u_t  (128 for EEG band-power features)\n",
    "        leaking_rate   : α in leaky-integrator update\n",
    "        \"\"\"\n",
    "        self.N          = reservoir_size\n",
    "        self.D_in       = input_dim\n",
    "        self.rho        = spectral_radius\n",
    "        self.in_scale   = input_scale\n",
    "        self.alpha      = leaking_rate\n",
    "        self.ridge_alpha= ridge_alpha\n",
    "        self.seed       = seed\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 1) allocate neurons:  per-hemisphere 4 : 3 : 2 : 1 (F:P:T:O)\n",
    "        # --------------------------------------------------------------\n",
    "        ratio = {'F': 4, 'P': 3, 'T': 2, 'O': 1}\n",
    "        total_w = sum(ratio.values())          # = 10\n",
    "        half_N  = self.N // 2                  # neurons per hemisphere\n",
    "\n",
    "        counts_per_hemi = {\n",
    "            l: int(half_N * ratio[l] / total_w) for l in self._LOBES\n",
    "        }\n",
    "\n",
    "        # distribute rounding residuals (largest fractional remainder first)\n",
    "        residual = half_N - sum(counts_per_hemi.values())\n",
    "        if residual > 0:\n",
    "            remainders = sorted(\n",
    "                self._LOBES,\n",
    "                key=lambda l: (half_N * ratio[l] / total_w) - counts_per_hemi[l],\n",
    "                reverse=True\n",
    "            )\n",
    "            for l in remainders[:residual]:\n",
    "                counts_per_hemi[l] += 1\n",
    "\n",
    "        # build slice table\n",
    "        self._module_slices = {}\n",
    "        idx0 = 0\n",
    "        for h in self._HEMIS:          # L then R\n",
    "            for l in self._LOBES:      # F, P, T, O\n",
    "                n = counts_per_hemi[l]\n",
    "                self._module_slices[(h, l)] = slice(idx0, idx0 + n)\n",
    "                idx0 += n\n",
    "        assert idx0 == self.N, \"Slice allocation error\"\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 2) construct adjacency matrix W according to HH-LR rules\n",
    "        # --------------------------------------------------------------\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        W = np.zeros((self.N, self.N), dtype=float)\n",
    "\n",
    "        # 2.1 intra-module small-world wiring\n",
    "        for (h, l), sl in self._module_slices.items():\n",
    "            n_mod = sl.stop - sl.start\n",
    "            k = min(k_ring, n_mod - 1)\n",
    "            p_rewire = p_rewire_frontal if l == 'F' else p_rewire_other\n",
    "\n",
    "            # ring lattice\n",
    "            for i_local in range(n_mod):\n",
    "                for m in range(1, k + 1):\n",
    "                    j_local = (i_local + m) % n_mod\n",
    "                    i_glob  = sl.start + i_local\n",
    "                    j_glob  = sl.start + j_local\n",
    "                    W[i_glob, j_glob] = rng.standard_normal()\n",
    "                    W[j_glob, i_glob] = rng.standard_normal()\n",
    "\n",
    "            # random rewiring\n",
    "            for i_local in range(n_mod):\n",
    "                for m in range(1, k + 1):\n",
    "                    if rng.random() < p_rewire:\n",
    "                        j_local_new = rng.integers(0, n_mod - 1)\n",
    "                        if j_local_new >= i_local:\n",
    "                            j_local_new += 1\n",
    "                        i_glob = sl.start + i_local\n",
    "                        j_glob_new = sl.start + j_local_new\n",
    "                        w_new = rng.standard_normal()\n",
    "                        W[i_glob, j_glob_new] = w_new\n",
    "                        W[j_glob_new, i_glob] = rng.standard_normal()\n",
    "\n",
    "        # 2.2 intra-hemisphere distance-weighted shortcuts\n",
    "        for h in self._HEMIS:\n",
    "            for l1 in self._LOBES:\n",
    "                for l2 in self._LOBES:\n",
    "                    if l1 == l2:\n",
    "                        continue\n",
    "                    sl1 = self._module_slices[(h, l1)]\n",
    "                    sl2 = self._module_slices[(h, l2)]\n",
    "                    d = np.linalg.norm(\n",
    "                        np.array(self._CENTROIDS[(h, l1)]) -\n",
    "                        np.array(self._CENTROIDS[(h, l2)])\n",
    "                    )\n",
    "                    p_edge = P_lat * np.exp(-d / sigma)\n",
    "                    for i in range(sl1.start, sl1.stop):\n",
    "                        mask = rng.random(sl2.stop - sl2.start) < p_edge\n",
    "                        js = np.nonzero(mask)[0] + sl2.start\n",
    "                        if js.size:\n",
    "                            W[i, js] = rng.standard_normal(size=js.size)\n",
    "                            W[js, i] = rng.standard_normal(size=js.size)\n",
    "\n",
    "        # 2.3 inter-hemisphere homotopic callosal links\n",
    "        for l in self._LOBES:\n",
    "            sl_L = self._module_slices[('L', l)]\n",
    "            sl_R = self._module_slices[('R', l)]\n",
    "            for i in range(sl_L.start, sl_L.stop):\n",
    "                mask = rng.random(sl_R.stop - sl_R.start) < P_call\n",
    "                js = np.nonzero(mask)[0] + sl_R.start\n",
    "                if js.size:\n",
    "                    W[i, js] = rng.standard_normal(size=js.size)\n",
    "                    W[js, i] = rng.standard_normal(size=js.size)\n",
    "\n",
    "        # 2.4 spectral scaling to enforce ESP\n",
    "        self.W = scale_spectral_radius(W, self.rho)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 3) random input weights\n",
    "        # --------------------------------------------------------------\n",
    "        rng = np.random.default_rng(self.seed + 1)\n",
    "        self.W_in = (rng.random((self.N, self.D_in)) - 0.5) * 2.0 * self.in_scale\n",
    "\n",
    "        # initialize state & read-out\n",
    "        self.x = np.zeros(self.N)\n",
    "        self.W_out = None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # ESN core methods --------------------------------------------------\n",
    "    # ------------------------------------------------------------------\n",
    "    def reset_state(self):\n",
    "        self.x.fill(0.0)\n",
    "\n",
    "    def _update(self, u):\n",
    "        pre = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.tanh(pre)\n",
    "        self.x = (1.0 - self.alpha) * self.x + self.alpha * x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for u in inputs:\n",
    "            self._update(u)\n",
    "            states.append(self.x.copy())\n",
    "        return np.array(states[discard:]), np.array(states[:discard])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # read-out ----------------------------------------------------------\n",
    "    # ------------------------------------------------------------------\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        states_use, _ = self.collect_states(train_input, discard)\n",
    "        targets_use = train_target[discard:]\n",
    "        X_aug = np.vstack([augment_state_with_squares(s) for s in states_use])\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_sequence(self, inputs):\n",
    "        preds = []\n",
    "        self.reset_state()\n",
    "        for u in inputs:\n",
    "            self._update(u)\n",
    "            preds.append(self.W_out @ augment_state_with_squares(self.x))\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        preds = []\n",
    "        current_in = np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            out = self.W_out @ augment_state_with_squares(self.x)\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {\n",
    "#     \"reservoir_size\": [300],\n",
    "#     \"input_dim\": [3],\n",
    "#     \"spectral_radius\": [0.92, 0.95, 0.98],\n",
    "#     \"input_scale\": [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "#     \"leaking_rate\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "#     \"ridge_alpha\": [1e-8],\n",
    "#     \"k_ring\": [6, 8, 10],\n",
    "#     \"p_rewire_frontal\": [0.2, 0.3, 0.4],\n",
    "#     \"p_rewire_other\": [0.05, 0.10, 0.2],\n",
    "#     \"P_lat\": [0.02, 0.04, 0.06],\n",
    "#     \"sigma\": [3, 5, 7],\n",
    "#     \"P_call\": [0.005, 0.01],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d45a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {\n",
    "#     \"reservoir_size\": [300],\n",
    "#     \"input_dim\": [3],\n",
    "#     \"spectral_radius\": [0.92, 0.95, 0.98],\n",
    "#     \"input_scale\": [1.0],\n",
    "#     \"leaking_rate\": [0.7, 0.9],\n",
    "#     \"ridge_alpha\": [1e-8],\n",
    "#     \"k_ring\": [6, 8, 10],\n",
    "#     \"p_rewire_frontal\": [0.3, 0.4],\n",
    "#     \"p_rewire_other\": [0.1, 0.05, 0.2],\n",
    "#     \"P_lat\": [0.04, 0.06],\n",
    "#     \"sigma\": [3, 5, 7],\n",
    "#     \"P_call\": [0.005, 0.01],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"reservoir_size\": [300],\n",
    "    \"input_dim\": [3],\n",
    "    \"spectral_radius\": [0.92],\n",
    "    \"input_scale\": [1.0],\n",
    "    \"leaking_rate\": [0.7, 0.9],\n",
    "    \"ridge_alpha\": [1e-8],\n",
    "    \"k_ring\": [6, 8, 10],\n",
    "    \"p_rewire_frontal\": [0.3, 0.4],\n",
    "    \"p_rewire_other\": [0.1, 0.05, 0.2],\n",
    "    \"P_lat\": [0.04, 0.06],\n",
    "    \"sigma\": [3, 5, 7],\n",
    "    \"P_call\": [0.005, 0.01],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d307858",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:10:39.931527Z",
     "iopub.status.busy": "2025-06-02T12:10:39.931191Z",
     "iopub.status.idle": "2025-06-02T12:10:39.941796Z",
     "shell.execute_reply": "2025-06-02T12:10:39.940869Z"
    },
    "papermill": {
     "duration": 0.016065,
     "end_time": "2025-06-02T12:10:39.943473",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.927408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\", f=generate_chen_data, lambda_max=0.9, train_fracs=[0.7, 0.75, 0.8]):\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    # horizons = list(range(10, 1001, 10))\n",
    "    horizons = [200, 400, 600, 800, 1000]\n",
    "    \n",
    "\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores_vpt = []\n",
    "        horizon_nrmse_all = {h: [] for h in horizons}\n",
    "        # horizon_nrmse_all_open_loop = {h: [] for h in horizons}\n",
    "        adev_scores = []\n",
    "        # ldev_scores = []\n",
    "\n",
    "        for initial_state in [[1.0, 1.0, 1.0], [1.0, 2.0, 3.0], [2.0, 1.5, 4.0]]:\n",
    "            tmax = 250\n",
    "            dt = 0.02\n",
    "            t_vals, lorenz_traj = f(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "\n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(lorenz_traj)\n",
    "            lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "            T_data = len(lorenz_traj)\n",
    "            for train_frac in train_fracs:\n",
    "                train_end = int(train_frac * (T_data - 1))\n",
    "                train_input = lorenz_traj[:train_end]\n",
    "                train_target = lorenz_traj[1:train_end + 1]\n",
    "                test_input = lorenz_traj[train_end:-1]\n",
    "                test_target = lorenz_traj[train_end + 1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "\n",
    "                for seed in np.arange(1, 6):\n",
    "                    model = model_class(**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "                    # preds_open_loop = model.predict_open_loop(test_input)\n",
    "\n",
    "                    T_VPT_s, _, ratio = compute_valid_prediction_time(test_target, preds, t_vals, 0.4, lambda_max, dt)\n",
    "                    seed_scores_vpt.append(ratio)\n",
    "\n",
    "                    horizon_nrmse = evaluate_nrmse(preds, test_target, horizons)\n",
    "                    for h in horizons:\n",
    "                        horizon_nrmse_all[h].append(horizon_nrmse[h])\n",
    "\n",
    "                    # horizon_nrmse_open_loop = evaluate_nrmse(preds_open_loop, test_target, horizons)\n",
    "                    # for h in horizons:\n",
    "                    #     horizon_nrmse_all_open_loop[h].append(horizon_nrmse_open_loop[h])\n",
    "\n",
    "                    adev = compute_attractor_deviation(preds, test_target)\n",
    "                    adev_scores.append(adev)\n",
    "\n",
    "                    # ldev = compute_lyapunov_exponent(\"Lorenz\", preds, dt)\n",
    "                    # ldev_scores.append(ldev)\n",
    "\n",
    "        mean_vpt = float(np.mean(seed_scores_vpt))\n",
    "        std_vpt = float(np.std(seed_scores_vpt))\n",
    "        mean_nrmse_dict = {str(h): float(np.mean(horizon_nrmse_all[h])) for h in horizons}\n",
    "        std_nrmse_dict  = {str(h): float(np.std(horizon_nrmse_all[h]))  for h in horizons}\n",
    "        # mean_nrmse_open_loop_dict = {str(h): float(np.mean(horizon_nrmse_all_open_loop[h])) for h in horizons}\n",
    "        # std_nrmse_open_loop_dict  = {str(h): float(np.std(horizon_nrmse_all_open_loop[h]))  for h in horizons}\n",
    "        mean_adev = float(np.mean(adev_scores))\n",
    "        std_adev = float(np.std(adev_scores))\n",
    "        # mean_ldev = float(np.mean(ldev_scores))\n",
    "        # std_ldev = float(np.std(ldev_scores))\n",
    "\n",
    "        results.append({\n",
    "            \"params\": params,\n",
    "            \"seed_scores_T_VPT\": seed_scores_vpt,\n",
    "            \"mean_T_VPT\": mean_vpt,\n",
    "            \"std_T_VPT\": std_vpt,\n",
    "            \"mean_NRMSEs\": mean_nrmse_dict,\n",
    "            \"std_NRMSEs\": std_nrmse_dict,\n",
    "            # \"mean_NRMSEs_open_loop\": mean_nrmse_open_loop_dict,\n",
    "            # \"std_NRMSEs_open_loop\": std_nrmse_open_loop_dict,\n",
    "            \"mean_ADev\": mean_adev,\n",
    "            \"std_ADev\": std_adev,\n",
    "            # \"mean_LDev\": mean_ldev,\n",
    "            # \"std_LDev\": std_ldev\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1976c77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:10:39.949727Z",
     "iopub.status.busy": "2025-06-02T12:10:39.949446Z",
     "iopub.status.idle": "2025-06-02T15:55:28.912311Z",
     "shell.execute_reply": "2025-06-02T15:55:28.911395Z"
    },
    "papermill": {
     "duration": 13488.978805,
     "end_time": "2025-06-02T15:55:28.924887",
     "exception": false,
     "start_time": "2025-06-02T12:10:39.946082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Initial grid search for HH-LR with 1 combinations ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search: 100%|██████████| 1/1 [00:12<00:00, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All results saved to `hhlr2.json`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'params': {'reservoir_size': 300,\n",
       "   'input_dim': 3,\n",
       "   'spectral_radius': 0.92,\n",
       "   'input_scale': 1.0,\n",
       "   'leaking_rate': 0.5,\n",
       "   'ridge_alpha': 1e-08,\n",
       "   'k_ring': 8,\n",
       "   'p_rewire_frontal': 0.3,\n",
       "   'p_rewire_other': 0.1,\n",
       "   'P_lat': 0.04,\n",
       "   'sigma': 5.0,\n",
       "   'P_call': 0.01},\n",
       "  'seed_scores': [np.float64(9.864),\n",
       "   np.float64(8.028),\n",
       "   np.float64(10.602),\n",
       "   np.float64(11.970000000000004),\n",
       "   np.float64(11.952),\n",
       "   np.float64(8.873999999999999),\n",
       "   np.float64(8.856000000000003),\n",
       "   np.float64(10.674),\n",
       "   np.float64(12.114),\n",
       "   np.float64(8.820000000000004),\n",
       "   np.float64(10.547999999999998),\n",
       "   np.float64(9.306000000000003),\n",
       "   np.float64(9.270000000000003),\n",
       "   np.float64(11.358000000000004),\n",
       "   np.float64(11.304000000000002),\n",
       "   np.float64(10.709999999999999),\n",
       "   np.float64(10.674),\n",
       "   np.float64(10.026),\n",
       "   np.float64(8.766000000000002),\n",
       "   np.float64(10.782000000000004),\n",
       "   np.float64(10.854000000000001),\n",
       "   np.float64(12.833999999999998),\n",
       "   np.float64(10.854000000000001),\n",
       "   np.float64(10.944000000000003),\n",
       "   np.float64(10.944000000000003),\n",
       "   np.float64(9.630000000000003),\n",
       "   np.float64(9.612),\n",
       "   np.float64(9.594000000000003),\n",
       "   np.float64(9.774),\n",
       "   np.float64(9.738)],\n",
       "  'mean_T_VPT': 10.309200000000002,\n",
       "  'std_dev': 1.1222335585786052}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_grid_search(HHLobeReservoir, grid, \"hhlr\", output_path=\"hhlr_l 1.json\", f=generate_lorenz_data, lambda_max=0.9)\n",
    "run_grid_search(HHLobeReservoir, grid, \"hhlr\", output_path=\"hhlr_c 1.json\", f=generate_chen_data, lambda_max=0.829, train_fracs=[0.7, 0.75, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47057b",
   "metadata": {
    "papermill": {
     "duration": 0.025218,
     "end_time": "2025-06-02T15:55:28.978478",
     "exception": false,
     "start_time": "2025-06-02T15:55:28.953260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13496.185164,
   "end_time": "2025-06-02T15:55:29.637237",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-02T12:10:33.452073",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
