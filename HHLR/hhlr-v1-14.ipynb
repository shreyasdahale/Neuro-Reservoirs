{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":13496.185164,"end_time":"2025-06-02T15:55:29.637237","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-02T12:10:33.452073","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\nfrom sklearn.preprocessing import MinMaxScaler\nimport json\nimport itertools\nfrom tqdm import tqdm\nfrom __future__ import annotations\nfrom scipy.sparse.linalg import expm\nfrom sklearn.linear_model import Ridge\nfrom typing import Sequence","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-06-02T12:10:38.123217Z","iopub.status.busy":"2025-06-02T12:10:38.122783Z","iopub.status.idle":"2025-06-02T12:10:39.770953Z","shell.execute_reply":"2025-06-02T12:10:39.770123Z"},"papermill":{"duration":1.654633,"end_time":"2025-06-02T12:10:39.772629","exception":false,"start_time":"2025-06-02T12:10:38.117996","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n    x, y, z = state\n    dxdt = sigma * (y - x)\n    dydt = x*(rho - z) - y\n    dzdt = x*y - beta*z\n    return [dxdt, dydt, dzdt]\n\ndef generate_lorenz_data(\n    initial_state=[1.0, 1.0, 1.0],\n    tmax=25.0,\n    dt=0.01,\n    sigma=10.0,\n    rho=28.0,\n    beta=8.0/3.0\n):\n    num_steps = int(tmax / dt) + 1 # +1 to include t=0\n    t_vals = np.linspace(0, tmax, num_steps)\n    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n    return t_vals, sol","metadata":{"execution":{"iopub.execute_input":"2025-06-02T12:10:39.779847Z","iopub.status.busy":"2025-06-02T12:10:39.779422Z","iopub.status.idle":"2025-06-02T12:10:39.786327Z","shell.execute_reply":"2025-06-02T12:10:39.785458Z"},"papermill":{"duration":0.012055,"end_time":"2025-06-02T12:10:39.787851","exception":false,"start_time":"2025-06-02T12:10:39.775796","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n    \"\"\"\n    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n    \n    Parameters\n    ----------\n    y_true : ndarray of shape (N, dim)\n        True trajectory over time.\n    y_pred : ndarray of shape (N, dim)\n        Model's predicted trajectory over time (closed-loop).\n    t_vals : ndarray of shape (N,)\n        Time values corresponding to the trajectory steps.\n    threshold : float, optional\n        The error threshold, default is 0.4 as in your snippet.\n    lambda_max : float, optional\n        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n        \n    Returns\n    -------\n    T_VPT : float\n        Valid prediction time. The earliest time at which normalized error surpasses threshold\n        (or the last time if never surpassed).\n    T_lambda : float\n        Lyapunov time = 1 / lambda_max\n    ratio : float\n        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n    \"\"\"\n    # 1) Average of y_true\n    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n    \n    # 2) Time-averaged norm^2 of (y_true - y_mean)\n    y_centered = y_true - y_mean\n    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n    \n    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n    diff = y_true - y_pred\n    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n    delta_gamma = err_sq / denom      # shape (N,)\n    \n    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n    idx_exceed = np.where(delta_gamma > threshold)[0]\n    if len(idx_exceed) == 0:\n        # never exceeds threshold => set T_VPT to the final time\n        T_VPT = t_vals[-1]\n    else:\n        T_VPT = t_vals[idx_exceed[0]]\n    \n    # 5) Compute T_lambda and ratio\n    T_lambda = 1.0 / lambda_max\n\n    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n\n    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n    ratio = T_VPT / T_lambda\n\n    return T_VPT, T_lambda, ratio","metadata":{"execution":{"iopub.execute_input":"2025-06-02T12:10:39.794593Z","iopub.status.busy":"2025-06-02T12:10:39.794263Z","iopub.status.idle":"2025-06-02T12:10:39.802048Z","shell.execute_reply":"2025-06-02T12:10:39.801063Z"},"papermill":{"duration":0.012858,"end_time":"2025-06-02T12:10:39.803577","exception":false,"start_time":"2025-06-02T12:10:39.790719","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import Ridge\nfrom numpy.linalg import eigvals\n\n# ---------------------------------------------------------------------\n# Utility helpers\n# ---------------------------------------------------------------------\ndef scale_spectral_radius(W, desired_radius=0.95):\n    \"\"\"Affine-scale square matrix W so that its spectral radius equals desired_radius.\"\"\"\n    eigs = eigvals(W)\n    current_radius = np.max(np.abs(eigs))\n    if current_radius == 0:\n        raise ValueError(\"Spectral radius of W is zero.\")\n    return W * (desired_radius / current_radius)\n\ndef augment_state_with_squares(x):\n    \"\"\"[x, x^2, 1]   (same convention as in CycleReservoir3D).\"\"\"\n    return np.concatenate([x, x**2, [1.0]])\n\n# ---------------------------------------------------------------------\n# HH-LR class\n# ---------------------------------------------------------------------\nclass HHLobeReservoir:\n    \"\"\"\n    Hemispherically-Hierarchical Lobe Reservoir (HH-LR).\n\n    Topology:\n      * 8 anatomical modules  —  L/R × {F, P, T, O}\n      * Intra-module: Watts–Strogatz small-world graphs (ring+rewire)\n      * Intra-hemisphere inter-lobe: distance-modulated shortcuts\n      * Inter-hemisphere (callosal): sparse homotopic bridges\n    \"\"\"\n\n    # --- anatomical bookkeeping -------------------------------------------------\n    _LOBES   = ['F', 'P', 'T', 'O']\n    _HEMIS   = ['L', 'R']\n    # rough 2-D centroids (arbitrary units) for distance computation\n    _CENTROIDS = {\n        ('L', 'F'): (-1.0,  1.0),\n        ('L', 'P'): (-1.0,  0.0),\n        ('L', 'T'): (-1.0, -1.0),\n        ('L', 'O'): (-1.0, -2.0),\n        ('R', 'F'): ( 1.0,  1.0),\n        ('R', 'P'): ( 1.0,  0.0),\n        ('R', 'T'): ( 1.0, -1.0),\n        ('R', 'O'): ( 1.0, -2.0),\n    }\n\n    # ------------------------------------------------------------------\n    def __init__(self,\n                 reservoir_size=800,\n                 input_dim=128,\n                 spectral_radius=0.9,\n                 input_scale=1.0,\n                 leaking_rate=1.0,\n                 ridge_alpha=1e-6,\n                 # small-world / shortcut hyper-parameters\n                 k_ring=8,\n                 p_rewire_frontal=0.30,\n                 p_rewire_other=0.10,\n                 P_lat=0.04,\n                 sigma=5.0,\n                 P_call=0.01,\n                 seed=42):\n        \"\"\"\n        Parameters\n        ----------\n        reservoir_size   : total neuron count (split equally across the 8 modules if not divisible)\n        input_dim        : dimensionality of input vector u_t  (128 for EEG band-power features)\n        leaking_rate     : α in leaky-integrator update\n        \"\"\"\n        self.N     = reservoir_size\n        self.D_in  = input_dim\n        self.rho   = spectral_radius\n        self.in_scale   = input_scale\n        self.alpha = leaking_rate\n        self.ridge_alpha = ridge_alpha\n        self.seed  = seed\n\n        # ------------------------------------------------------------------\n        # 1) allocate neurons to the 8 modules as evenly as possible\n        # ------------------------------------------------------------------\n        base = self.N // 8\n        counts = [base] * 8\n        for i in range(self.N - base*8):\n            counts[i] += 1\n        self._module_slices = {}\n        idx0 = 0\n        for h in self._HEMIS:\n            for l in self._LOBES:\n                n = counts[len(self._module_slices)]\n                self._module_slices[(h, l)] = slice(idx0, idx0 + n)\n                idx0 += n\n\n        # ------------------------------------------------------------------\n        # 2) build adjacency matrix W according to HH-LR rules\n        # ------------------------------------------------------------------\n        rng = np.random.default_rng(self.seed)\n        W = np.zeros((self.N, self.N), dtype=float)\n\n        # 2.1 intra-module small-world wiring\n        for (h, l), sl in self._module_slices.items():\n            n_mod = sl.stop - sl.start\n            k = min(k_ring, n_mod-1)  # guard tiny modules\n            p_rewire = p_rewire_frontal if l == 'F' else p_rewire_other\n            # ring lattice\n            for i_local in range(n_mod):\n                for m in range(1, k+1):\n                    j_local = (i_local + m) % n_mod\n                    i_glob = sl.start + i_local\n                    j_glob = sl.start + j_local\n                    W[i_glob, j_glob] = rng.standard_normal()\n                    W[j_glob, i_glob] = rng.standard_normal()\n            # rewire each existing edge with prob p_rewire\n            for i_local in range(n_mod):\n                for m in range(1, k+1):\n                    if rng.random() < p_rewire:\n                        # pick a random new target in same module (avoid self-loop)\n                        j_local_new = rng.integers(0, n_mod-1)\n                        if j_local_new >= i_local:\n                            j_local_new += 1\n                        i_glob = sl.start + i_local\n                        j_glob_new = sl.start + j_local_new\n                        # overwrite previous weight (both directions)\n                        w_new = rng.standard_normal()\n                        W[i_glob, j_glob_new] = w_new\n                        W[j_glob_new, i_glob] = rng.standard_normal()\n\n        # 2.2 intra-hemisphere inter-lobe shortcuts (distance-weighted)\n        for h in self._HEMIS:\n            for l1 in self._LOBES:\n                for l2 in self._LOBES:\n                    if l1 == l2:\n                        continue\n                    sl1 = self._module_slices[(h, l1)]\n                    sl2 = self._module_slices[(h, l2)]\n                    c1 = np.array(self._CENTROIDS[(h, l1)])\n                    c2 = np.array(self._CENTROIDS[(h, l2)])\n                    dist = np.linalg.norm(c1 - c2)\n                    p_edge = P_lat * np.exp(-dist / sigma)\n                    for i in range(sl1.start, sl1.stop):\n                        mask = rng.random(sl2.stop - sl2.start) < p_edge\n                        js = np.nonzero(mask)[0] + sl2.start\n                        if js.size:\n                            W[i, js] = rng.standard_normal(size=js.size)\n                            W[js, i] = rng.standard_normal(size=js.size)\n\n        # 2.3 inter-hemisphere callosal bridges (homotopic)\n        for l in self._LOBES:\n            sl_L = self._module_slices[('L', l)]\n            sl_R = self._module_slices[('R', l)]\n            for i in range(sl_L.start, sl_L.stop):\n                mask = rng.random(sl_R.stop - sl_R.start) < P_call\n                js = np.nonzero(mask)[0] + sl_R.start\n                if js.size:\n                    W[i, js] = rng.standard_normal(size=js.size)\n                    W[js, i] = rng.standard_normal(size=js.size)\n\n        # 2.4 spectral scaling\n        W = scale_spectral_radius(W, self.rho)\n        self.W = W\n\n        # ------------------------------------------------------------------\n        # 3) random input weights\n        # ------------------------------------------------------------------\n        rng = np.random.default_rng(self.seed + 1)\n        self.W_in = (rng.random((self.N, self.D_in)) - 0.5) * 2.0 * self.in_scale\n\n        # readout and state\n        self.W_out = None\n        self.x = np.zeros(self.N)\n\n    # ------------------------------------------------------------------\n    # ESN core methods (same signatures as CycleReservoir3D)\n    # ------------------------------------------------------------------\n    def reset_state(self):\n        self.x = np.zeros(self.N)\n\n    def _update(self, u):\n        pre_activation = self.W @ self.x + self.W_in @ u\n        x_new = np.tanh(pre_activation)\n        self.x = (1.0 - self.alpha) * self.x + self.alpha * x_new\n\n    def collect_states(self, inputs, discard=100):\n        \"\"\"\n        Parameters\n        ----------\n        inputs  : iterable / array of shape (T, input_dim)\n        discard : number of initial time-steps to omit from training\n        \"\"\"\n        self.reset_state()\n        states = []\n        for u in inputs:\n            self._update(u)\n            states.append(self.x.copy())\n        return np.array(states[discard:]), np.array(states[:discard])\n\n    def fit_readout(self, train_input, train_target, discard=100):\n        \"\"\"\n        Ridge regression read-out; identical augmentation as baseline.\n        \"\"\"\n        states_use, _ = self.collect_states(train_input, discard=discard)\n        targets_use = train_target[discard:]\n        X_aug = np.vstack([augment_state_with_squares(s) for s in states_use])\n\n        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n        reg.fit(X_aug, targets_use)\n        self.W_out = reg.coef_\n\n    def predict_sequence(self, inputs):\n        \"\"\"\n        Feed-forward prediction (no teacher forcing).  Suitable for classification:\n        returns raw linear outputs; apply threshold/sigmoid externally.\n        \"\"\"\n        preds = []\n        self.reset_state()\n        for u in inputs:\n            self._update(u)\n            preds.append(self.W_out @ augment_state_with_squares(self.x)) # quadratic lift-off\n            #preds.append(self.W_out @ self.x)\n        return np.array(preds)\n    \n    def predict_autoregressive(self, initial_input, n_steps):\n        preds = []\n        current_in = np.array(initial_input)\n        for _ in range(n_steps):\n            self._update(current_in)\n            out = self.W_out @ augment_state_with_squares(self.x)\n            preds.append(out)\n            current_in = out\n        return np.array(preds)","metadata":{"execution":{"iopub.execute_input":"2025-06-02T12:10:39.810566Z","iopub.status.busy":"2025-06-02T12:10:39.810231Z","iopub.status.idle":"2025-06-02T12:10:39.909131Z","shell.execute_reply":"2025-06-02T12:10:39.90817Z"},"papermill":{"duration":0.104497,"end_time":"2025-06-02T12:10:39.910805","exception":false,"start_time":"2025-06-02T12:10:39.806308","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grid = {\n    \"reservoir_size\": [300],\n    \"input_dim\": [3],\n    \"spectral_radius\": [0.92],\n    \"input_scale\": [0.8],\n    \"leaking_rate\": [0.3, 0.5, 0.7, 0.9],\n    \"ridge_alpha\": [1e-8],\n    \"k_ring\": [6, 8, 10],\n    \"p_rewire_frontal\": [0.2, 0.3, 0.4],\n    \"p_rewire_other\": [0.05, 0.10, 0.2],\n    \"P_lat\": [0.02, 0.04, 0.06],\n    \"sigma\": [3, 5, 7],\n    \"P_call\": [0.005, 0.01],\n}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_grid_search(model_class, param_grid, model_name,\n                    output_path=\"grid_search_results.json\"):\n    combos = list(itertools.product(*param_grid.values()))\n    param_keys = list(param_grid.keys())\n    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n\n    results = []\n    for comb in tqdm(combos, desc=\"Grid Search\"):\n        params = dict(zip(param_keys, comb))\n        seed_scores = []\n        \n        for initial_state in [[1.0, 1.0, 1.0], [1.0, 2.0, 3.0], [2.0, 1.5, 4.0]]:\n            tmax = 250\n            dt   = 0.02\n            t_vals, lorenz_traj = generate_lorenz_data(\n                initial_state=initial_state,\n                tmax=tmax,\n                dt=dt\n            )\n            \n            washout = 2000\n            t_vals = t_vals[washout:]\n            lorenz_traj = lorenz_traj[washout:]\n            \n            scaler = MinMaxScaler()\n            scaler.fit(lorenz_traj)\n            lorenz_traj = scaler.transform(lorenz_traj)\n            \n            T_data = len(lorenz_traj)\n            for train_frac in [0.75, 0.8]:\n                train_end = int(train_frac*(T_data-1))\n                train_input  = lorenz_traj[:train_end]\n                train_target = lorenz_traj[1:train_end+1]\n                test_input   = lorenz_traj[train_end:-1]\n                test_target  = lorenz_traj[train_end+1:]\n                n_test_steps = len(test_input)\n                initial_in = test_input[0]\n                for seed in np.arange(1, 6):\n                    model = model_class(**params, seed=seed)\n                    model.fit_readout(train_input, train_target, discard=100)\n                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n                    _, _, T_VPT_s = compute_valid_prediction_time(test_target, preds, t_vals, 0.4, 0.9, dt)\n                    seed_scores.append(T_VPT_s)\n        mean_score = float(np.mean(seed_scores))\n        std_dev    = float(np.std(seed_scores))\n        # is_stable  = std_dev < 1.5\n        # status     = \"Stable\" if is_stable else \"Unstable\"\n        \n        # print(f\"Params: {params} → Avg T_VPT={mean_score:.3f}, \"\n        #       f\"Std Dev={std_dev:.3f} → {status}\")\n\n        results.append({\n            \"params\":      params,\n            \"seed_scores\": seed_scores,\n            \"mean_T_VPT\":  mean_score,\n            \"std_dev\":     std_dev,\n            # \"stable\":      is_stable\n        })\n\n    # Save results\n    with open(output_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\"\\nAll results saved to `{output_path}`\")\n    \n    return results","metadata":{"execution":{"iopub.execute_input":"2025-06-02T12:10:39.931527Z","iopub.status.busy":"2025-06-02T12:10:39.931191Z","iopub.status.idle":"2025-06-02T12:10:39.941796Z","shell.execute_reply":"2025-06-02T12:10:39.940869Z"},"papermill":{"duration":0.016065,"end_time":"2025-06-02T12:10:39.943473","exception":false,"start_time":"2025-06-02T12:10:39.927408","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_grid_search(HHLobeReservoir, grid, \"HH-LR\", output_path=\"hhlr[v1] 14.json\")","metadata":{"execution":{"iopub.execute_input":"2025-06-02T12:10:39.949727Z","iopub.status.busy":"2025-06-02T12:10:39.949446Z","iopub.status.idle":"2025-06-02T15:55:28.912311Z","shell.execute_reply":"2025-06-02T15:55:28.911395Z"},"papermill":{"duration":13488.978805,"end_time":"2025-06-02T15:55:28.924887","exception":false,"start_time":"2025-06-02T12:10:39.946082","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.025218,"end_time":"2025-06-02T15:55:28.978478","exception":false,"start_time":"2025-06-02T15:55:28.95326","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}