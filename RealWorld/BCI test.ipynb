{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a722d1",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e605f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib.colors import Normalize\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import welch\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "from numpy.linalg import eigvals\n",
    "import os\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac8933",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf80a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_spectral_radius(W, target_radius=0.95):\n",
    "    \"\"\"\n",
    "    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(W)\n",
    "    radius = np.max(np.abs(eigvals))\n",
    "    if radius == 0:\n",
    "        return W\n",
    "    return (W / radius) * target_radius\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"\n",
    "    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n",
    "    We'll use this for both training and prediction.\n",
    "    \"\"\"\n",
    "    x_sq = x**2\n",
    "    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c330c",
   "metadata": {},
   "source": [
    "# Baseline ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00babe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESN3D:\n",
    "    \"\"\"\n",
    "    Dense random ESN for 3D->3D single-step.\n",
    "    Teacher forcing for training, autoregressive for testing.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reservoir_size=300,\n",
    "                 input_dim=62,\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.randn(reservoir_size, reservoir_size)*0.1\n",
    "        W = scale_spectral_radius(W, self.spectral_radius)\n",
    "        self.W = W\n",
    "\n",
    "        np.random.seed(self.seed+1)\n",
    "        self.W_in = (np.random.rand(reservoir_size,input_dim) - 0.5)*2.0*self.input_scale\n",
    "        # self.W_in = np.random.uniform(-self.input_scale, self.input_scale, (reservoir_size, 3))\n",
    "\n",
    "        self.W_out = None\n",
    "        self.x = np.zeros(reservoir_size)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.x = np.zeros(self.reservoir_size)\n",
    "\n",
    "    def _update(self, u):\n",
    "        pre_activation = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.tanh(pre_activation)\n",
    "        alpha = self.leaking_rate\n",
    "        self.x = (1.0 - alpha)*self.x + alpha*x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states = np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]\n",
    "        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0],1))])\n",
    "\n",
    "        # polynomial readout\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append(augment_state_with_squares(s))\n",
    "        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        preds = []\n",
    "        current_in = np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            # x_aug = np.concatenate([self.x, [1.0]])\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7bb5cd",
   "metadata": {},
   "source": [
    "# SCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc4400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CR3D:\n",
    "    \"\"\"\n",
    "    Cycle (ring) reservoir for 3D->3D single-step,\n",
    "    teacher forcing for training, autoregressive for testing.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reservoir_size=300,\n",
    "                 input_dim=62,\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.zeros((reservoir_size, reservoir_size))\n",
    "        for i in range(reservoir_size):\n",
    "            j = (i+1) % reservoir_size\n",
    "            W[i, j] = 1.0\n",
    "        W = scale_spectral_radius(W, self.spectral_radius)\n",
    "        self.W = W\n",
    "        \n",
    "        np.random.seed(self.seed+1)\n",
    "        self.W_in = (np.random.rand(reservoir_size,input_dim) - 0.5)*2.0*self.input_scale\n",
    "\n",
    "        self.W_out = None\n",
    "        self.x = np.zeros(reservoir_size)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.x = np.zeros(self.reservoir_size)\n",
    "\n",
    "    def _update(self, u):\n",
    "        pre_activation = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.tanh(pre_activation)\n",
    "        alpha = self.leaking_rate\n",
    "        self.x = (1.0 - alpha)*self.x + alpha*x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states = np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]\n",
    "        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0],1))])\n",
    "\n",
    "        # polynomial readout\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append(augment_state_with_squares(s))\n",
    "        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        preds = []\n",
    "        current_in = np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            # x_aug = np.concatenate([self.x, [1.0]])\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150e19b",
   "metadata": {},
   "source": [
    "# CRJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ca3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRJ3D:\n",
    "    \"\"\"\n",
    "    Cycle Reservoir with Jumps (CRJ) for 3D->3D single-step tasks.\n",
    "    We form a ring adjacency with an extra 'jump' edge in each row.\n",
    "    This can help capture multiple timescales or delayed memory\n",
    "    while retaining the easy ring structure.\n",
    "\n",
    "    The adjacency is built as follows (reservoir_size = mod N):\n",
    "      For each i in [0..N-1]:\n",
    "        W[i, (i+1) % mod N] = 1.0\n",
    "        W[i, (i+jump) % mod N] = 1.0\n",
    "    Then we scale by 'spectral_radius.' We do an ESN update\n",
    "    with readout [ x, x^2, 1 ] -> next step in R^3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 reservoir_size=300,\n",
    "                 input_dim=62,\n",
    "                 jump=10,                # offset for the jump\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        reservoir_size: how many nodes in the ring\n",
    "        jump            : the offset for the 2nd connection from node i\n",
    "        spectral_radius : scale adjacency\n",
    "        input_scale     : scale factor for W_in\n",
    "        leaking_rate    : ESN 'alpha'\n",
    "        ridge_alpha     : ridge penalty for readout\n",
    "        seed            : random seed\n",
    "        \"\"\"\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.jump = jump\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.seed = seed\n",
    "\n",
    "        # build adjacency\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.zeros((reservoir_size, reservoir_size))\n",
    "        for i in range(reservoir_size):\n",
    "            # cycle edge: i -> (i+1)%N\n",
    "            W[i, (i+1) % reservoir_size] = 1.0\n",
    "            # jump edge: i -> (i+jump)%N\n",
    "            W[i, (i + self.jump) % reservoir_size] = 1.0\n",
    "\n",
    "        # scale spectral radius\n",
    "        W = scale_spectral_radius(W, self.spectral_radius)\n",
    "        self.W = W\n",
    "\n",
    "        # input weights => shape [N,3]\n",
    "        np.random.seed(self.seed+100)\n",
    "        W_in = (np.random.rand(reservoir_size, input_dim) - 0.5)*2.0*self.input_scale\n",
    "        self.W_in = W_in\n",
    "\n",
    "        # readout\n",
    "        self.W_out = None\n",
    "        self.x = np.zeros(self.reservoir_size)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.x = np.zeros(self.reservoir_size)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        Single-step ESN update:\n",
    "          x(t+1) = (1-alpha)*x(t) + alpha*tanh( W x(t) + W_in u(t) )\n",
    "        \"\"\"\n",
    "        pre_activation = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.tanh(pre_activation)\n",
    "        alpha = self.leaking_rate\n",
    "        self.x = (1.0 - alpha)*self.x + alpha*x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Teacher forcing => feed the real 3D inputs => gather states.\n",
    "        Return (states_after_discard, states_discarded).\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states = np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        gather states => polynomial readout => solve ridge\n",
    "        \"\"\"\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            # polynomial expansion => [ x, x^2, 1 ]\n",
    "            X_list.append(augment_state_with_squares(s))\n",
    "        X_aug = np.array(X_list)\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        self.W_out = reg.coef_  # shape => (3, 2N+1)\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        fully autoregressive => feed last output => next input\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        #self.reset_state()\n",
    "        current_in = np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            big_x = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ big_x  # shape => (3,)\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)\n",
    "        \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540a5d6",
   "metadata": {},
   "source": [
    "# MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47bdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCI3D:\n",
    "    \"\"\"\n",
    "    Minimum Complexity Interaction ESN (MCI-ESN).\n",
    "\n",
    "    This class implements the approach described in:\n",
    "      \"A Minimum Complexity Interaction Echo State Network\"\n",
    "        by Jianming Liu, Xu Xu, Eric Li (2024).\n",
    "    \n",
    "    The model structure:\n",
    "      - We maintain two 'simple cycle' reservoirs (each of size N).\n",
    "      - Each reservoir is a ring with weight = l, i.e. \n",
    "            W_res[i, (i+1)%N] = l\n",
    "        plus the corner wrap from (N-1)->0, also = l. ##(unnecessary as already called for in the prev. line)\n",
    "      - The two reservoirs interact via a minimal connection matrix: \n",
    "         exactly 2 cross-connections with weight = g. \n",
    "         (One might connect x2[-1], x2[-2], ... \n",
    "          But we do where reservoir1 sees x2[-1] \n",
    "          in one location, and reservoir2 sees x1[-1] likewise.)\n",
    "      - Activation function in reservoir1 is cos(·), and in reservoir2 is sin(·).\n",
    "      - They each have a separate input weight matrix: Win1 and Win2. \n",
    "        The final state is a linear combination \n",
    "           x(t) = h*x1(t) + (1-h)*x2(t).\n",
    "      - Then we do a polynomial readout [x, x^2, 1] -> output.\n",
    "      - We feed teacher forcing in collect_states, \n",
    "        then solve readout with Ridge regression.\n",
    "\n",
    "    References:\n",
    "      - Liu, J., Xu, X., & Li, E. (2024). \n",
    "        \"A minimum complexity interaction echo state network,\" \n",
    "         Neural Computing and Applications.\n",
    "    \n",
    "    notes:\n",
    "      - The reservoir_size is N for each reservoir, \n",
    "        so total param dimension is 2*N for states, \n",
    "        but we produce a single final \"combined\" state x(t) in R^N for readout.\n",
    "      - The activation f1=cos(...) for reservoir1, f2=sin(...) for reservoir2, \n",
    "        as recommended by the paper for MCI-ESN.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reservoir_size=250,\n",
    "        cycle_weight=0.9,      # 'l' in the paper\n",
    "        connect_weight=0.9,    # 'g' in the paper\n",
    "        input_scale=0.2,\n",
    "        leaking_rate=1.0,\n",
    "        ridge_alpha=1e-6,\n",
    "        combine_factor=0.1,    # 'h' in the paper\n",
    "        seed=47,\n",
    "        v1=0.6, v2=0.6         # fixed values for v1, v2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        reservoir_size: N, size of each cycle reservoir \n",
    "        cycle_weight : l, ring adjacency weight in [0,1), ensures cycle synergy\n",
    "        connect_weight: g, cross-connection weight between the two cycle reservoirs\n",
    "        input_scale   : scale factor for input->reservoir weights\n",
    "        leaking_rate  : ESN update alpha \n",
    "        ridge_alpha   : readout ridge penalty\n",
    "        combine_factor: h in [0,1], to form x(t)= h*x1(t)+(1-h)*x2(t) as final combined state\n",
    "        seed          : random seed\n",
    "        \"\"\"\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.cycle_weight   = cycle_weight\n",
    "        self.connect_weight = connect_weight\n",
    "        self.input_scale    = input_scale\n",
    "        self.leaking_rate   = leaking_rate\n",
    "        self.ridge_alpha    = ridge_alpha\n",
    "        self.combine_factor = combine_factor\n",
    "        self.seed           = seed\n",
    "        self.v1 = v1\n",
    "        self.v2 = v2\n",
    "\n",
    "        # We'll define (and build) adjacency for each cycle, \n",
    "        # plus cross-connection for two sub-reservoirs.\n",
    "        # We'll define 2 input weight mats: Win1, Win2.\n",
    "        # We'll define states x1(t), x2(t).\n",
    "        # We'll define readout W_out after training.\n",
    "\n",
    "        self._build_mci_esn()\n",
    "\n",
    "    def _build_mci_esn(self):\n",
    "        \"\"\"\n",
    "        Build all the internal parameters: \n",
    "         - ring adjacency for each reservoir\n",
    "         - cross-reservoir connection\n",
    "         - input weights for each reservoir\n",
    "         - initial states\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        N = self.reservoir_size\n",
    "\n",
    "        # Build ring adjacency W_res in shape [N, N], with cycle_weight on ring\n",
    "        W_res = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            j = (i+1) % N\n",
    "            W_res[j, i] = self.cycle_weight\n",
    "        self.W_res = W_res  # shared by both sub-reservoirs\n",
    "\n",
    "        # Build cross-connection W_cn for shape [N,N], \n",
    "        # minimal 2 nonzero elements. \n",
    "        # For the simplest approach from the paper:\n",
    "        #   W_cn[0, N-1] = g, W_cn[1, N-2] = g or similar.\n",
    "        # The paper's eq(7) suggests the last 2 elements in x(t) cross to first 2 in the other reservoir:\n",
    "        # We'll do the simplest reference: if i=0 or i=1, we connect from the other reservoir's last or second-last. \n",
    "        # We'll define a function for each sub-res to pick up from the other sub-res. \n",
    "        # We can store them in separate arrays, or define them in code. \n",
    "        # We'll just store \"We want index 0 to see x2[-1], index 1 to see x2[-2].\"\n",
    "\n",
    "        # But as done in the original code snippet from the paper:\n",
    "        #   Wcn has\n",
    "        # effectively 2 nonzero positions. We'll define that pattern:\n",
    "        W_cn = np.zeros((N, N))\n",
    "        # e.g. W_cn[0, N-1] = g, W_cn[N-1, N-2] = g or something. \n",
    "        # The paper example used W_cn = diag(0,g,...) plus the corner. We'll do the simplest:\n",
    "        # let W_cn[0, N-1]=g, W_cn[1, N-2]=g.\n",
    "        # This matches the minimal cross. \n",
    "        # For clarity we do:\n",
    "        W_cn[0, N-1] = self.connect_weight\n",
    "        if N>1:\n",
    "            # W_cn[1, N-2] = self.connect_weight\n",
    "            W_cn[N-1, 0] = self.connect_weight\n",
    "        self.W_cn = W_cn\n",
    "\n",
    "        # We'll define input weights for each sub-reservoir, shape [N, dim_input].\n",
    "        # The paper sets them as eq(10) in the snippet, with different signs. \n",
    "        # We'll define them as parted. \n",
    "        # We define V1, V2 => shape [N, dim_input], with constant magnitude t1, t2, random sign. \n",
    "        # We'll do random. Need to check this in the paper again\n",
    "        # We'll keep \"two\" separate. user can define input_scale but not two separate. \n",
    "        # We'll do the simplest approach: the absolute value is the same => input_scale, \n",
    "        # sign is random. Then we define Win1 = V1 - V2, Win2 = V1 + V2.\n",
    "        # This is consistent with eq(10) from the paper.\n",
    "\n",
    "        self.Win1 = None\n",
    "        self.Win2 = None\n",
    "\n",
    "        # We'll define states x1(t), x2(t). We'll do them after dimension known. \n",
    "        self.x1 = None\n",
    "        self.x2 = None\n",
    "\n",
    "        self.W_out = None\n",
    "\n",
    "    def _init_substates(self):\n",
    "        \"\"\"\n",
    "        Once we know reservoir_size, we define x1, x2 as zeros. \n",
    "        We'll call this in reset_state or at fit time.\n",
    "        \"\"\"\n",
    "        N = self.reservoir_size\n",
    "        self.x1 = np.zeros(N)\n",
    "        self.x2 = np.zeros(N)\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.x1 is not None:\n",
    "            self.x1[:] = 0.0\n",
    "        if self.x2 is not None:\n",
    "            self.x2[:] = 0.0\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        Single-step reservoir update.\n",
    "        x1(t+1) = cos( Win1*u(t+1) + W_res*x1(t) + W_cn*x2(t) )\n",
    "        x2(t+1) = sin( Win2*u(t+1) + W_res*x2(t) + W_cn*x1(t) )\n",
    "        Then x(t)= h*x1(t+1) + (1-h)* x2(t+1).\n",
    "        We'll define the leaky integration. \n",
    "        But the paper uses an approach with no leak? Be careful.\n",
    "        We'll do the approach: x1(t+1)= (1-alpha)* x1(t) + alpha*cos(...).\n",
    "        \"\"\"\n",
    "        alpha = self.leaking_rate\n",
    "\n",
    "        # pre activation for reservoir1\n",
    "        pre1 = self.Win1 @ u + self.W_res @ self.x1 + self.W_cn @ self.x2\n",
    "        # reservoir1 uses cos\n",
    "        new_x1 = np.cos(pre1)\n",
    "\n",
    "        # reservoir2 uses sin\n",
    "        pre2 = self.Win2 @ u + self.W_res @ self.x2 + self.W_cn @ self.x1\n",
    "        new_x2 = np.sin(pre2)\n",
    "\n",
    "        self.x1 = (1.0 - alpha)*self.x1 + alpha*new_x1\n",
    "        self.x2 = (1.0 - alpha)*self.x2 + alpha*new_x2\n",
    "\n",
    "    def _combine_state(self):\n",
    "        \"\"\"\n",
    "        Combine x1(t), x2(t) => x(t) = h*x1 + (1-h)*x2\n",
    "        \"\"\"\n",
    "        h = self.combine_factor\n",
    "        return h*self.x1 + (1.0 - h)*self.x2\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        # We reset the reservoir to zero\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for t in range(len(inputs)):\n",
    "            self._update(inputs[t])   # feed the REAL input from the dataset\n",
    "            combined = self._combine_state()\n",
    "            states.append(combined.copy())\n",
    "        states = np.array(states)  # shape => [T, N]\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Build input weights if needed, gather states on the training data (teacher forcing),\n",
    "        then solve a polynomial readout [x, x^2, 1]->train_target(t).\n",
    "\n",
    "        train_input : shape [T, d_in]\n",
    "        train_target: shape [T, d_out]\n",
    "        discard     : # of states to discard for warmup\n",
    "        \"\"\"\n",
    "        T = len(train_input)\n",
    "        if T<2:\n",
    "            raise ValueError(\"Not enough training data\")\n",
    "\n",
    "        d_in = train_input.shape[1]\n",
    "        # d_out = train_target.shape[1]\n",
    "\n",
    "        # built Win1, Win2\n",
    "        if self.Win1 is None or self.Win2 is None:\n",
    "            np.random.seed(self.seed+100)\n",
    "            # build V1, V2 in shape [N, d_in]\n",
    "            N = self.reservoir_size\n",
    "            # V1 = (np.random.rand(N, d_in)-0.5)*2.0*self.input_scale\n",
    "            # V2 = (np.random.rand(N, d_in)-0.5)*2.0*self.input_scale\n",
    "\n",
    "            sign_V1 = np.random.choice([-1, 1], size=(N, d_in))\n",
    "            sign_V2 = np.random.choice([-1, 1], size=(N, d_in))\n",
    "\n",
    "            v1, v2 = self.v1, self.v2  # fixed values for V1, V2\n",
    "\n",
    "            V1 = v1 * sign_V1 * self.input_scale\n",
    "            V2 = v2 * sign_V2 * self.input_scale\n",
    "\n",
    "            # eq(10): Win1= V1 - V2, Win2= V1 + V2\n",
    "            self.Win1 = V1 - V2\n",
    "            self.Win2 = V1 + V2\n",
    "\n",
    "        # define x1, x2\n",
    "        self._init_substates()\n",
    "\n",
    "        # gather states\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]  # shape => [T-discard, d_out]\n",
    "\n",
    "        # polynomial readout\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append(augment_state_with_squares(s))\n",
    "        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n",
    "\n",
    "        # Solve ridge\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        # W_out => shape [d_out, 2N+1]\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        Fully autoregressive: \n",
    "          We do not use teacher forcing, \n",
    "          we feed the model's last output as the next input \n",
    "        Typically, for MCI-ESN the paper does input(t+1) in R^d. \n",
    "        We do the test_input\n",
    "        For multi-step chaotic forecast, we feed the model's output as input? \n",
    "        That means the system dimension d_in must match d_out. \n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        # re-init states\n",
    "        #self._init_substates()\n",
    "\n",
    "        # we assume initial_input => shape (d_in,)\n",
    "        current_in = np.array(initial_input)\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            # read out\n",
    "            combined = self._combine_state()\n",
    "            big_x = augment_state_with_squares(combined)\n",
    "            out = self.W_out @ big_x  # shape => (d_out,)\n",
    "\n",
    "            preds.append(out)\n",
    "            current_in = out  # feed output back as next input\n",
    "\n",
    "        return np.array(preds)\n",
    "        \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            combined = self._combine_state()\n",
    "            x_aug = augment_state_with_squares(combined)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b7d64",
   "metadata": {},
   "source": [
    "# DeepESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bbc9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepESN3D:\n",
    "    \"\"\"\n",
    "    Deep Echo State Network (DeepESN) for multi-layered reservoir computing.\n",
    "    Each layer has its own reservoir, and the states are propagated through layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers=3,\n",
    "                 reservoir_size=100,\n",
    "                 input_dim=62,\n",
    "                 spectral_radius=0.95,\n",
    "                 connectivity=0.1,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 activation_choices=('tanh','relu','sin','linear'),\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_layers: Number of reservoir layers.\n",
    "        - reservoir_size: Number of neurons in each reservoir layer.\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.connectivity = connectivity\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.activation_choices = activation_choices\n",
    "        self.seed = seed\n",
    "\n",
    "        # Initialize reservoirs and input weights for each layer\n",
    "        self.reservoirs = []\n",
    "        self.input_weights = []\n",
    "        self.states = []\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        for layer in range(num_layers):\n",
    "            np.random.seed(seed + layer)\n",
    "            W = np.random.randn(reservoir_size, reservoir_size) * 0.1\n",
    "            mask = (np.random.rand(reservoir_size, reservoir_size) < self.connectivity)\n",
    "            W = W * mask\n",
    "            W = scale_spectral_radius(W, spectral_radius)\n",
    "            self.reservoirs.append(W)\n",
    "\n",
    "            if layer == 0 : \n",
    "                W_in = (np.random.rand(reservoir_size, input_dim) - 0.5) * 2.0 * input_scale\n",
    "            else:\n",
    "                W_in = (np.random.rand(reservoir_size, reservoir_size) - 0.5) * 2.0 * input_scale\n",
    "            self.input_weights.append(W_in)\n",
    "\n",
    "        np.random.seed(self.seed + 200)\n",
    "        self.node_activations = np.random.choice(self.activation_choices, size=self.reservoir_size)\n",
    "        \n",
    "        self.W_out = None\n",
    "        self.reset_state()\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"\n",
    "        Reset the states of all reservoir layers.\n",
    "        \"\"\"\n",
    "        self.states = [np.zeros(self.reservoir_size) for _ in range(self.num_layers)]\n",
    "\n",
    "    def _apply_activation(self, act_type, val):\n",
    "        return np.tanh(val)\n",
    "        # if act_type=='tanh':\n",
    "        #     return np.tanh(val)\n",
    "        # elif act_type=='relu':\n",
    "        #     return max(0.0, val)\n",
    "        # elif act_type=='sin':\n",
    "        #     return np.sin(val)\n",
    "        # elif act_type=='linear':\n",
    "        #     return val\n",
    "        # else:\n",
    "        #     return np.tanh(val)\n",
    "\n",
    "    def _update_layer(self, layer_idx, u):\n",
    "        \"\"\"\n",
    "        Update a single reservoir layer.\n",
    "        \"\"\"\n",
    "        pre_activation = self.reservoirs[layer_idx] @ self.states[layer_idx]\n",
    "        if layer_idx == 0:\n",
    "            pre_activation += self.input_weights[layer_idx] @ u\n",
    "        else:\n",
    "            pre_activation += self.input_weights[layer_idx] @ self.states[layer_idx - 1]\n",
    "\n",
    "        x_new = np.zeros_like(pre_activation)\n",
    "        for i in range(self.reservoir_size):\n",
    "            activation = self.node_activations[i]\n",
    "            x_new[i] = self._apply_activation(activation, pre_activation[i])\n",
    "        alpha = self.leaking_rate\n",
    "        self.states[layer_idx] = (1.0 - alpha) * self.states[layer_idx] + alpha * x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        all_states = []\n",
    "        for u in inputs:\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                self._update_layer(layer_idx, u)\n",
    "            all_states.append(np.concatenate(self.states))\n",
    "        all_states = np.array(all_states)\n",
    "        return all_states[discard:], all_states[:discard]\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Train the readout layer using ridge regression.\n",
    "        \"\"\"\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]\n",
    "\n",
    "        # Augment states with bias\n",
    "        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0], 1))])  # shape [T-discard, N*L+1]\n",
    "\n",
    "        # Quadratic readout\n",
    "        # Build augmented matrix [ x, x^2, 1 ]\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append( np.concatenate([s, s**2, [1.0]]) )\n",
    "        X_aug = np.array(X_list)                                    # shape [T-discard, 2N*L+1]\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_open_loop(self, inputs):\n",
    "        \"\"\"\n",
    "        Single-step-ahead inference on test data.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for u in inputs:\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                self._update_layer(layer_idx, u)\n",
    "            state = np.concatenate(self.states)\n",
    "            # x_aug = np.concatenate([state, [1.0]])\n",
    "            x_aug = np.concatenate([state, (state)**2, [1.0]])  # For quadrartic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, num_steps):\n",
    "        \"\"\"\n",
    "        Autoregressive multi-step forecasting for num_steps\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        current_input = initial_input.copy()\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                self._update_layer(layer_idx, current_input)\n",
    "            state = np.concatenate(self.states)\n",
    "            # x_aug = np.concatenate([state, [1.0]])\n",
    "            x_aug = np.concatenate([state, (state)**2, [1.0]])  # For quadrartic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "            current_input = out\n",
    "        \n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4741c6",
   "metadata": {},
   "source": [
    "# HHLR [v1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "817c595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def scale_spectral_radius(W, desired_radius=0.95):\n",
    "    \"\"\"Affine-scale square matrix W so that its spectral radius equals desired_radius.\"\"\"\n",
    "    eigs = eigvals(W)\n",
    "    current_radius = np.max(np.abs(eigs))\n",
    "    if current_radius == 0:\n",
    "        raise ValueError(\"Spectral radius of W is zero.\")\n",
    "    return W * (desired_radius / current_radius)\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"[x, x^2, 1]   (same convention as in CycleReservoir3D).\"\"\"\n",
    "    return np.concatenate([x, x**2, [1.0]])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# HH-LR class\n",
    "# ---------------------------------------------------------------------\n",
    "class HHLobeReservoir1:\n",
    "    \"\"\"\n",
    "    Hemispherically-Hierarchical Lobe Reservoir (HH-LR).\n",
    "\n",
    "    Topology:\n",
    "      * 8 anatomical modules  —  L/R × {F, P, T, O}\n",
    "      * Intra-module: Watts–Strogatz small-world graphs (ring+rewire)\n",
    "      * Intra-hemisphere inter-lobe: distance-modulated shortcuts\n",
    "      * Inter-hemisphere (callosal): sparse homotopic bridges\n",
    "    \"\"\"\n",
    "\n",
    "    # --- anatomical bookkeeping -------------------------------------------------\n",
    "    _LOBES   = ['F', 'P', 'T', 'O']\n",
    "    _HEMIS   = ['L', 'R']\n",
    "    # rough 2-D centroids (arbitrary units) for distance computation\n",
    "    _CENTROIDS = {\n",
    "        ('L', 'F'): (-1.0,  1.0),\n",
    "        ('L', 'P'): (-1.0,  0.0),\n",
    "        ('L', 'T'): (-1.0, -1.0),\n",
    "        ('L', 'O'): (-1.0, -2.0),\n",
    "        ('R', 'F'): ( 1.0,  1.0),\n",
    "        ('R', 'P'): ( 1.0,  0.0),\n",
    "        ('R', 'T'): ( 1.0, -1.0),\n",
    "        ('R', 'O'): ( 1.0, -2.0),\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def __init__(self,\n",
    "                 reservoir_size=800,\n",
    "                 input_dim=128,\n",
    "                 spectral_radius=0.9,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 # small-world / shortcut hyper-parameters\n",
    "                 k_ring=8,\n",
    "                 p_rewire_frontal=0.30,\n",
    "                 p_rewire_other=0.10,\n",
    "                 P_lat=0.04,\n",
    "                 sigma=5.0,\n",
    "                 P_call=0.01,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        reservoir_size   : total neuron count (split equally across the 8 modules if not divisible)\n",
    "        input_dim        : dimensionality of input vector u_t  (128 for EEG band-power features)\n",
    "        leaking_rate     : α in leaky-integrator update\n",
    "        \"\"\"\n",
    "        self.N     = reservoir_size\n",
    "        self.D_in  = input_dim\n",
    "        self.rho   = spectral_radius\n",
    "        self.in_scale   = input_scale\n",
    "        self.alpha = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.seed  = seed\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 1) allocate neurons to the 8 modules as evenly as possible\n",
    "        # ------------------------------------------------------------------\n",
    "        base = self.N // 8\n",
    "        counts = [base] * 8\n",
    "        for i in range(self.N - base*8):\n",
    "            counts[i] += 1\n",
    "        self._module_slices = {}\n",
    "        idx0 = 0\n",
    "        for h in self._HEMIS:\n",
    "            for l in self._LOBES:\n",
    "                n = counts[len(self._module_slices)]\n",
    "                self._module_slices[(h, l)] = slice(idx0, idx0 + n)\n",
    "                idx0 += n\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 2) build adjacency matrix W according to HH-LR rules\n",
    "        # ------------------------------------------------------------------\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        W = np.zeros((self.N, self.N), dtype=float)\n",
    "\n",
    "        # 2.1 intra-module small-world wiring\n",
    "        for (h, l), sl in self._module_slices.items():\n",
    "            n_mod = sl.stop - sl.start\n",
    "            k = min(k_ring, n_mod-1)  # guard tiny modules\n",
    "            p_rewire = p_rewire_frontal if l == 'F' else p_rewire_other\n",
    "            # ring lattice\n",
    "            for i_local in range(n_mod):\n",
    "                for m in range(1, k+1):\n",
    "                    j_local = (i_local + m) % n_mod\n",
    "                    i_glob = sl.start + i_local\n",
    "                    j_glob = sl.start + j_local\n",
    "                    W[i_glob, j_glob] = rng.standard_normal()\n",
    "                    W[j_glob, i_glob] = rng.standard_normal()\n",
    "            # rewire each existing edge with prob p_rewire\n",
    "            for i_local in range(n_mod):\n",
    "                for m in range(1, k+1):\n",
    "                    if rng.random() < p_rewire:\n",
    "                        # pick a random new target in same module (avoid self-loop)\n",
    "                        j_local_new = rng.integers(0, n_mod-1)\n",
    "                        if j_local_new >= i_local:\n",
    "                            j_local_new += 1\n",
    "                        i_glob = sl.start + i_local\n",
    "                        j_glob_new = sl.start + j_local_new\n",
    "                        # overwrite previous weight (both directions)\n",
    "                        w_new = rng.standard_normal()\n",
    "                        W[i_glob, j_glob_new] = w_new\n",
    "                        W[j_glob_new, i_glob] = rng.standard_normal()\n",
    "\n",
    "        # 2.2 intra-hemisphere inter-lobe shortcuts (distance-weighted)\n",
    "        for h in self._HEMIS:\n",
    "            for l1 in self._LOBES:\n",
    "                for l2 in self._LOBES:\n",
    "                    if l1 == l2:\n",
    "                        continue\n",
    "                    sl1 = self._module_slices[(h, l1)]\n",
    "                    sl2 = self._module_slices[(h, l2)]\n",
    "                    c1 = np.array(self._CENTROIDS[(h, l1)])\n",
    "                    c2 = np.array(self._CENTROIDS[(h, l2)])\n",
    "                    dist = np.linalg.norm(c1 - c2)\n",
    "                    p_edge = P_lat * np.exp(-dist / sigma)\n",
    "                    for i in range(sl1.start, sl1.stop):\n",
    "                        mask = rng.random(sl2.stop - sl2.start) < p_edge\n",
    "                        js = np.nonzero(mask)[0] + sl2.start\n",
    "                        if js.size:\n",
    "                            W[i, js] = rng.standard_normal(size=js.size)\n",
    "                            W[js, i] = rng.standard_normal(size=js.size)\n",
    "\n",
    "        # 2.3 inter-hemisphere callosal bridges (homotopic)\n",
    "        for l in self._LOBES:\n",
    "            sl_L = self._module_slices[('L', l)]\n",
    "            sl_R = self._module_slices[('R', l)]\n",
    "            for i in range(sl_L.start, sl_L.stop):\n",
    "                mask = rng.random(sl_R.stop - sl_R.start) < P_call\n",
    "                js = np.nonzero(mask)[0] + sl_R.start\n",
    "                if js.size:\n",
    "                    W[i, js] = rng.standard_normal(size=js.size)\n",
    "                    W[js, i] = rng.standard_normal(size=js.size)\n",
    "\n",
    "        # 2.4 spectral scaling\n",
    "        W = scale_spectral_radius(W, self.rho)\n",
    "        self.W = W\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 3) random input weights\n",
    "        # ------------------------------------------------------------------\n",
    "        rng = np.random.default_rng(self.seed + 1)\n",
    "        self.W_in = (rng.random((self.N, self.D_in)) - 0.5) * 2.0 * self.in_scale\n",
    "\n",
    "        # readout and state\n",
    "        self.W_out = None\n",
    "        self.x = np.zeros(self.N)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # ESN core methods (same signatures as CycleReservoir3D)\n",
    "    # ------------------------------------------------------------------\n",
    "    def reset_state(self):\n",
    "        self.x = np.zeros(self.N)\n",
    "\n",
    "    def _update(self, u):\n",
    "        pre_activation = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.tanh(pre_activation)\n",
    "        self.x = (1.0 - self.alpha) * self.x + self.alpha * x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs  : iterable / array of shape (T, input_dim)\n",
    "        discard : number of initial time-steps to omit from training\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for u in inputs:\n",
    "            self._update(u)\n",
    "            states.append(self.x.copy())\n",
    "        return np.array(states[discard:]), np.array(states[:discard])\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Ridge regression read-out; identical augmentation as baseline.\n",
    "        \"\"\"\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]\n",
    "        X_aug = np.vstack([augment_state_with_squares(s) for s in states_use])\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_sequence(self, inputs):\n",
    "        \"\"\"\n",
    "        Feed-forward prediction (no teacher forcing).  Suitable for classification:\n",
    "        returns raw linear outputs; apply threshold/sigmoid externally.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        self.reset_state()\n",
    "        for u in inputs:\n",
    "            self._update(u)\n",
    "            preds.append(self.W_out @ augment_state_with_squares(self.x)) # quadratic lift-off\n",
    "            #preds.append(self.W_out @ self.x)\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        preds = []\n",
    "        current_in = np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            out = self.W_out @ augment_state_with_squares(self.x)\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba81542",
   "metadata": {},
   "source": [
    "# HHLR[v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3977813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Utility helpers\n",
    "# ---------------------------------------------------------------------\n",
    "def scale_spectral_radius(W, desired_radius=0.95):\n",
    "    \"\"\"Affine-scale W so that its spectral radius equals desired_radius.\"\"\"\n",
    "    eigs = eigvals(W)\n",
    "    radius = np.max(np.abs(eigs))\n",
    "    if radius == 0:\n",
    "        raise ValueError(\"Spectral radius of W is zero.\")\n",
    "    return W * (desired_radius / radius)\n",
    "\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"Return [x, x², 1] (same convention as in CycleReservoir3D).\"\"\"\n",
    "    return np.concatenate([x, x**2, [1.0]])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Hemispherically–Hierarchical Lobe Reservoir\n",
    "# ---------------------------------------------------------------------\n",
    "class HHLobeReservoir2:\n",
    "    \"\"\"\n",
    "    Hemispherically-Hierarchical Lobe Reservoir (HH-LR)\n",
    "\n",
    "    • 8 anatomical modules  —  L/R × {F, P, T, O}\n",
    "    • Intra-module: Watts–Strogatz small-world graphs (ring + rewire)\n",
    "    • Intra-hemisphere shortcuts: distance-modulated (exp decay)\n",
    "    • Inter-hemisphere bridges: sparse homotopic callosal links\n",
    "    • Lobe cardinalities follow MRI volume ratio 4 : 3 : 2 : 1\n",
    "    \"\"\"\n",
    "\n",
    "    # bookkeeping ------------------------------------------------------\n",
    "    _LOBES  = ['F', 'P', 'T', 'O']\n",
    "    _HEMIS  = ['L', 'R']\n",
    "    _CENTROIDS = {                             # rough 2-D montage coords\n",
    "        ('L', 'F'): (-1.0,  1.0), ('L', 'P'): (-1.0,  0.0),\n",
    "        ('L', 'T'): (-1.0, -1.0), ('L', 'O'): (-1.0, -2.0),\n",
    "        ('R', 'F'): ( 1.0,  1.0), ('R', 'P'): ( 1.0,  0.0),\n",
    "        ('R', 'T'): ( 1.0, -1.0), ('R', 'O'): ( 1.0, -2.0),\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def __init__(self,\n",
    "                 reservoir_size=800,\n",
    "                 input_dim=32,\n",
    "                 spectral_radius=0.9,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 # small-world / shortcut hyper-parameters\n",
    "                 k_ring=8,\n",
    "                 p_rewire_frontal=0.30,\n",
    "                 p_rewire_other=0.10,\n",
    "                 P_lat=0.04,\n",
    "                 sigma=5.0,\n",
    "                 P_call=0.01,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        reservoir_size : total number of neurons (N)\n",
    "        input_dim      : dimension of u_t  (128 for EEG band-power features)\n",
    "        leaking_rate   : α in leaky-integrator update\n",
    "        \"\"\"\n",
    "        self.N          = reservoir_size\n",
    "        self.D_in       = input_dim\n",
    "        self.rho        = spectral_radius\n",
    "        self.in_scale   = input_scale\n",
    "        self.alpha      = leaking_rate\n",
    "        self.ridge_alpha= ridge_alpha\n",
    "        self.seed       = seed\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 1) allocate neurons:  per-hemisphere 4 : 3 : 2 : 1 (F:P:T:O)\n",
    "        # --------------------------------------------------------------\n",
    "        ratio = {'F': 4, 'P': 3, 'T': 2, 'O': 1}\n",
    "        total_w = sum(ratio.values())          # = 10\n",
    "        half_N  = self.N // 2                  # neurons per hemisphere\n",
    "\n",
    "        counts_per_hemi = {\n",
    "            l: int(half_N * ratio[l] / total_w) for l in self._LOBES\n",
    "        }\n",
    "\n",
    "        # distribute rounding residuals (largest fractional remainder first)\n",
    "        residual = half_N - sum(counts_per_hemi.values())\n",
    "        if residual > 0:\n",
    "            remainders = sorted(\n",
    "                self._LOBES,\n",
    "                key=lambda l: (half_N * ratio[l] / total_w) - counts_per_hemi[l],\n",
    "                reverse=True\n",
    "            )\n",
    "            for l in remainders[:residual]:\n",
    "                counts_per_hemi[l] += 1\n",
    "\n",
    "        # build slice table\n",
    "        self._module_slices = {}\n",
    "        idx0 = 0\n",
    "        for h in self._HEMIS:          # L then R\n",
    "            for l in self._LOBES:      # F, P, T, O\n",
    "                n = counts_per_hemi[l]\n",
    "                self._module_slices[(h, l)] = slice(idx0, idx0 + n)\n",
    "                idx0 += n\n",
    "        assert idx0 == self.N, \"Slice allocation error\"\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 2) construct adjacency matrix W according to HH-LR rules\n",
    "        # --------------------------------------------------------------\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        W = np.zeros((self.N, self.N), dtype=float)\n",
    "\n",
    "        # 2.1 intra-module small-world wiring\n",
    "        for (h, l), sl in self._module_slices.items():\n",
    "            n_mod = sl.stop - sl.start\n",
    "            k = min(k_ring, n_mod - 1)\n",
    "            p_rewire = p_rewire_frontal if l == 'F' else p_rewire_other\n",
    "\n",
    "            # ring lattice\n",
    "            for i_local in range(n_mod):\n",
    "                for m in range(1, k + 1):\n",
    "                    j_local = (i_local + m) % n_mod\n",
    "                    i_glob  = sl.start + i_local\n",
    "                    j_glob  = sl.start + j_local\n",
    "                    W[i_glob, j_glob] = rng.standard_normal()\n",
    "                    W[j_glob, i_glob] = rng.standard_normal()\n",
    "\n",
    "            # random rewiring\n",
    "            for i_local in range(n_mod):\n",
    "                for m in range(1, k + 1):\n",
    "                    if rng.random() < p_rewire:\n",
    "                        j_local_new = rng.integers(0, n_mod - 1)\n",
    "                        if j_local_new >= i_local:\n",
    "                            j_local_new += 1\n",
    "                        i_glob = sl.start + i_local\n",
    "                        j_glob_new = sl.start + j_local_new\n",
    "                        w_new = rng.standard_normal()\n",
    "                        W[i_glob, j_glob_new] = w_new\n",
    "                        W[j_glob_new, i_glob] = rng.standard_normal()\n",
    "\n",
    "        # 2.2 intra-hemisphere distance-weighted shortcuts\n",
    "        for h in self._HEMIS:\n",
    "            for l1 in self._LOBES:\n",
    "                for l2 in self._LOBES:\n",
    "                    if l1 == l2:\n",
    "                        continue\n",
    "                    sl1 = self._module_slices[(h, l1)]\n",
    "                    sl2 = self._module_slices[(h, l2)]\n",
    "                    d = np.linalg.norm(\n",
    "                        np.array(self._CENTROIDS[(h, l1)]) -\n",
    "                        np.array(self._CENTROIDS[(h, l2)])\n",
    "                    )\n",
    "                    p_edge = P_lat * np.exp(-d / sigma)\n",
    "                    for i in range(sl1.start, sl1.stop):\n",
    "                        mask = rng.random(sl2.stop - sl2.start) < p_edge\n",
    "                        js = np.nonzero(mask)[0] + sl2.start\n",
    "                        if js.size:\n",
    "                            W[i, js] = rng.standard_normal(size=js.size)\n",
    "                            W[js, i] = rng.standard_normal(size=js.size)\n",
    "\n",
    "        # 2.3 inter-hemisphere homotopic callosal links\n",
    "        for l in self._LOBES:\n",
    "            sl_L = self._module_slices[('L', l)]\n",
    "            sl_R = self._module_slices[('R', l)]\n",
    "            for i in range(sl_L.start, sl_L.stop):\n",
    "                mask = rng.random(sl_R.stop - sl_R.start) < P_call\n",
    "                js = np.nonzero(mask)[0] + sl_R.start\n",
    "                if js.size:\n",
    "                    W[i, js] = rng.standard_normal(size=js.size)\n",
    "                    W[js, i] = rng.standard_normal(size=js.size)\n",
    "\n",
    "        # 2.4 spectral scaling to enforce ESP\n",
    "        self.W = scale_spectral_radius(W, self.rho)\n",
    "\n",
    "        # --------------------------------------------------------------\n",
    "        # 3) random input weights\n",
    "        # --------------------------------------------------------------\n",
    "        rng = np.random.default_rng(self.seed + 1)\n",
    "        self.W_in = (rng.random((self.N, self.D_in)) - 0.5) * 2.0 * self.in_scale\n",
    "\n",
    "        # initialize state & read-out\n",
    "        self.x = np.zeros(self.N)\n",
    "        self.W_out = None\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # ESN core methods --------------------------------------------------\n",
    "    # ------------------------------------------------------------------\n",
    "    def reset_state(self):\n",
    "        self.x.fill(0.0)\n",
    "\n",
    "    def _update(self, u):\n",
    "        pre = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.tanh(pre)\n",
    "        self.x = (1.0 - self.alpha) * self.x + self.alpha * x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for u in inputs:\n",
    "            self._update(u)\n",
    "            states.append(self.x.copy())\n",
    "        return np.array(states[discard:]), np.array(states[:discard])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # read-out ----------------------------------------------------------\n",
    "    # ------------------------------------------------------------------\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        states_use, _ = self.collect_states(train_input, discard)\n",
    "        targets_use = train_target[discard:]\n",
    "        X_aug = np.vstack([augment_state_with_squares(s) for s in states_use])\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_sequence(self, inputs):\n",
    "        preds = []\n",
    "        self.reset_state()\n",
    "        for u in inputs:\n",
    "            self._update(u)\n",
    "            preds.append(self.W_out @ augment_state_with_squares(self.x))\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        preds = []\n",
    "        current_in = np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            out = self.W_out @ augment_state_with_squares(self.x)\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f6038",
   "metadata": {},
   "source": [
    "### NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55e0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons\n",
    "    for teacher-forced single-step forecasting or autoregressive rollout.\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets) ** 2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        variance[variance == 0] = 1e-8  # avoid divide-by-zero\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * np.sum(variance)))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9a21c",
   "metadata": {},
   "source": [
    "### VPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193a03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, test_time, lyapunov_time, threshold=0.4):\n",
    "    y_mean = np.mean(y_true, axis=0)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))\n",
    "\n",
    "    error = y_true - y_pred\n",
    "    squared_error = np.sum(error**2, axis=1)\n",
    "    delta = squared_error / denom\n",
    "\n",
    "    idx_exceed = np.where(delta > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = test_time[-1]\n",
    "    else:\n",
    "        T_VPT = test_time[idx_exceed[0]]\n",
    "\n",
    "    ratio = T_VPT / lyapunov_time\n",
    "\n",
    "    return T_VPT, ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e8e78",
   "metadata": {},
   "source": [
    "### ADev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38f84534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c2de7",
   "metadata": {},
   "source": [
    "### PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "402a5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(y, dt=0.01):\n",
    "    z = y[:, 2]  # Extract Z-component\n",
    "    \n",
    "    # Compute PSD using Welch’s method\n",
    "    freqs, psd = welch(z, fs=1/dt, window='hamming', nperseg=len(z))  # Using Hamming window\n",
    "    \n",
    "    return freqs, psd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b69b157",
   "metadata": {},
   "source": [
    "# BCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07af946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5df01577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape:  (15000, 3)\n",
      "Train target shape: (15000, 3)\n",
      "Test input shape:   (10000, 3)\n",
      "Test target shape:  (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb\n",
    "\n",
    "# ─── Load Data ─────────────────────────────────────────────────────────────\n",
    "data_dir = 'datasets/BCI_Competion4_dataset4'\n",
    "comp_path = os.path.join(data_dir, 'sub1_comp.mat')\n",
    "\n",
    "comp_data = sio.loadmat(comp_path)\n",
    "X_all = comp_data['train_data']       # shape: (timepoints × channels)\n",
    "Y_all = comp_data['train_dg']         # shape: (timepoints × 5)\n",
    "\n",
    "# ─── Parameters ────────────────────────────────────────────────────────────\n",
    "N_train = 15000\n",
    "N_test = 10000\n",
    "emb_dim = 3\n",
    "\n",
    "# ─── Select and Normalize First Channel ────────────────────────────────────\n",
    "u = X_all[:, 0]\n",
    "u_norm = (u - np.min(u)) / (np.max(u) - np.min(u))\n",
    "\n",
    "v = Y_all[:, 0]\n",
    "v_norm = (v - np.min(v)) / (np.max(v) - np.min(v))\n",
    "\n",
    "# ─── Delay Embed Input Signal ──────────────────────────────────────────────\n",
    "inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "\n",
    "# ─── Delay Embed First Finger Flexion ──────────────────────────────────────\n",
    "targets = create_delay_embedding(v, emb_dim)\n",
    "\n",
    "# ─── Split Train/Test ──────────────────────────────────────────────────────\n",
    "train_input = inputs[:N_train]\n",
    "train_target = targets[:N_train]\n",
    "test_input = inputs[N_train+1:N_train+1+N_test]\n",
    "test_target = targets[N_train+1:N_train+1+N_test]\n",
    "\n",
    "# ─── Summary ───────────────────────────────────────────────────────────────\n",
    "print(f\"Train input shape:  {train_input.shape}\")\n",
    "print(f\"Train target shape: {train_target.shape}\")\n",
    "print(f\"Test input shape:   {test_input.shape}\")\n",
    "print(f\"Test target shape:  {test_target.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a486e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    hhlr = HHLobeReservoir1(\n",
    "        reservoir_size=500,\n",
    "        input_dim=emb_dim,\n",
    "        spectral_radius=0.99,\n",
    "        input_scale=0.001,\n",
    "        leaking_rate=0.3,\n",
    "        ridge_alpha=1e-07,\n",
    "        k_ring=30,\n",
    "        p_rewire_frontal=0.60,\n",
    "        p_rewire_other=0.10,\n",
    "        P_lat=0.04,\n",
    "        sigma=5.0,\n",
    "        P_call=0.01,\n",
    "        seed=seed\n",
    "    )\n",
    "    hhlr.fit_readout(train_input, train_target, discard=5000)\n",
    "    hhlr_preds = hhlr.predict_sequence(test_input)\n",
    "    hhlr_nrmse = evaluate_nrmse(hhlr_preds, test_target, horizons)\n",
    "    nrmse_dict['HHLR[v1]'].append(hhlr_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "82f49314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Horizon   HHLR[v1]          \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300       1.2056 ± 0.0313   \n",
      "600       1.2795 ± 0.0220   \n",
      "1000      1.4193 ± 0.0097   \n"
     ]
    }
   ],
   "source": [
    "model_names = ['HHLR[v1]']\n",
    "\n",
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "header = \"Horizon\".ljust(10) + \"\".join([model.ljust(18) for model in model_names])\n",
    "print(header)\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    row = f\"{str(horizon):<10}\"\n",
    "    for model in model_names:\n",
    "        model_vals = [np.mean(run[horizon]) for run in nrmse_dict[model]]\n",
    "        mean = np.mean(model_vals)\n",
    "        std = np.std(model_vals)\n",
    "        row += f\"{mean:.4f} ± {std:.4f}\".ljust(18)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515dff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[32m      2\u001b[39m     esn = BaselineESN3D(\n\u001b[32m      3\u001b[39m         reservoir_size=\u001b[32m500\u001b[39m,\n\u001b[32m      4\u001b[39m         input_dim=\u001b[32m62\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m         seed=seed\n\u001b[32m     11\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mesn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_readout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscard\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     esn_preds = esn.predict(X_test)\n\u001b[32m     14\u001b[39m     esn_nrmse = evaluate_nrmse(esn_preds, Y_test, all_horizons)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mBaselineESN3D.fit_readout\u001b[39m\u001b[34m(self, train_input, train_target, discard)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_readout\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_input, train_target, discard=\u001b[32m100\u001b[39m):\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    Teacher forcing for single-step:\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m        - input(t) = [x(t), y(t), z(t)]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m \u001b[33;03m        W_out * [x(t); x²(t); 1] ~ target(t+1).\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     states_use, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscard\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiscard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     targets_use = train_target[discard:]  \u001b[38;5;66;03m# shape [T-discard, 3]\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# Augment states with bias\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# X_aug = np.hstack([states_use, np.ones((states_use.shape[0], 1))])  # shape [T-discard, N+1]\u001b[39;00m\n\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# Quadratic readout\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# Build augmented matrix [ x, x^2, 1 ]\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mBaselineESN3D.collect_states\u001b[39m\u001b[34m(self, inputs, discard)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)):\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m._update(inputs[t])\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     states.append(\u001b[38;5;28mself\u001b[39m.x.copy())\n\u001b[32m     84\u001b[39m states = np.array(states)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m states[discard:], states[:discard]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    esn = ESN3D(\n",
    "        reservoir_size=500,\n",
    "        input_dim=62,\n",
    "        spectral_radius=0.95,\n",
    "        connectivity=0.05,\n",
    "        input_scale=0.2,\n",
    "        leaking_rate=0.8,\n",
    "        ridge_alpha=1e-6,\n",
    "        seed=seed\n",
    "    )\n",
    "    esn.fit_readout(train_input, train_target, discard=5000)\n",
    "    esn_preds = esn.predict(test_target)\n",
    "    esn_nrmse = evaluate_nrmse(esn_preds, Y_test, all_horizons)\n",
    "    nrmse_dict['ESN'].append(esn_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seed in seeds:\n",
    "#     cycle_res = CycleReservoir3D(\n",
    "#         reservoir_size=500,\n",
    "#         input_dim=62,\n",
    "#         cycle_weight = 0.8,\n",
    "#         spectral_radius=0.95,\n",
    "#         input_scale=0.2,\n",
    "#         leaking_rate=0.8,\n",
    "#         ridge_alpha=1e-6,\n",
    "#         seed=seed\n",
    "#     )\n",
    "#     cycle_res.fit_readout(X_train, Y_train, discard=5000)\n",
    "#     cycle_res_preds = cycle_res.predict(X_test)\n",
    "#     cycle_res_nrmse = evaluate_nrmse(cycle_res_preds, Y_test, all_horizons)\n",
    "#     nrmse_dict['SCR'].append(cycle_res_nrmse)\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     crj = CRJRes3D(\n",
    "#         reservoir_size=500,\n",
    "#         input_dim=62,\n",
    "#         edge_weight=0.8,\n",
    "#         jump=15,\n",
    "#         spectral_radius=0.95,\n",
    "#         input_scale=0.2,\n",
    "#         leaking_rate=0.8,\n",
    "#         ridge_alpha=1e-6,\n",
    "#         seed=seed\n",
    "#     )\n",
    "#     crj.fit_readout(X_train, Y_train, discard=5000)\n",
    "#     crj_preds = crj.predict(X_test)\n",
    "#     crj_nrmse = evaluate_nrmse(crj_preds, Y_test, all_horizons)\n",
    "#     nrmse_dict['CRJ'].append(crj_nrmse)\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     mci_esn = MCIESN3D(\n",
    "#         reservoir_size=250,\n",
    "#         input_dim=62,\n",
    "#         cycle_weight=0.8,\n",
    "#         connect_weight=0.8,\n",
    "#         combine_factor=0.1,\n",
    "#         v1=0.03,\n",
    "#         v2=0.03,\n",
    "#         spectral_radius=0.95,\n",
    "#         leaking_rate=0.8,\n",
    "#         ridge_alpha=1e-6,\n",
    "#         seed=seed\n",
    "#     )\n",
    "#     mci_esn.fit_readout(X_train, Y_train, discard=5000)\n",
    "#     mci_esn_preds = mci_esn.predict(X_test)\n",
    "#     mci_esn_nrmse = evaluate_nrmse(mci_esn_preds, Y_test, all_horizons)\n",
    "#     nrmse_dict['MCI-ESN'].append(mci_esn_nrmse)\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     deepesn = DeepESN3D(\n",
    "#         num_layers=5,\n",
    "#         reservoir_size=100,\n",
    "#         input_dim=62,\n",
    "#         spectral_radius=0.95,\n",
    "#         input_scale=0.2,\n",
    "#         leaking_rate=0.8,\n",
    "#         ridge_alpha=1e-6,\n",
    "#         seed=seed\n",
    "#     )\n",
    "#     deepesn.fit_readout(X_train, Y_train, discard=5000)\n",
    "#     deepesn_preds = deepesn.predict(X_test)\n",
    "#     deepesn_nrmse = evaluate_nrmse(deepesn_preds, Y_test, all_horizons)\n",
    "#     nrmse_dict['DeepESN'].append(deepesn_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c11af2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('BCI Baseline results/nrmse_dict_BCI2.pkl', 'wb') as f:\n",
    "#     pickle.dump(nrmse_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb859b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
