{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a722d1",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e605f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib.colors import Normalize\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import welch\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac8933",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf80a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_spectral_radius(W, target_radius=0.95):\n",
    "    \"\"\"\n",
    "    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(W)\n",
    "    radius = np.max(np.abs(eigvals))\n",
    "    if radius == 0:\n",
    "        return W\n",
    "    return (W / radius) * target_radius\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"\n",
    "    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n",
    "    We'll use this for both training and prediction.\n",
    "    \"\"\"\n",
    "    x_sq = x**2\n",
    "    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4741c6",
   "metadata": {},
   "source": [
    "# HFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817c595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFRRes3D:\n",
    "    \"\"\"\n",
    "    Hierarchical Fractal Reservoir (HFR) for 3D chaotic systems.\n",
    "    \n",
    "    This novel reservoir architecture partitions the chaotic attractor at multiple\n",
    "    hierarchical scales, combining them in a fractal-like adjacency structure.\n",
    "    The method is model-free, relying solely on the observed trajectory in R^3,\n",
    "    and does not require knowledge of any system parameters such as sigma, rho, beta\n",
    "    for Lorenz63. \n",
    "    \n",
    "    Key Idea:\n",
    "     1) Define multiple 'scales' of partition of the data's bounding region.\n",
    "     2) Each scale is subdivided into a certain number of cells (regions).\n",
    "     3) Each cell at level l has links to both:\n",
    "        - other cells at the same level (horizontal adjacency),\n",
    "        - 'child' cells at the finer level l+1 (vertical adjacency).\n",
    "     4) We gather all cells across levels => a multi-level fractal graph => adjacency => W.\n",
    "     5) We build a typical ESN from this adjacency, feed data with W_in, run leaky tanh updates,\n",
    "        then do a polynomial readout for 3D next-step prediction.\n",
    "\n",
    "    This approach is suitable for chaotic systems whose attractors often exhibit fractal\n",
    "    self-similarity, thus capturing multi-scale structures in a single reservoir.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_levels=3,             # number of hierarchical levels\n",
    "                 cells_per_level=None,   # list of number of cells at each level, e.g. [8, 32, 128]\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_levels       : int, number of hierarchical scales\n",
    "        cells_per_level: list[int], the number of partitions/cells at each level\n",
    "                         if None, we auto-generate e.g. 2^(level+2)\n",
    "        spectral_radius: final scaling for adjacency\n",
    "        input_scale    : random input scale W_in\n",
    "        leaking_rate   : ESN leaky alpha\n",
    "        ridge_alpha    : readout ridge penalty\n",
    "        seed           : random seed\n",
    "        \"\"\"\n",
    "        self.n_levels        = n_levels\n",
    "        self.cells_per_level = cells_per_level\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale     = input_scale\n",
    "        self.leaking_rate    = leaking_rate\n",
    "        self.ridge_alpha     = ridge_alpha\n",
    "        self.seed            = seed\n",
    "\n",
    "        if self.cells_per_level is None:\n",
    "            # default scheme e.g. 8, 16, 32 for 3 levels\n",
    "            self.cells_per_level = [8*(2**i) for i in range(n_levels)]\n",
    "\n",
    "        # We'll store adjacency W, input W_in, readout W_out, reservoir state x\n",
    "        self.W     = None\n",
    "        self.W_in  = None\n",
    "        self.W_out = None\n",
    "        self.x     = None\n",
    "        self.n_levels = len(self.cells_per_level)\n",
    "\n",
    "        # We'll define a total number of nodes = sum(cells_per_level)\n",
    "        self.n_nodes = sum(self.cells_per_level)\n",
    "\n",
    "    def _build_partitions(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build hierarchical partitions for each level.\n",
    "        We'll store the bounding box for data_3d, then for each level l in [0..n_levels-1]\n",
    "        run e.g. k-means with K = cells_per_level[l], each point gets a label => we track transitions.\n",
    "\n",
    "        Return: \n",
    "          partitions => list of arrays, partitions[l] => shape (N, ) cluster assignment in [0..cells_per_level[l]-1]\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        N = len(data_3d)\n",
    "        partitions = []\n",
    "\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            # cluster\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.seed+10*level, n_init='auto')\n",
    "            kmeans.fit(data_3d)\n",
    "            labels = kmeans.predict(data_3d)\n",
    "            partitions.append(labels)\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def _build_hierarchical_adjacency(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build a block adjacency with cross-level links, then scale spectral radius.\n",
    "        Steps:\n",
    "          1) Build partitions for each level => partitions[l] in [0..cells_per_level[l]-1]\n",
    "          2) For each level l, build a transition matrix T_l of shape (cells_per_level[l], cells_per_level[l]).\n",
    "          3) Link scale l to scale l+1 by figuring out which cluster i at scale l maps to which cluster j at scale l+1\n",
    "             for each sample t => link i-> j if data_3d[t] is in i at scale l and j at scale l+1.\n",
    "          4) Combine all transitions in one big adjacency W in R^(n_nodes x n_nodes).\n",
    "          5) row-normalize W => scale largest eigenvalue => spectral_radius\n",
    "        \"\"\"\n",
    "        partitions = self._build_partitions(data_3d)\n",
    "        N = len(data_3d)\n",
    "\n",
    "        # offsets for each level => to index big W\n",
    "        offsets = []\n",
    "        running = 0\n",
    "        for level in range(self.n_levels):\n",
    "            offsets.append(running)\n",
    "            running += self.cells_per_level[level]\n",
    "\n",
    "        # total nodes\n",
    "        n_tot = self.n_nodes\n",
    "        # initialize adjacency\n",
    "        A = np.zeros((n_tot, n_tot))\n",
    "\n",
    "        # 1) horizontal adjacency in each level\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            labels = partitions[level]\n",
    "            # T_l => shape (k, k)\n",
    "            T_l = np.zeros((k, k))\n",
    "            for t in range(N-1):\n",
    "                i = labels[t]\n",
    "                j = labels[t+1]\n",
    "                T_l[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = T_l.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            T_l /= row_sum\n",
    "            # place T_l into big A\n",
    "            off = offsets[level]\n",
    "            A[off:off+k, off:off+k] = T_l\n",
    "\n",
    "        # 2) vertical adjacency between scale l and l+1\n",
    "        for level in range(self.n_levels-1):\n",
    "            k_l   = self.cells_per_level[level]\n",
    "            k_lp1 = self.cells_per_level[level+1]\n",
    "            labels_l   = partitions[level]\n",
    "            labels_lp1 = partitions[level+1]\n",
    "            # we define adjacency from i in [0..k_l-1] to j in [0..k_lp1-1] if the same sample t belongs to i at level l and j at l+1\n",
    "            # Count how many times\n",
    "            Xvert1 = np.zeros((k_l, k_lp1))\n",
    "            for t in range(N):\n",
    "                i = labels_l[t]\n",
    "                j = labels_lp1[t]\n",
    "                Xvert1[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = Xvert1.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            Xvert = Xvert1/row_sum\n",
    "            # place in big A\n",
    "            off_l   = offsets[level]\n",
    "            off_lp1 = offsets[level+1]\n",
    "            A[off_l:off_l+k_l, off_lp1:off_lp1+k_lp1] = Xvert\n",
    "            # tentative idea, we could also define adjacency from l+1 -> l (parent link), if desired\n",
    "            # we do the same for the 'child -> parent' link or skip it if we only want forward adjacency\n",
    "            # For now, let's do symmetrical\n",
    "            Yvert = Xvert1.T\n",
    "            col_sum = Yvert.sum(axis=1, keepdims=True)\n",
    "            col_sum[col_sum==0.0] = 1.0\n",
    "            Yvert /= col_sum\n",
    "            A[off_lp1:off_lp1+k_lp1, off_l:off_l+k_l] = Yvert\n",
    "\n",
    "        # now we have a big adjacency => row normalize again, then scale spectral radius\n",
    "        row_sum = A.sum(axis=1, keepdims=True)\n",
    "        row_sum[row_sum==0.0] = 1.0\n",
    "        A /= row_sum\n",
    "\n",
    "        A = scale_spectral_radius(A, self.spectral_radius)\n",
    "        return A\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Main training routine:\n",
    "          1) Build hierarchical adjacency from fractal partition => self.W\n",
    "          2) define W_in => shape(n_nodes, 3)\n",
    "          3) teacher forcing => polynomial readout => solve => self.W_out\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Build adjacency\n",
    "        W_big = self._build_hierarchical_adjacency(train_input)\n",
    "        self.W = W_big\n",
    "\n",
    "        # define W_in => shape(n_nodes,3)\n",
    "        self.n_nodes = W_big.shape[0]\n",
    "        self.W_in = (np.random.rand(self.n_nodes,3)-0.5)*2.0*self.input_scale\n",
    "\n",
    "        # define reservoir state\n",
    "        self.x = np.zeros(self.n_nodes)\n",
    "\n",
    "        # gather states => teacher forcing => polynomial => readout\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]\n",
    "        X_list= []\n",
    "        for s in states_use:\n",
    "            X_list.append( augment_state_with_squares(s) )\n",
    "        X_aug= np.array(X_list)\n",
    "\n",
    "        reg= Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        self.W_out= reg.coef_\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Teacher forcing => feed real 3D => gather states => shape => [T-discard, n_nodes].\n",
    "        returns (states_after_discard, states_discarded).\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states= []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states= np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.x is not None:\n",
    "            self.x.fill(0.0)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        x(t+1)= (1-alpha)x(t)+ alpha tanh( W*x(t)+ W_in*u(t) ).\n",
    "        \"\"\"\n",
    "        alpha= self.leaking_rate\n",
    "        pre_acts= self.W@self.x + self.W_in@u\n",
    "        x_new= np.tanh(pre_acts)\n",
    "        self.x= (1.0- alpha)*self.x+ alpha*x_new\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        fully autonomous => feed last predicted => next input\n",
    "        \"\"\"\n",
    "        preds= []\n",
    "        #self.reset_state()\n",
    "        current_in= np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            big_x= augment_state_with_squares(self.x)\n",
    "            out= self.W_out@big_x\n",
    "            preds.append(out)\n",
    "            current_in= out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f6038",
   "metadata": {},
   "source": [
    "### NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55e0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons for Teacher-forced Single-step Forecasting\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets)**2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * variance))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9a21c",
   "metadata": {},
   "source": [
    "### VPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193a03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, test_time, lyapunov_time, threshold=0.4):\n",
    "    y_mean = np.mean(y_true, axis=0)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))\n",
    "\n",
    "    error = y_true - y_pred\n",
    "    squared_error = np.sum(error**2, axis=1)\n",
    "    delta = squared_error / denom\n",
    "\n",
    "    idx_exceed = np.where(delta > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = test_time[-1]\n",
    "    else:\n",
    "        T_VPT = test_time[idx_exceed[0]]\n",
    "\n",
    "    ratio = T_VPT / lyapunov_time\n",
    "\n",
    "    return T_VPT, ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e8e78",
   "metadata": {},
   "source": [
    "### ADev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f84534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c2de7",
   "metadata": {},
   "source": [
    "### PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "402a5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(y, dt=0.01):\n",
    "    z = y[:, 2]  # Extract Z-component\n",
    "    \n",
    "    # Compute PSD using Welch’s method\n",
    "    freqs, psd = welch(z, fs=1/dt, window='hamming', nperseg=len(z))  # Using Hamming window\n",
    "    \n",
    "    return freqs, psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bbfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "218cb6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 10, 15, 20, 25, 30, 35, 40, 120], [5, 10, 15, 20, 25, 30, 35, 45, 115], [5, 10, 15, 20, 25, 30, 35, 50, 110], [5, 10, 15, 20, 25, 30, 35, 55, 105], [5, 10, 15, 20, 25, 30, 35, 60, 100], [5, 10, 15, 20, 25, 30, 35, 65, 95], [5, 10, 15, 20, 25, 30, 35, 70, 90], [5, 10, 15, 20, 25, 30, 35, 75, 85], [5, 10, 15, 20, 25, 30, 40, 45, 110], [5, 10, 15, 20, 25, 30, 40, 50, 105], [5, 10, 15, 20, 25, 30, 40, 55, 100], [5, 10, 15, 20, 25, 30, 40, 60, 95], [5, 10, 15, 20, 25, 30, 40, 65, 90], [5, 10, 15, 20, 25, 30, 40, 70, 85], [5, 10, 15, 20, 25, 30, 40, 75, 80], [5, 10, 15, 20, 25, 30, 45, 50, 100], [5, 10, 15, 20, 25, 30, 45, 55, 95], [5, 10, 15, 20, 25, 30, 45, 60, 90], [5, 10, 15, 20, 25, 30, 45, 65, 85], [5, 10, 15, 20, 25, 30, 45, 70, 80], [5, 10, 15, 20, 25, 30, 50, 55, 90], [5, 10, 15, 20, 25, 30, 50, 60, 85], [5, 10, 15, 20, 25, 30, 50, 65, 80], [5, 10, 15, 20, 25, 30, 50, 70, 75], [5, 10, 15, 20, 25, 30, 55, 60, 80], [5, 10, 15, 20, 25, 30, 55, 65, 75], [5, 10, 15, 20, 25, 30, 60, 65, 70], [5, 10, 15, 20, 25, 35, 40, 45, 105], [5, 10, 15, 20, 25, 35, 40, 50, 100], [5, 10, 15, 20, 25, 35, 40, 55, 95], [5, 10, 15, 20, 25, 35, 40, 60, 90], [5, 10, 15, 20, 25, 35, 40, 65, 85], [5, 10, 15, 20, 25, 35, 40, 70, 80], [5, 10, 15, 20, 25, 35, 45, 50, 95], [5, 10, 15, 20, 25, 35, 45, 55, 90], [5, 10, 15, 20, 25, 35, 45, 60, 85], [5, 10, 15, 20, 25, 35, 45, 65, 80], [5, 10, 15, 20, 25, 35, 45, 70, 75], [5, 10, 15, 20, 25, 35, 50, 55, 85], [5, 10, 15, 20, 25, 35, 50, 60, 80], [5, 10, 15, 20, 25, 35, 50, 65, 75], [5, 10, 15, 20, 25, 35, 55, 60, 75], [5, 10, 15, 20, 25, 35, 55, 65, 70], [5, 10, 15, 20, 25, 40, 45, 50, 90], [5, 10, 15, 20, 25, 40, 45, 55, 85], [5, 10, 15, 20, 25, 40, 45, 60, 80], [5, 10, 15, 20, 25, 40, 45, 65, 75], [5, 10, 15, 20, 25, 40, 50, 55, 80], [5, 10, 15, 20, 25, 40, 50, 60, 75], [5, 10, 15, 20, 25, 40, 50, 65, 70], [5, 10, 15, 20, 25, 40, 55, 60, 70], [5, 10, 15, 20, 25, 45, 50, 55, 75], [5, 10, 15, 20, 25, 45, 50, 60, 70], [5, 10, 15, 20, 25, 45, 55, 60, 65], [5, 10, 15, 20, 30, 35, 40, 45, 100], [5, 10, 15, 20, 30, 35, 40, 50, 95], [5, 10, 15, 20, 30, 35, 40, 55, 90], [5, 10, 15, 20, 30, 35, 40, 60, 85], [5, 10, 15, 20, 30, 35, 40, 65, 80], [5, 10, 15, 20, 30, 35, 40, 70, 75], [5, 10, 15, 20, 30, 35, 45, 50, 90], [5, 10, 15, 20, 30, 35, 45, 55, 85], [5, 10, 15, 20, 30, 35, 45, 60, 80], [5, 10, 15, 20, 30, 35, 45, 65, 75], [5, 10, 15, 20, 30, 35, 50, 55, 80], [5, 10, 15, 20, 30, 35, 50, 60, 75], [5, 10, 15, 20, 30, 35, 50, 65, 70], [5, 10, 15, 20, 30, 35, 55, 60, 70], [5, 10, 15, 20, 30, 40, 45, 50, 85], [5, 10, 15, 20, 30, 40, 45, 55, 80], [5, 10, 15, 20, 30, 40, 45, 60, 75], [5, 10, 15, 20, 30, 40, 45, 65, 70], [5, 10, 15, 20, 30, 40, 50, 55, 75], [5, 10, 15, 20, 30, 40, 50, 60, 70], [5, 10, 15, 20, 30, 40, 55, 60, 65], [5, 10, 15, 20, 30, 45, 50, 55, 70], [5, 10, 15, 20, 30, 45, 50, 60, 65], [5, 10, 15, 20, 35, 40, 45, 50, 80], [5, 10, 15, 20, 35, 40, 45, 55, 75], [5, 10, 15, 20, 35, 40, 45, 60, 70], [5, 10, 15, 20, 35, 40, 50, 55, 70], [5, 10, 15, 20, 35, 40, 50, 60, 65], [5, 10, 15, 20, 35, 45, 50, 55, 65], [5, 10, 15, 20, 40, 45, 50, 55, 60], [5, 10, 15, 25, 30, 35, 40, 45, 95], [5, 10, 15, 25, 30, 35, 40, 50, 90], [5, 10, 15, 25, 30, 35, 40, 55, 85], [5, 10, 15, 25, 30, 35, 40, 60, 80], [5, 10, 15, 25, 30, 35, 40, 65, 75], [5, 10, 15, 25, 30, 35, 45, 50, 85], [5, 10, 15, 25, 30, 35, 45, 55, 80], [5, 10, 15, 25, 30, 35, 45, 60, 75], [5, 10, 15, 25, 30, 35, 45, 65, 70], [5, 10, 15, 25, 30, 35, 50, 55, 75], [5, 10, 15, 25, 30, 35, 50, 60, 70], [5, 10, 15, 25, 30, 35, 55, 60, 65], [5, 10, 15, 25, 30, 40, 45, 50, 80], [5, 10, 15, 25, 30, 40, 45, 55, 75], [5, 10, 15, 25, 30, 40, 45, 60, 70], [5, 10, 15, 25, 30, 40, 50, 55, 70], [5, 10, 15, 25, 30, 40, 50, 60, 65], [5, 10, 15, 25, 30, 45, 50, 55, 65], [5, 10, 15, 25, 35, 40, 45, 50, 75], [5, 10, 15, 25, 35, 40, 45, 55, 70], [5, 10, 15, 25, 35, 40, 45, 60, 65], [5, 10, 15, 25, 35, 40, 50, 55, 65], [5, 10, 15, 25, 35, 45, 50, 55, 60], [5, 10, 15, 30, 35, 40, 45, 50, 70], [5, 10, 15, 30, 35, 40, 45, 55, 65], [5, 10, 15, 30, 35, 40, 50, 55, 60], [5, 10, 20, 25, 30, 35, 40, 45, 90], [5, 10, 20, 25, 30, 35, 40, 50, 85], [5, 10, 20, 25, 30, 35, 40, 55, 80], [5, 10, 20, 25, 30, 35, 40, 60, 75], [5, 10, 20, 25, 30, 35, 40, 65, 70], [5, 10, 20, 25, 30, 35, 45, 50, 80], [5, 10, 20, 25, 30, 35, 45, 55, 75], [5, 10, 20, 25, 30, 35, 45, 60, 70], [5, 10, 20, 25, 30, 35, 50, 55, 70], [5, 10, 20, 25, 30, 35, 50, 60, 65], [5, 10, 20, 25, 30, 40, 45, 50, 75], [5, 10, 20, 25, 30, 40, 45, 55, 70], [5, 10, 20, 25, 30, 40, 45, 60, 65], [5, 10, 20, 25, 30, 40, 50, 55, 65], [5, 10, 20, 25, 30, 45, 50, 55, 60], [5, 10, 20, 25, 35, 40, 45, 50, 70], [5, 10, 20, 25, 35, 40, 45, 55, 65], [5, 10, 20, 25, 35, 40, 50, 55, 60], [5, 10, 20, 30, 35, 40, 45, 50, 65], [5, 10, 20, 30, 35, 40, 45, 55, 60], [5, 10, 25, 30, 35, 40, 45, 50, 60], [5, 15, 20, 25, 30, 35, 40, 45, 85], [5, 15, 20, 25, 30, 35, 40, 50, 80], [5, 15, 20, 25, 30, 35, 40, 55, 75], [5, 15, 20, 25, 30, 35, 40, 60, 70], [5, 15, 20, 25, 30, 35, 45, 50, 75], [5, 15, 20, 25, 30, 35, 45, 55, 70], [5, 15, 20, 25, 30, 35, 45, 60, 65], [5, 15, 20, 25, 30, 35, 50, 55, 65], [5, 15, 20, 25, 30, 40, 45, 50, 70], [5, 15, 20, 25, 30, 40, 45, 55, 65], [5, 15, 20, 25, 30, 40, 50, 55, 60], [5, 15, 20, 25, 35, 40, 45, 50, 65], [5, 15, 20, 25, 35, 40, 45, 55, 60], [5, 15, 20, 30, 35, 40, 45, 50, 60], [5, 15, 25, 30, 35, 40, 45, 50, 55], [10, 15, 20, 25, 30, 35, 40, 45, 80], [10, 15, 20, 25, 30, 35, 40, 50, 75], [10, 15, 20, 25, 30, 35, 40, 55, 70], [10, 15, 20, 25, 30, 35, 40, 60, 65], [10, 15, 20, 25, 30, 35, 45, 50, 70], [10, 15, 20, 25, 30, 35, 45, 55, 65], [10, 15, 20, 25, 30, 35, 50, 55, 60], [10, 15, 20, 25, 30, 40, 45, 50, 65], [10, 15, 20, 25, 30, 40, 45, 55, 60], [10, 15, 20, 25, 35, 40, 45, 50, 60], [10, 15, 20, 30, 35, 40, 45, 50, 55]]\n",
      "Total unique combinations: 157\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "step = 5\n",
    "for a in range(5, 101, step):\n",
    "    for b in range(a + step, 111, step):\n",
    "        for c in range(b + step, 121, step):\n",
    "            for d in range(c + step, 131, step):\n",
    "                for e in range(d + step, 141, step):\n",
    "                    for f in range(e + step, 151, step):\n",
    "                        for g in range(f + step, 161, step):\n",
    "                            for h in range(g + step, 171, step):\n",
    "                                i = 300 - a - b - c - d - e - f - g - h\n",
    "                                if i > h and i % step == 0 and i <= 300:\n",
    "                                    result.append([a, b, c, d, e, f, g, h, i])\n",
    "\n",
    "print(result)\n",
    "print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2407cfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.53</td>\n",
       "      <td>8320</td>\n",
       "      <td>7771</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.53</td>\n",
       "      <td>8117</td>\n",
       "      <td>7774</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.15</td>\n",
       "      <td>7620</td>\n",
       "      <td>7788</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75.39</td>\n",
       "      <td>6413</td>\n",
       "      <td>7787</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.51</td>\n",
       "      <td>7518</td>\n",
       "      <td>7767</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>73.57</td>\n",
       "      <td>16021</td>\n",
       "      <td>6498</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>73.79</td>\n",
       "      <td>-6957</td>\n",
       "      <td>6547</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>74.54</td>\n",
       "      <td>11476</td>\n",
       "      <td>6576</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16998</th>\n",
       "      <td>74.36</td>\n",
       "      <td>15058</td>\n",
       "      <td>6573</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999</th>\n",
       "      <td>72.91</td>\n",
       "      <td>13510</td>\n",
       "      <td>6676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1     2   3\n",
       "0      76.53   8320  7771 NaN\n",
       "1      76.53   8117  7774 NaN\n",
       "2      76.15   7620  7788 NaN\n",
       "3      75.39   6413  7787 NaN\n",
       "4      75.51   7518  7767 NaN\n",
       "...      ...    ...   ...  ..\n",
       "16995  73.57  16021  6498 NaN\n",
       "16996  73.79  -6957  6547 NaN\n",
       "16997  74.54  11476  6576 NaN\n",
       "16998  74.36  15058  6573 NaN\n",
       "16999  72.91  13510  6676 NaN\n",
       "\n",
       "[17000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'datasets/santa-fe-time-series-competition-data-set-b-1.0.0/b1.txt'\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, sep=' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5624624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the first column (column 0) of the DataFrame\n",
    "df[0] = (df[0] - df[0].min()) / (df[0].max() - df[0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5bf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:7503, 0].values\n",
    "chosen_system = \"SantaFe\"\n",
    "dt = 1\n",
    "T_data = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Data length: {T_data}.\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 10000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}  \\nTest size: {len(test_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df4b7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"cells_per_level\": result,\n",
    "    \"spectral_radius\": [0.95],\n",
    "    \"input_scale\": [1.0],\n",
    "    \"leaking_rate\": [0.5],\n",
    "    \"ridge_alpha\": [1e-8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2f85de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    hfr = HFRRes3D(\n",
    "        n_levels=8,      \n",
    "        cells_per_level=[10, 15, 20, 25, 30, 40, 45, 55, 60],\n",
    "        spectral_radius=0.95,\n",
    "        input_scale=1,\n",
    "        leaking_rate=0.5,\n",
    "        ridge_alpha=1e-8,\n",
    "        seed=seed\n",
    "    )\n",
    "    hfr.fit_readout(train_input, train_target, discard=5000)\n",
    "    hfr_preds = hfr.predict_open_loop(test_input)\n",
    "    hfr_nrmse = evaluate_nrmse(hfr_preds, test_target, horizons)\n",
    "    nrmse_dict['HFR'].append(hfr_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60917d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    # Precompute param combinations\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    horizons = [300, 600, 1000]\n",
    "    seeds = range(995, 1025)\n",
    "    # tqdm adds a progress bar for better visualization\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores = []\n",
    "        for seed in seeds:\n",
    "            model = model_class(seed=seed, **params)\n",
    "            model.fit_readout(train_input, train_target, discard=5000)\n",
    "            preds = model.predict_open_loop(test_input)\n",
    "            nrmse = evaluate_nrmse(preds, test_target, horizons)\n",
    "            seed_scores.append(nrmse)\n",
    "\n",
    "        results.append({\n",
    "            \"params\": params,\n",
    "            \"scores\": seed_scores\n",
    "        })\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7df19d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Initial grid search for HFRRes3D with 157 combinations ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:  57%|█████▋    | 89/157 [47:47<36:30, 32.22s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mHFRRes3D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHFRRes3D\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhfr_grid_search_results.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mrun_grid_search\u001b[39m\u001b[34m(model_class, param_grid, model_name, output_path)\u001b[39m\n\u001b[32m     16\u001b[39m model = model_class(seed=seed, **params)\n\u001b[32m     17\u001b[39m model.fit_readout(train_input, train_target, discard=\u001b[32m5000\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_open_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m nrmse = evaluate_nrmse(preds, test_target, horizons)\n\u001b[32m     20\u001b[39m seed_scores.append(nrmse)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 248\u001b[39m, in \u001b[36mHFRRes3D.predict_open_loop\u001b[39m\u001b[34m(self, test_input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m true_input \u001b[38;5;129;01min\u001b[39;00m test_input:\n\u001b[32m    247\u001b[39m     \u001b[38;5;28mself\u001b[39m._update(true_input)\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     x_aug = \u001b[43maugment_state_with_squares\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.W_out @ x_aug\n\u001b[32m    250\u001b[39m     preds.append(out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36maugment_state_with_squares\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mGiven state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03mWe'll use this for both training and prediction.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m x_sq = x**\u001b[32m2\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\multiarray.py:180\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    empty_like(prototype, dtype=None, order='K', subok=True, shape=None, *,\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m               device=None)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m \n\u001b[32m    176\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m   \u001b[38;5;66;03m# NOQA\u001b[39;00m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (prototype,)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath.concatenate)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconcatenate\u001b[39m(arrays, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, *, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, casting=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m    concatenate(\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m        (a1, a2, ...),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m \n\u001b[32m    281\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    283\u001b[39m         \u001b[38;5;66;03m# optimize for the typical case where only arrays is provided\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "run_grid_search(\n",
    "    HFRRes3D,\n",
    "    grid,\n",
    "    model_name=\"HFRRes3D\",\n",
    "    output_path=\"hfr_grid_search_results.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
