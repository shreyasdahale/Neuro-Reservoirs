{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a722d1",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e605f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib.colors import Normalize\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import welch\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac8933",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf80a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_spectral_radius(W, target_radius=0.95):\n",
    "    \"\"\"\n",
    "    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(W)\n",
    "    radius = np.max(np.abs(eigvals))\n",
    "    if radius == 0:\n",
    "        return W\n",
    "    return (W / radius) * target_radius\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"\n",
    "    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n",
    "    We'll use this for both training and prediction.\n",
    "    \"\"\"\n",
    "    x_sq = x**2\n",
    "    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4741c6",
   "metadata": {},
   "source": [
    "# HFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817c595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFRRes3D:\n",
    "    \"\"\"\n",
    "    Hierarchical Fractal Reservoir (HFR) for 3D chaotic systems.\n",
    "    \n",
    "    This novel reservoir architecture partitions the chaotic attractor at multiple\n",
    "    hierarchical scales, combining them in a fractal-like adjacency structure.\n",
    "    The method is model-free, relying solely on the observed trajectory in R^3,\n",
    "    and does not require knowledge of any system parameters such as sigma, rho, beta\n",
    "    for Lorenz63. \n",
    "    \n",
    "    Key Idea:\n",
    "     1) Define multiple 'scales' of partition of the data's bounding region.\n",
    "     2) Each scale is subdivided into a certain number of cells (regions).\n",
    "     3) Each cell at level l has links to both:\n",
    "        - other cells at the same level (horizontal adjacency),\n",
    "        - 'child' cells at the finer level l+1 (vertical adjacency).\n",
    "     4) We gather all cells across levels => a multi-level fractal graph => adjacency => W.\n",
    "     5) We build a typical ESN from this adjacency, feed data with W_in, run leaky tanh updates,\n",
    "        then do a polynomial readout for 3D next-step prediction.\n",
    "\n",
    "    This approach is suitable for chaotic systems whose attractors often exhibit fractal\n",
    "    self-similarity, thus capturing multi-scale structures in a single reservoir.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_levels=3,             # number of hierarchical levels\n",
    "                 cells_per_level=None,   # list of number of cells at each level, e.g. [8, 32, 128]\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_levels       : int, number of hierarchical scales\n",
    "        cells_per_level: list[int], the number of partitions/cells at each level\n",
    "                         if None, we auto-generate e.g. 2^(level+2)\n",
    "        spectral_radius: final scaling for adjacency\n",
    "        input_scale    : random input scale W_in\n",
    "        leaking_rate   : ESN leaky alpha\n",
    "        ridge_alpha    : readout ridge penalty\n",
    "        seed           : random seed\n",
    "        \"\"\"\n",
    "        self.n_levels        = n_levels\n",
    "        self.cells_per_level = cells_per_level\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale     = input_scale\n",
    "        self.leaking_rate    = leaking_rate\n",
    "        self.ridge_alpha     = ridge_alpha\n",
    "        self.seed            = seed\n",
    "\n",
    "        if self.cells_per_level is None:\n",
    "            # default scheme e.g. 8, 16, 32 for 3 levels\n",
    "            self.cells_per_level = [8*(2**i) for i in range(n_levels)]\n",
    "\n",
    "        # We'll store adjacency W, input W_in, readout W_out, reservoir state x\n",
    "        self.W     = None\n",
    "        self.W_in  = None\n",
    "        self.W_out = None\n",
    "        self.x     = None\n",
    "        self.n_levels = len(self.cells_per_level)\n",
    "\n",
    "        # We'll define a total number of nodes = sum(cells_per_level)\n",
    "        self.n_nodes = sum(self.cells_per_level)\n",
    "\n",
    "    def _build_partitions(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build hierarchical partitions for each level.\n",
    "        We'll store the bounding box for data_3d, then for each level l in [0..n_levels-1]\n",
    "        run e.g. k-means with K = cells_per_level[l], each point gets a label => we track transitions.\n",
    "\n",
    "        Return: \n",
    "          partitions => list of arrays, partitions[l] => shape (N, ) cluster assignment in [0..cells_per_level[l]-1]\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        N = len(data_3d)\n",
    "        partitions = []\n",
    "\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            # cluster\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.seed+10*level, n_init='auto')\n",
    "            kmeans.fit(data_3d)\n",
    "            labels = kmeans.predict(data_3d)\n",
    "            partitions.append(labels)\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def _build_hierarchical_adjacency(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build a block adjacency with cross-level links, then scale spectral radius.\n",
    "        Steps:\n",
    "          1) Build partitions for each level => partitions[l] in [0..cells_per_level[l]-1]\n",
    "          2) For each level l, build a transition matrix T_l of shape (cells_per_level[l], cells_per_level[l]).\n",
    "          3) Link scale l to scale l+1 by figuring out which cluster i at scale l maps to which cluster j at scale l+1\n",
    "             for each sample t => link i-> j if data_3d[t] is in i at scale l and j at scale l+1.\n",
    "          4) Combine all transitions in one big adjacency W in R^(n_nodes x n_nodes).\n",
    "          5) row-normalize W => scale largest eigenvalue => spectral_radius\n",
    "        \"\"\"\n",
    "        partitions = self._build_partitions(data_3d)\n",
    "        N = len(data_3d)\n",
    "\n",
    "        # offsets for each level => to index big W\n",
    "        offsets = []\n",
    "        running = 0\n",
    "        for level in range(self.n_levels):\n",
    "            offsets.append(running)\n",
    "            running += self.cells_per_level[level]\n",
    "\n",
    "        # total nodes\n",
    "        n_tot = self.n_nodes\n",
    "        # initialize adjacency\n",
    "        A = np.zeros((n_tot, n_tot))\n",
    "\n",
    "        # 1) horizontal adjacency in each level\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            labels = partitions[level]\n",
    "            # T_l => shape (k, k)\n",
    "            T_l = np.zeros((k, k))\n",
    "            for t in range(N-1):\n",
    "                i = labels[t]\n",
    "                j = labels[t+1]\n",
    "                T_l[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = T_l.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            T_l /= row_sum\n",
    "            # place T_l into big A\n",
    "            off = offsets[level]\n",
    "            A[off:off+k, off:off+k] = T_l\n",
    "\n",
    "        # 2) vertical adjacency between scale l and l+1\n",
    "        for level in range(self.n_levels-1):\n",
    "            k_l   = self.cells_per_level[level]\n",
    "            k_lp1 = self.cells_per_level[level+1]\n",
    "            labels_l   = partitions[level]\n",
    "            labels_lp1 = partitions[level+1]\n",
    "            # we define adjacency from i in [0..k_l-1] to j in [0..k_lp1-1] if the same sample t belongs to i at level l and j at l+1\n",
    "            # Count how many times\n",
    "            Xvert1 = np.zeros((k_l, k_lp1))\n",
    "            for t in range(N):\n",
    "                i = labels_l[t]\n",
    "                j = labels_lp1[t]\n",
    "                Xvert1[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = Xvert1.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            Xvert = Xvert1/row_sum\n",
    "            # place in big A\n",
    "            off_l   = offsets[level]\n",
    "            off_lp1 = offsets[level+1]\n",
    "            A[off_l:off_l+k_l, off_lp1:off_lp1+k_lp1] = Xvert\n",
    "            # tentative idea, we could also define adjacency from l+1 -> l (parent link), if desired\n",
    "            # we do the same for the 'child -> parent' link or skip it if we only want forward adjacency\n",
    "            # For now, let's do symmetrical\n",
    "            Yvert = Xvert1.T\n",
    "            col_sum = Yvert.sum(axis=1, keepdims=True)\n",
    "            col_sum[col_sum==0.0] = 1.0\n",
    "            Yvert /= col_sum\n",
    "            A[off_lp1:off_lp1+k_lp1, off_l:off_l+k_l] = Yvert\n",
    "\n",
    "        # now we have a big adjacency => row normalize again, then scale spectral radius\n",
    "        row_sum = A.sum(axis=1, keepdims=True)\n",
    "        row_sum[row_sum==0.0] = 1.0\n",
    "        A /= row_sum\n",
    "\n",
    "        A = scale_spectral_radius(A, self.spectral_radius)\n",
    "        return A\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Main training routine:\n",
    "          1) Build hierarchical adjacency from fractal partition => self.W\n",
    "          2) define W_in => shape(n_nodes, 3)\n",
    "          3) teacher forcing => polynomial readout => solve => self.W_out\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Build adjacency\n",
    "        W_big = self._build_hierarchical_adjacency(train_input)\n",
    "        self.W = W_big\n",
    "\n",
    "        # define W_in => shape(n_nodes,3)\n",
    "        self.n_nodes = W_big.shape[0]\n",
    "        self.W_in = (np.random.rand(self.n_nodes,3)-0.5)*2.0*self.input_scale\n",
    "\n",
    "        # define reservoir state\n",
    "        self.x = np.zeros(self.n_nodes)\n",
    "\n",
    "        # gather states => teacher forcing => polynomial => readout\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]\n",
    "        X_list= []\n",
    "        for s in states_use:\n",
    "            X_list.append( augment_state_with_squares(s) )\n",
    "        X_aug= np.array(X_list)\n",
    "\n",
    "        reg= Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        self.W_out= reg.coef_\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Teacher forcing => feed real 3D => gather states => shape => [T-discard, n_nodes].\n",
    "        returns (states_after_discard, states_discarded).\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states= []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states= np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.x is not None:\n",
    "            self.x.fill(0.0)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        x(t+1)= (1-alpha)x(t)+ alpha tanh( W*x(t)+ W_in*u(t) ).\n",
    "        \"\"\"\n",
    "        alpha= self.leaking_rate\n",
    "        pre_acts= self.W@self.x + self.W_in@u\n",
    "        x_new= np.tanh(pre_acts)\n",
    "        self.x= (1.0- alpha)*self.x+ alpha*x_new\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        fully autonomous => feed last predicted => next input\n",
    "        \"\"\"\n",
    "        preds= []\n",
    "        #self.reset_state()\n",
    "        current_in= np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            big_x= augment_state_with_squares(self.x)\n",
    "            out= self.W_out@big_x\n",
    "            preds.append(out)\n",
    "            current_in= out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34bf6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineESN3D:\n",
    "    \"\"\"\n",
    "    A baseline Echo State Network that handles:\n",
    "        - 3D input (x,y,z)\n",
    "        - 3D output (x(t+1), y(t+1), z(t+1))\"\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reservoir_size=400,\n",
    "                 spectral_radius=0.95,\n",
    "                 connectivity=0.1,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=0.8,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 activation_choices=('tanh','relu','sin','linear'),\n",
    "                 seed=42):\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.connectivity = connectivity\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.activation_choices = activation_choices\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        # Initialize W with small random values (mean=0, variance=0.01)\n",
    "        W = np.random.randn(reservoir_size, reservoir_size) * 0.1\n",
    "        # Create a boolean sparsity mask\n",
    "        mask = (np.random.rand(reservoir_size, reservoir_size) < self.connectivity)\n",
    "        W = W * mask\n",
    "        W = scale_spectral_radius(W, self.spectral_radius)\n",
    "        self.W = W\n",
    "\n",
    "        np.random.seed(self.seed + 100)\n",
    "        # Initialize input weights in range [-input_scale, input_scale]\n",
    "        self.W_in = (np.random.rand(self.reservoir_size, 3) - 0.5) * 2.0 * self.input_scale\n",
    "\n",
    "        np.random.seed(self.seed + 200)\n",
    "        self.node_activations = np.random.choice(self.activation_choices, size=self.reservoir_size)\n",
    "\n",
    "        # Readout: shape (3, reservoir_size+1) -> 3D output\n",
    "        self.W_out = None\n",
    "        self.reset_state()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.x = np.zeros(self.reservoir_size)\n",
    "\n",
    "    def _apply_activation(self, act_type, val):\n",
    "        return np.tanh(val)\n",
    "        # if act_type=='tanh':\n",
    "        #     return np.tanh(val)\n",
    "        # elif act_type=='relu':\n",
    "        #     return max(0.0, val)\n",
    "        # elif act_type=='sin':\n",
    "        #     return np.sin(val)\n",
    "        # elif act_type=='linear':\n",
    "        #     return val\n",
    "        # else:\n",
    "        #     return np.tanh(val)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        x(t+1) = (1-alpha)*x(t) + alpha*[ node-by-node activation( W*x(t)+W_in*u ) ]\n",
    "        \"\"\"\n",
    "        pre_activations = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.zeros_like(self.x)\n",
    "        for i in range(self.reservoir_size):\n",
    "            activation = self.node_activations[i]\n",
    "            x_new[i] = self._apply_activation(activation, pre_activations[i])\n",
    "        alpha = self.leaking_rate\n",
    "        self.x = (1.0 - alpha)*self.x + alpha*x_new\n",
    "        \n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Run reservoir on 'inputs' (shape [T, 3]), discarding the first 'discard' steps.\n",
    "        Returns: states [T-discard, reservoir_size]\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for t in range(len(inputs)):\n",
    "            self._update(inputs[t])\n",
    "            states.append(self.x.copy())\n",
    "        states = np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "    \n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Teacher forcing for single-step:\n",
    "            - input(t) = [x(t), y(t), z(t)]\n",
    "            - target(t) = [x(t+1), y(t+1), z(t+1)]\n",
    "        We collect states(t), then solve a multi-output linear ridge regression:\n",
    "            W_out * [x(t); 1] ~ target(t+1).\n",
    "        For quadratic readout:\n",
    "            W_out * [x(t); x²(t); 1] ~ target(t+1).\n",
    "        \"\"\"\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]  # shape [T-discard, 3]\n",
    "\n",
    "        # Augment states with bias\n",
    "        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0], 1))])  # shape [T-discard, N+1]\n",
    "\n",
    "        # Quadratic readout\n",
    "        # Build augmented matrix [ x, x^2, 1 ]\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append( np.concatenate([s, s**2, [1.0]]) )\n",
    "        X_aug = np.array(X_list)                                # shape [T-discard, 2N+1]\n",
    "\n",
    "        ridge_reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        ridge_reg.fit(X_aug, targets_use)\n",
    "        self.W_out = ridge_reg.coef_\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Single-step-ahead inference on test data:\n",
    "        For each inputs[t], we update reservoir, then read out 3D prediction.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for t in range(len(inputs)):\n",
    "            self._update(inputs[t])\n",
    "            # x_aug = np.concatenate([self.x, [1.0]])\n",
    "            x_aug = np.concatenate([self.x, (self.x)**2, [1.0]])  # For quadrartic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def autoregressive_predict(self, initial_input, num_steps):\n",
    "        \"\"\"\n",
    "        Autoregressive multi-step forecasting for num_steps\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        current_input = initial_input.copy()\n",
    "      \n",
    "        for _ in range(num_steps):\n",
    "            self._update(current_input)\n",
    "            # x_aug = np.concatenate([self.x, [1.0]])\n",
    "            x_aug = np.concatenate([self.x, (self.x)**2, [1.0]])  # For quadratic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "            current_input = out\n",
    "        \n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f81122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWRes3D(BaselineESN3D):\n",
    "    \"\"\"\n",
    "    Small-World (SW) Reservoir for 3D->3D single-step prediction using the Watts-Strogatz (WS) method.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reservoir_size=300,\n",
    "                 rewiring_prob=0.1,\n",
    "                 degree=6,\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 activation_choices=('tanh','relu','sin','linear'),\n",
    "                 seed=42):\n",
    "\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.rewiring_prob = rewiring_prob\n",
    "        self.degree = degree\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.activation_choices = activation_choices\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        # Create a Watts-Strogatz small-world graph\n",
    "        ws_graph = nx.watts_strogatz_graph(n=reservoir_size, k=self.degree, p=self.rewiring_prob, seed=self.seed)\n",
    "        adjacency_matrix = nx.to_numpy_array(ws_graph)\n",
    "        \n",
    "        # Initialize reservoir weights\n",
    "        W = adjacency_matrix * np.random.uniform(-1, 1, (reservoir_size, reservoir_size))\n",
    "        W = scale_spectral_radius(W, spectral_radius)\n",
    "        self.W = W\n",
    "\n",
    "        np.random.seed(self.seed+100)\n",
    "        self.W_in = (np.random.rand(reservoir_size, 3) - 0.5) * 2.0 * input_scale\n",
    "\n",
    "        np.random.seed(self.seed + 200)\n",
    "        self.node_activations = np.random.choice(self.activation_choices, size=self.reservoir_size)\n",
    "\n",
    "        self.W_out = None\n",
    "        self.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0525e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCIESN3D:\n",
    "    def __init__(self,\n",
    "                 reservoir_size=300,\n",
    "                 cycle_weight=0.8,\n",
    "                 connect_weight=0.8,\n",
    "                 combine_factor=0.5,\n",
    "                 v1=0.6,\n",
    "                 v2=0.6,\n",
    "                 spectral_radius=0.95,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42\n",
    "                 ):\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.cycle_weight = cycle_weight\n",
    "        self.connect_weight = connect_weight\n",
    "        self.combine_factor = combine_factor\n",
    "        self.v1 = v1\n",
    "        self.v2 = v2\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        W_res = np.zeros((reservoir_size, reservoir_size))\n",
    "        for i in range(reservoir_size):\n",
    "            j = (i+1) % reservoir_size\n",
    "            W_res[j, i] = self.cycle_weight\n",
    "\n",
    "        W_res = scale_spectral_radius(W_res, spectral_radius)\n",
    "        self.W_res = W_res                      # shared by both sub-reservoirs\n",
    "\n",
    "        np.random.seed(self.seed + 100)\n",
    "        W_cn = np.zeros((reservoir_size, reservoir_size))\n",
    "        W_cn[0, reservoir_size-1] = self.connect_weight\n",
    "        W_cn[reservoir_size-1, 0] = self.connect_weight\n",
    "        self.W_cn = W_cn\n",
    "\n",
    "        np.random.seed(self.seed+200)\n",
    "\n",
    "        sign_V1 = np.random.choice([-1, 1], size=(reservoir_size, 3))\n",
    "        sign_V2 = np.random.choice([-1, 1], size=(reservoir_size, 3))\n",
    "\n",
    "        V1 = self.v1 * sign_V1\n",
    "        V2 = self.v2 * sign_V2\n",
    "\n",
    "        self.W_in1 = V1 - V2\n",
    "        self.W_in2 = V1 + V2\n",
    "        \n",
    "        self.W_out = None\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.x1 = np.zeros(self.reservoir_size)\n",
    "        self.x2 = np.zeros(self.reservoir_size)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        Single-step reservoir update.\n",
    "        x1(t+1) = cos( Win1*u(t+1) + W_res*x1(t) + W_cn*x2(t) )\n",
    "        x2(t+1) = sin( Win2*u(t+1) + W_res*x2(t) + W_cn*x1(t) )\n",
    "        Then x(t+1)= h*x1(t+1) + (1-h)* x2(t+1).\n",
    "        \"\"\"\n",
    "        # pre activation for reservoir1\n",
    "        pre_activation1 = self.W_in1 @ u + self.W_res @ self.x1 + self.W_cn @ self.x2\n",
    "        # reservoir1 uses cos\n",
    "        x1_new = np.cos(pre_activation1)\n",
    "\n",
    "        # reservoir2 uses sin\n",
    "        pre_activation2 = self.W_in2 @ u + self.W_res @ self.x2 + self.W_cn @ self.x1\n",
    "        x2_new = np.sin(pre_activation2)\n",
    "\n",
    "        alpha = self.leaking_rate\n",
    "        self.x1 = (1.0 - alpha)*self.x1 + alpha*x1_new\n",
    "        self.x2 = (1.0 - alpha)*self.x2 + alpha*x2_new\n",
    "\n",
    "    def _combine_state(self):\n",
    "        \"\"\"\n",
    "        Combine x1(t), x2(t) => x(t) = h*x1(t) + (1-h)*x2(t)\n",
    "        \"\"\"\n",
    "        h = self.combine_factor\n",
    "        return h*self.x1 + (1.0 - h)*self.x2\n",
    "    \n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for t in range(len(inputs)):\n",
    "            self._update(inputs[t])\n",
    "            combined = self._combine_state()\n",
    "            states.append(combined.copy())\n",
    "        states = np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "    \n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]\n",
    "\n",
    "        # Augment states with bias\n",
    "        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0], 1))])  # shape [T-discard, N+1]\n",
    "\n",
    "        # Quadratic readout\n",
    "        # Build augmented matrix [ x, x^2, 1 ]\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append( np.concatenate([s, s**2, [1.0]]) )\n",
    "        X_aug = np.array(X_list)                                # shape [T-discard, 2N+1]\n",
    "\n",
    "        ridge_reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        ridge_reg.fit(X_aug, targets_use)\n",
    "        self.W_out = ridge_reg.coef_\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        preds = []\n",
    "        for t in range(len(inputs)):\n",
    "            self._update(inputs[t])\n",
    "            combined = self._combine_state()\n",
    "            # x_aug = np.concatenate([self.x, [1.0]])\n",
    "            x_aug = np.concatenate([combined, combined**2, [1.0]])  # For quadrartic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def autoregressive_predict(self, initial_input, num_steps):\n",
    "        preds = []\n",
    "        current_input = initial_input.copy()\n",
    "      \n",
    "        for _ in range(num_steps):\n",
    "            self._update(current_input)\n",
    "            combined = self._combine_state()\n",
    "            # x_aug = np.concatenate([self.x, [1.0]])\n",
    "            x_aug = np.concatenate([combined, combined**2, [1.0]])  # For quadratic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "            current_input = out\n",
    "        \n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33764056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRJRes3D(BaselineESN3D):\n",
    "    def __init__(self,\n",
    "                 reservoir_size=300,\n",
    "                 edge_weight = 0.8,\n",
    "                 jump=10,\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 activation_choices=('tanh','relu','sin','linear'),\n",
    "                 seed=42):\n",
    "        \n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.edge_weight = edge_weight\n",
    "        self.jump = jump\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.activation_choices = activation_choices\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.zeros((reservoir_size, reservoir_size))\n",
    "        for i in range(reservoir_size):\n",
    "            W[i, (i+1) % reservoir_size] = edge_weight              # Cycle edge\n",
    "            W[i, (i + self.jump) % reservoir_size] = edge_weight    # Jump edge\n",
    "\n",
    "        W = scale_spectral_radius(W, self.spectral_radius)\n",
    "        self.W = W\n",
    "\n",
    "        np.random.seed(self.seed+100)\n",
    "        self.W_in = (np.random.rand(self.reservoir_size, 3) - 0.5) * 2.0 * self.input_scale\n",
    "\n",
    "        np.random.seed(self.seed + 200)\n",
    "        self.node_activations = np.random.choice(self.activation_choices, size=self.reservoir_size)\n",
    "\n",
    "        # Readout: shape (3, reservoir_size+1) -> 3D output\n",
    "        self.W_out = None\n",
    "        self.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a2aa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepESN3D:\n",
    "    \"\"\"\n",
    "    Deep Echo State Network (DeepESN) for multi-layered reservoir computing.\n",
    "    Each layer has its own reservoir, and the states are propagated through layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_layers=3,\n",
    "                 reservoir_size=100,\n",
    "                 spectral_radius=0.95,\n",
    "                 connectivity=0.1,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 activation_choices=('tanh','relu','sin','linear'),\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_layers: Number of reservoir layers.\n",
    "        - reservoir_size: Number of neurons in each reservoir layer.\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.connectivity = connectivity\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.activation_choices = activation_choices\n",
    "        self.seed = seed\n",
    "\n",
    "        # Initialize reservoirs and input weights for each layer\n",
    "        self.reservoirs = []\n",
    "        self.input_weights = []\n",
    "        self.states = []\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        for layer in range(num_layers):\n",
    "            np.random.seed(seed + layer)\n",
    "            W = np.random.randn(reservoir_size, reservoir_size) * 0.1\n",
    "            mask = (np.random.rand(reservoir_size, reservoir_size) < self.connectivity)\n",
    "            W = W * mask\n",
    "            W = scale_spectral_radius(W, spectral_radius)\n",
    "            self.reservoirs.append(W)\n",
    "\n",
    "            if layer == 0 : \n",
    "                W_in = (np.random.rand(reservoir_size, 3) - 0.5) * 2.0 * input_scale\n",
    "            else:\n",
    "                W_in = (np.random.rand(reservoir_size, reservoir_size) - 0.5) * 2.0 * input_scale\n",
    "            self.input_weights.append(W_in)\n",
    "\n",
    "        np.random.seed(self.seed + 200)\n",
    "        self.node_activations = np.random.choice(self.activation_choices, size=self.reservoir_size)\n",
    "        \n",
    "        self.W_out = None\n",
    "        self.reset_state()\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"\n",
    "        Reset the states of all reservoir layers.\n",
    "        \"\"\"\n",
    "        self.states = [np.zeros(self.reservoir_size) for _ in range(self.num_layers)]\n",
    "\n",
    "    def _apply_activation(self, act_type, val):\n",
    "        return np.tanh(val)\n",
    "        # if act_type=='tanh':\n",
    "        #     return np.tanh(val)\n",
    "        # elif act_type=='relu':\n",
    "        #     return max(0.0, val)\n",
    "        # elif act_type=='sin':\n",
    "        #     return np.sin(val)\n",
    "        # elif act_type=='linear':\n",
    "        #     return val\n",
    "        # else:\n",
    "        #     return np.tanh(val)\n",
    "\n",
    "    def _update_layer(self, layer_idx, u):\n",
    "        \"\"\"\n",
    "        Update a single reservoir layer.\n",
    "        \"\"\"\n",
    "        pre_activation = self.reservoirs[layer_idx] @ self.states[layer_idx]\n",
    "        if layer_idx == 0:\n",
    "            pre_activation += self.input_weights[layer_idx] @ u\n",
    "        else:\n",
    "            pre_activation += self.input_weights[layer_idx] @ self.states[layer_idx - 1]\n",
    "\n",
    "        x_new = np.zeros_like(pre_activation)\n",
    "        for i in range(self.reservoir_size):\n",
    "            activation = self.node_activations[i]\n",
    "            x_new[i] = self._apply_activation(activation, pre_activation[i])\n",
    "        alpha = self.leaking_rate\n",
    "        self.states[layer_idx] = (1.0 - alpha) * self.states[layer_idx] + alpha * x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        all_states = []\n",
    "        for u in inputs:\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                self._update_layer(layer_idx, u)\n",
    "            all_states.append(np.concatenate(self.states))\n",
    "        all_states = np.array(all_states)\n",
    "        return all_states[discard:], all_states[:discard]\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Train the readout layer using ridge regression.\n",
    "        \"\"\"\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]\n",
    "\n",
    "        # Augment states with bias\n",
    "        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0], 1))])  # shape [T-discard, N*L+1]\n",
    "\n",
    "        # Quadratic readout\n",
    "        # Build augmented matrix [ x, x^2, 1 ]\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append( np.concatenate([s, s**2, [1.0]]) )\n",
    "        X_aug = np.array(X_list)                                    # shape [T-discard, 2N*L+1]\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Single-step-ahead inference on test data.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for u in inputs:\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                self._update_layer(layer_idx, u)\n",
    "            state = np.concatenate(self.states)\n",
    "            # x_aug = np.concatenate([state, [1.0]])\n",
    "            x_aug = np.concatenate([state, (state)**2, [1.0]])  # For quadrartic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)\n",
    "\n",
    "    def autoregressive_predict(self, initial_input, num_steps):\n",
    "        \"\"\"\n",
    "        Autoregressive multi-step forecasting for num_steps\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        current_input = initial_input.copy()\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "            for layer_idx in range(self.num_layers):\n",
    "                self._update_layer(layer_idx, current_input)\n",
    "            state = np.concatenate(self.states)\n",
    "            # x_aug = np.concatenate([state, [1.0]])\n",
    "            x_aug = np.concatenate([state, (state)**2, [1.0]])  # For quadrartic readout\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "            current_input = out\n",
    "        \n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f6038",
   "metadata": {},
   "source": [
    "### NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55e0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons for Teacher-forced Single-step Forecasting\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets)**2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * variance))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9a21c",
   "metadata": {},
   "source": [
    "### VPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193a03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, test_time, lyapunov_time, threshold=0.4):\n",
    "    y_mean = np.mean(y_true, axis=0)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))\n",
    "\n",
    "    error = y_true - y_pred\n",
    "    squared_error = np.sum(error**2, axis=1)\n",
    "    delta = squared_error / denom\n",
    "\n",
    "    idx_exceed = np.where(delta > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = test_time[-1]\n",
    "    else:\n",
    "        T_VPT = test_time[idx_exceed[0]]\n",
    "\n",
    "    ratio = T_VPT / lyapunov_time\n",
    "\n",
    "    return T_VPT, ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e8e78",
   "metadata": {},
   "source": [
    "### ADev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38f84534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c2de7",
   "metadata": {},
   "source": [
    "### PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402a5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(y, dt=0.01):\n",
    "    z = y[:, 2]  # Extract Z-component\n",
    "    \n",
    "    # Compute PSD using Welch’s method\n",
    "    freqs, psd = welch(z, fs=1/dt, window='hamming', nperseg=len(z))  # Using Hamming window\n",
    "    \n",
    "    return freqs, psd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8719c3c",
   "metadata": {},
   "source": [
    "# MIT-BIH Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49bbfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b46a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "\n",
    "# Download and load record and annotations for patient #100\n",
    "record = wfdb.rdrecord('100', sampfrom=0, sampto=25002, pn_dir='mitdb')  # first 20,000 samples\n",
    "annotation = wfdb.rdann('100', 'atr', sampfrom=0, sampto=25002, pn_dir='mitdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0e8cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.145, -0.145, -0.145, ..., -0.41 , -0.415, -0.425],\n",
       "      shape=(25002,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get input signal u(t) from the first channel\n",
    "u = record.p_signal[:, 0] \n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93538ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input\n",
    "u_min = np.min(u)\n",
    "u_max = np.max(u)\n",
    "u_norm = (u - u_min) / (u_max - u_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfcd0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = record.fs  # sampling frequency (should be 360 Hz)\n",
    "t_vals = np.arange(len(u_norm)) / fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef734865",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 3\n",
    "# inputs = u_norm\n",
    "inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "\n",
    "# Create target array (heartbeat locations)\n",
    "targets = np.zeros(len(u_norm))\n",
    "targets[annotation.sample] = 1  # mark annotations as 1 (heartbeat)\n",
    "targets = create_delay_embedding(targets, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "938389fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 25000, train size: 15000, test size: 9999\n"
     ]
    }
   ],
   "source": [
    "data_size = len(inputs)\n",
    "train_size = 15000\n",
    "train_input = inputs[:train_size]\n",
    "train_target = targets[:train_size]\n",
    "test_input = inputs[train_size+1:]\n",
    "test_target = targets[train_size+1:]\n",
    "test_size = len(test_input)\n",
    "print(f\"Total samples: {data_size}, train size: {train_size}, test size: {test_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "218cb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# step = 5\n",
    "# for a in range(5, 101, step):\n",
    "#     for b in range(a + step, 111, step):\n",
    "#         for c in range(b + step, 121, step):\n",
    "#             for d in range(c + step, 131, step):\n",
    "#                 for e in range(d + step, 141, step):\n",
    "#                     for f in range(e + step, 151, step):\n",
    "#                         for g in range(f + step, 161, step):\n",
    "#                             for h in range(g + step, 171, step):\n",
    "#                                 i = 300 - a - b - c - d - e - f - g - h\n",
    "#                                 if i > h and i % step == 0 and i <= 300:\n",
    "#                                     result.append([a, b, c, d, e, f, g, h, i])\n",
    "\n",
    "# print(result)\n",
    "# print(f\"Total unique combinations: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4b7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {\n",
    "#     \"cells_per_level\": [[5, 10, 20, 25, 30, 35, 40, 60, 75, 90, 110]],\n",
    "#     \"spectral_radius\": [0.92],\n",
    "#     \"input_scale\": [0.1],\n",
    "#     \"leaking_rate\": [0.1],\n",
    "#     \"ridge_alpha\": [1e-8],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2f85de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    hfr = HFRRes3D(\n",
    "        n_levels=8,      \n",
    "        cells_per_level=[5, 10, 20, 25, 30, 35, 40, 60, 75, 90, 110],\n",
    "        spectral_radius=0.01,\n",
    "        input_scale=1.5,\n",
    "        leaking_rate=0.5,\n",
    "        ridge_alpha=1e-8,\n",
    "        seed=seed\n",
    "    )\n",
    "    hfr.fit_readout(train_input, train_target, discard=5000)\n",
    "    hfr_preds = hfr.predict_open_loop(test_input)\n",
    "    hfr_nrmse = evaluate_nrmse(hfr_preds, test_target, horizons)\n",
    "    nrmse_dict['HFR'].append(hfr_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "337c012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "HFR              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        0.6152950314523217 ± 0.07004022207145659\n",
      "600        0.6463315980507007 ± 0.03593006864943821\n",
      "1000       0.8261457146386831 ± 0.016216600395920888\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'HFR':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['HFR']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60917d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_grid_search(model_class, param_grid, model_name,\n",
    "#                     output_path=\"grid_search_results.json\"):\n",
    "#     # Precompute param combinations\n",
    "#     combos = list(itertools.product(*param_grid.values()))\n",
    "#     param_keys = list(param_grid.keys())\n",
    "#     print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "#     results = []\n",
    "#     horizons = [300, 600, 1000]\n",
    "#     seeds = range(995, 1025)\n",
    "#     # tqdm adds a progress bar for better visualization\n",
    "#     for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "#         params = dict(zip(param_keys, comb))\n",
    "#         seed_scores = []\n",
    "#         for seed in seeds:\n",
    "#             model = model_class(seed=seed, **params)\n",
    "#             model.fit_readout(train_input, train_target, discard=5000)\n",
    "#             preds = model.predict_open_loop(test_input)\n",
    "#             nrmse = evaluate_nrmse(preds, test_target, horizons)\n",
    "#             seed_scores.append(nrmse)\n",
    "\n",
    "#         results.append({\n",
    "#             \"params\": params,\n",
    "#             \"scores\": seed_scores\n",
    "#         })\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7df19d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = run_grid_search(\n",
    "#     HFRRes3D,\n",
    "#     grid,\n",
    "#     model_name=\"HFRRes3D\",\n",
    "#     output_path=\"hfr_grid_search_results.json\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3ab10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d21c45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87f6a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rows = []\n",
    "# for entry in results:\n",
    "#     params = entry[\"params\"]\n",
    "#     scores = entry[\"scores\"]\n",
    "\n",
    "#     # Convert list of dicts to dict of lists per horizon\n",
    "#     all_scores = {300: [], 600: [], 1000: []}\n",
    "#     for score_dict in scores:\n",
    "#         for h in all_scores:\n",
    "#             all_scores[h].append(score_dict[h])\n",
    "\n",
    "#     # Compute mean and std for each horizon\n",
    "#     row = params.copy()\n",
    "#     for h in all_scores:\n",
    "#         values = np.array(all_scores[h])\n",
    "#         row[f\"nrmse_{h}_mean\"] = values.mean()\n",
    "#         row[f\"nrmse_{h}_std\"] = values.std()\n",
    "\n",
    "#     rows.append(row)\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(rows)\n",
    "\n",
    "# # Optional: Convert list-type params to string if needed\n",
    "# if 'cells_per_level' in df.columns:\n",
    "#     df['cells_per_level'] = df['cells_per_level'].apply(str)\n",
    "\n",
    "# # Save to CSV\n",
    "# df.to_csv(\"results.csv\", index=False)\n",
    "# print(\"Saved to grid_search_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c1e20",
   "metadata": {},
   "source": [
    "# Sunspot Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3232502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1749</td>\n",
       "      <td>1</td>\n",
       "      <td>1749.042</td>\n",
       "      <td>96.7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1749</td>\n",
       "      <td>2</td>\n",
       "      <td>1749.123</td>\n",
       "      <td>104.3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1749</td>\n",
       "      <td>3</td>\n",
       "      <td>1749.204</td>\n",
       "      <td>116.7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1749</td>\n",
       "      <td>4</td>\n",
       "      <td>1749.288</td>\n",
       "      <td>92.8</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1749</td>\n",
       "      <td>5</td>\n",
       "      <td>1749.371</td>\n",
       "      <td>141.7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>2024.873</td>\n",
       "      <td>152.5</td>\n",
       "      <td>20.9</td>\n",
       "      <td>681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>2024</td>\n",
       "      <td>12</td>\n",
       "      <td>2024.958</td>\n",
       "      <td>154.5</td>\n",
       "      <td>25.6</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>2025.042</td>\n",
       "      <td>137.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>2025.122</td>\n",
       "      <td>154.6</td>\n",
       "      <td>23.3</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>2025.204</td>\n",
       "      <td>134.2</td>\n",
       "      <td>20.4</td>\n",
       "      <td>1011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3315 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1         2      3     4     5  6\n",
       "0     1749   1  1749.042   96.7  -1.0    -1  1\n",
       "1     1749   2  1749.123  104.3  -1.0    -1  1\n",
       "2     1749   3  1749.204  116.7  -1.0    -1  1\n",
       "3     1749   4  1749.288   92.8  -1.0    -1  1\n",
       "4     1749   5  1749.371  141.7  -1.0    -1  1\n",
       "...    ...  ..       ...    ...   ...   ... ..\n",
       "3310  2024  11  2024.873  152.5  20.9   681  0\n",
       "3311  2024  12  2024.958  154.5  25.6   572  0\n",
       "3312  2025   1  2025.042  137.0  23.3   670  0\n",
       "3313  2025   2  2025.122  154.6  23.3   655  0\n",
       "3314  2025   3  2025.204  134.2  20.4  1011  0\n",
       "\n",
       "[3315 rows x 7 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'datasets/SN_m_tot_V2.0.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, sep=';', header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98f8602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3315\n",
      "Train size: 2000\n",
      "Test size: 1312\n"
     ]
    }
   ],
   "source": [
    "data = df.iloc[:, 3].values\n",
    "dt = 1\n",
    "dataset_size = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 2000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}\\nTest size: {len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c7684b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_horizons = list(range(10, 1001, 10))\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    hfr = HFRRes3D(\n",
    "        n_levels=8,      \n",
    "        cells_per_level=[5, 10, 20, 25, 30, 35, 40, 60, 75, 90, 110],\n",
    "        spectral_radius=0.2,\n",
    "        input_scale=0.0001,\n",
    "        leaking_rate=0.1,\n",
    "        ridge_alpha=1e-8,\n",
    "        seed=seed\n",
    "    )\n",
    "    hfr.fit_readout(train_input, train_target, discard=100)\n",
    "    hfr_preds = hfr.predict_open_loop(test_input)\n",
    "    hfr_nrmse = evaluate_nrmse(hfr_preds, test_target, horizons)\n",
    "    nrmse_dict['HFR'].append(hfr_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "daaef00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "HFR              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        0.41731371746663587 ± 0.00014355956521133864\n",
      "600        0.33843368051308065 ± 0.00011557675302777137\n",
      "1000       0.33626407063947517 ± 9.510715917861278e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'HFR':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['HFR']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6ee32",
   "metadata": {},
   "source": [
    "# Sante Fe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2407cfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.53</td>\n",
       "      <td>8320</td>\n",
       "      <td>7771</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.53</td>\n",
       "      <td>8117</td>\n",
       "      <td>7774</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76.15</td>\n",
       "      <td>7620</td>\n",
       "      <td>7788</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75.39</td>\n",
       "      <td>6413</td>\n",
       "      <td>7787</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.51</td>\n",
       "      <td>7518</td>\n",
       "      <td>7767</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16995</th>\n",
       "      <td>73.57</td>\n",
       "      <td>16021</td>\n",
       "      <td>6498</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16996</th>\n",
       "      <td>73.79</td>\n",
       "      <td>-6957</td>\n",
       "      <td>6547</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16997</th>\n",
       "      <td>74.54</td>\n",
       "      <td>11476</td>\n",
       "      <td>6576</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16998</th>\n",
       "      <td>74.36</td>\n",
       "      <td>15058</td>\n",
       "      <td>6573</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16999</th>\n",
       "      <td>72.91</td>\n",
       "      <td>13510</td>\n",
       "      <td>6676</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1     2   3\n",
       "0      76.53   8320  7771 NaN\n",
       "1      76.53   8117  7774 NaN\n",
       "2      76.15   7620  7788 NaN\n",
       "3      75.39   6413  7787 NaN\n",
       "4      75.51   7518  7767 NaN\n",
       "...      ...    ...   ...  ..\n",
       "16995  73.57  16021  6498 NaN\n",
       "16996  73.79  -6957  6547 NaN\n",
       "16997  74.54  11476  6576 NaN\n",
       "16998  74.36  15058  6573 NaN\n",
       "16999  72.91  13510  6676 NaN\n",
       "\n",
       "[17000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'datasets/santa-fe-time-series-competition-data-set-b-1.0.0/b1.txt'\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, sep=' ')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5624624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the first column (column 0) of the DataFrame\n",
    "df[0] = (df[0] - df[0].min()) / (df[0].max() - df[0].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae5bf803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 17000.\n",
      "Train size: 7000  \n",
      "Test size: 9997\n"
     ]
    }
   ],
   "source": [
    "data = df.iloc[:, 0].values\n",
    "chosen_system = \"SantaFe\"\n",
    "dt = 1\n",
    "T_data = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Data length: {T_data}.\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 7000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}  \\nTest size: {len(test_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_horizons = list(range(10, 1001, 10))\n",
    "horizons = [300, 600, 1000]\n",
    "\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "# for seed in seeds:\n",
    "#     hfr = HFRRes3D(\n",
    "#         n_levels=8,      \n",
    "#         cells_per_level=[5, 10, 20, 25, 30, 35, 40, 60, 75, 90, 110],\n",
    "#         spectral_radius=0.01,\n",
    "#         input_scale=0.1,\n",
    "#         leaking_rate=0.1,\n",
    "#         ridge_alpha=1e-8,\n",
    "#         seed=seed\n",
    "#     )\n",
    "#     hfr.fit_readout(train_input, train_target, discard=2000)\n",
    "#     hfr_preds = hfr.predict_open_loop(test_input)\n",
    "#     hfr_nrmse = evaluate_nrmse(hfr_preds, test_target, horizons)\n",
    "#     nrmse_dict['HFR'].append(hfr_nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27efeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    crj = CRJRes3D(\n",
    "        reservoir_size=500,\n",
    "        edge_weight=0.8,\n",
    "        jump=15,\n",
    "        spectral_radius=0.99,\n",
    "        input_scale=0.2,\n",
    "        leaking_rate=0.8,\n",
    "        ridge_alpha=1e-6,\n",
    "        seed=seed\n",
    "    )\n",
    "    crj.fit_readout(train_input, train_target, discard=5000)\n",
    "    crj_preds = crj.predict(test_input)\n",
    "    crj_nrmse = evaluate_nrmse(crj_preds, test_target, all_horizons)\n",
    "    nrmse_dict['CRJ'].append(crj_nrmse)\n",
    "\n",
    "# for seed in seeds:\n",
    "#     mci_esn = MCIESN3D(\n",
    "#         reservoir_size=500,\n",
    "#         cycle_weight=0.8,\n",
    "#         connect_weight=0.8,\n",
    "#         combine_factor=0.1,\n",
    "#         v1=0.03,\n",
    "#         v2=0.03,\n",
    "#         spectral_radius=0.99,\n",
    "#         leaking_rate=0.8,\n",
    "#         ridge_alpha=1e-6,\n",
    "#         seed=seed\n",
    "#     )\n",
    "#     mci_esn.fit_readout(train_input, train_target, discard=5000)\n",
    "#     mci_esn_preds = mci_esn.predict(test_input)\n",
    "#     mci_esn_nrmse = evaluate_nrmse(mci_esn_preds, test_target, horizons)\n",
    "#     nrmse_dict['MCI-ESN'].append(mci_esn_nrmse)\n",
    "\n",
    "\n",
    "# for seed in seeds:\n",
    "#     deepesn = DeepESN3D(\n",
    "#         num_layers=5,\n",
    "#         reservoir_size=100,\n",
    "#         spectral_radius=0.99,\n",
    "#         input_scale=0.2,\n",
    "#         leaking_rate=0.8,\n",
    "#         ridge_alpha=1e-6,\n",
    "#         seed=seed\n",
    "#     )\n",
    "#     deepesn.fit_readout(train_input, train_target, discard=5000)\n",
    "#     deepesn_preds = deepesn.predict(test_input)\n",
    "#     deepesn_nrmse = evaluate_nrmse(deepesn_preds, test_target, horizons)\n",
    "#     nrmse_dict['DeepESN'].append(deepesn_nrmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afd4e124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "CRJ              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        0.3311207992831167 ± 0.009378925794365722\n",
      "600        0.2996007302129928 ± 0.016803369628981674\n",
      "1000       0.31110027173794896 ± 0.014826514671935544\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'CRJ':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    crj_vals = [np.mean(crj_nrmse[horizon]) for crj_nrmse in nrmse_dict['CRJ']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [crj_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e5f0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon in [1000]:\n",
    "    crj_vals = [np.mean(crj_nrmse[horizon]) for crj_nrmse in nrmse_dict['CRJ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20bd0846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.29000843893248435),\n",
       " np.float64(0.309148323024945),\n",
       " np.float64(0.32659144846892424),\n",
       " np.float64(0.3039714851291318),\n",
       " np.float64(0.29258113930886226),\n",
       " np.float64(0.31024836734397154),\n",
       " np.float64(0.2951963586421195),\n",
       " np.float64(0.2853331750098682),\n",
       " np.float64(0.31924539710353983),\n",
       " np.float64(0.30016568182513714),\n",
       " np.float64(0.3191322226103948),\n",
       " np.float64(0.33379069862031424),\n",
       " np.float64(0.3143257694771222),\n",
       " np.float64(0.31721888143145544),\n",
       " np.float64(0.30552764770563207),\n",
       " np.float64(0.3011093344323173),\n",
       " np.float64(0.3555299804718019),\n",
       " np.float64(0.31279757786746004),\n",
       " np.float64(0.3062745634197625),\n",
       " np.float64(0.31540614075135104),\n",
       " np.float64(0.3073646034931301),\n",
       " np.float64(0.32850520523876053),\n",
       " np.float64(0.2868286264541298),\n",
       " np.float64(0.30146383711230884),\n",
       " np.float64(0.31231718734721303),\n",
       " np.float64(0.3251354111071609),\n",
       " np.float64(0.3295762473780636),\n",
       " np.float64(0.30554270928538646),\n",
       " np.float64(0.3057524375888075),\n",
       " np.float64(0.3169192555569122),\n",
       " np.float64(0.29000843893248435),\n",
       " np.float64(0.309148323024945),\n",
       " np.float64(0.32659144846892424),\n",
       " np.float64(0.3039714851291318),\n",
       " np.float64(0.29258113930886226),\n",
       " np.float64(0.31024836734397154),\n",
       " np.float64(0.2951963586421195),\n",
       " np.float64(0.2853331750098682),\n",
       " np.float64(0.31924539710353983),\n",
       " np.float64(0.30016568182513714),\n",
       " np.float64(0.3191322226103948),\n",
       " np.float64(0.33379069862031424),\n",
       " np.float64(0.3143257694771222),\n",
       " np.float64(0.31721888143145544),\n",
       " np.float64(0.30552764770563207),\n",
       " np.float64(0.3011093344323173),\n",
       " np.float64(0.3555299804718019),\n",
       " np.float64(0.31279757786746004),\n",
       " np.float64(0.3062745634197625),\n",
       " np.float64(0.31540614075135104),\n",
       " np.float64(0.3073646034931301),\n",
       " np.float64(0.32850520523876053),\n",
       " np.float64(0.2868286264541298),\n",
       " np.float64(0.30146383711230884),\n",
       " np.float64(0.31231718734721303),\n",
       " np.float64(0.3251354111071609),\n",
       " np.float64(0.3295762473780636),\n",
       " np.float64(0.30554270928538646),\n",
       " np.float64(0.3057524375888075),\n",
       " np.float64(0.3169192555569122)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crj_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614d25e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nrmse_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnrmse_dict\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mCRJ\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1000\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'nrmse_dict' is not defined"
     ]
    }
   ],
   "source": [
    "nrmse_dict['CRJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a243d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "HFR              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        nan ± nan         \n",
      "600        nan ± nan         \n",
      "1000       nan ± nan         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:223: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:181: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "c:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:215: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'HFR':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['HFR']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe19f3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
