{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import odeint\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Ridge\nimport json\nimport itertools\nfrom tqdm import tqdm","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-08T04:36:31.531679Z","iopub.execute_input":"2025-06-08T04:36:31.531906Z","iopub.status.idle":"2025-06-08T04:36:33.330283Z","shell.execute_reply.started":"2025-06-08T04:36:31.531885Z","shell.execute_reply":"2025-06-08T04:36:33.329083Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def chen_deriv(state, t, a=35.0, b=3.0, c=28.0):\n    \"\"\"\n    Computes derivatives [dx/dt, dy/dt, dz/dt] for Chen system:\n      dx/dt = a*(y - x)\n      dy/dt = (c - a)*x + c*y - x*z\n      dz/dt = x*y - b*z\n    \"\"\"\n    x, y, z = state\n    dxdt = a*(y - x)\n    dydt = (c - a)*x + c*y - x*z\n    dzdt = x*y - b*z\n    return [dxdt, dydt, dzdt]\n\ndef generate_chen_data(\n    initial_state=[1.0, 1.0, 1.0],\n    tmax=50.0,\n    dt=0.01,\n    a=35.0,\n    b=3.0,\n    c=28.0\n):\n    \"\"\"\n    Integrates Chen's system from 'initial_state' up to time 'tmax' with step size 'dt'.\n    Returns:\n      t_vals: time array of length T\n      sol   : array shape [T, 3], the trajectory [x(t), y(t), z(t)]\n    \"\"\"\n    num_steps = int(tmax / dt)\n    t_vals = np.linspace(0, tmax, num_steps)\n    sol = odeint(chen_deriv, initial_state, t_vals, args=(a, b, c))\n    return t_vals, sol","metadata":{"execution":{"iopub.status.busy":"2025-06-08T04:36:33.331886Z","iopub.execute_input":"2025-06-08T04:36:33.332392Z","iopub.status.idle":"2025-06-08T04:36:33.338966Z","shell.execute_reply.started":"2025-06-08T04:36:33.332359Z","shell.execute_reply":"2025-06-08T04:36:33.337973Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def evaluate_nrmse(all_preds, test_target, horizons):\n    \"\"\"\n    Evaluate model performance over multiple prediction horizons\n    for teacher-forced single-step forecasting or autoregressive rollout.\n    \"\"\"\n    horizon_nrmse = {}\n    for horizon in horizons:\n        preds = all_preds[:horizon]\n        targets = test_target[:horizon]\n        squared_errors = (preds - targets) ** 2\n        variance = np.var(targets, axis=0)\n        variance[variance == 0] = 1e-8  # avoid divide-by-zero\n        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * np.sum(variance)))\n        horizon_nrmse[horizon] = nrmse\n    return horizon_nrmse","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:36:33.341188Z","iopub.execute_input":"2025-06-08T04:36:33.341444Z","iopub.status.idle":"2025-06-08T04:36:33.364747Z","shell.execute_reply.started":"2025-06-08T04:36:33.341423Z","shell.execute_reply":"2025-06-08T04:36:33.363786Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n    \"\"\"\n    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n    \n    Parameters\n    ----------\n    y_true : ndarray of shape (N, dim)\n        True trajectory over time.\n    y_pred : ndarray of shape (N, dim)\n        Model's predicted trajectory over time (closed-loop).\n    t_vals : ndarray of shape (N,)\n        Time values corresponding to the trajectory steps.\n    threshold : float, optional\n        The error threshold, default is 0.4 as in your snippet.\n    lambda_max : float, optional\n        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n        \n    Returns\n    -------\n    T_VPT : float\n        Valid prediction time. The earliest time at which normalized error surpasses threshold\n        (or the last time if never surpassed).\n    T_lambda : float\n        Lyapunov time = 1 / lambda_max\n    ratio : float\n        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n    \"\"\"\n    # 1) Average of y_true\n    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n    \n    # 2) Time-averaged norm^2 of (y_true - y_mean)\n    y_centered = y_true - y_mean\n    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n    \n    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n    diff = y_true - y_pred\n    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n    delta_gamma = err_sq / denom      # shape (N,)\n    \n    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n    idx_exceed = np.where(delta_gamma > threshold)[0]\n    if len(idx_exceed) == 0:\n        # never exceeds threshold => set T_VPT to the final time\n        T_VPT = t_vals[-1]\n    else:\n        T_VPT = t_vals[idx_exceed[0]]\n    \n    # 5) Compute T_lambda and ratio\n    T_lambda = 1.0 / lambda_max\n\n    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n\n    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n    ratio = T_VPT / T_lambda\n\n    return T_VPT, T_lambda, ratio","metadata":{"execution":{"iopub.status.busy":"2025-06-08T04:36:33.365755Z","iopub.execute_input":"2025-06-08T04:36:33.366085Z","iopub.status.idle":"2025-06-08T04:36:33.386078Z","shell.execute_reply.started":"2025-06-08T04:36:33.366055Z","shell.execute_reply":"2025-06-08T04:36:33.385131Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n    \"\"\"\n    Compute the Attractor Deviation (ADev) metric.\n\n    Parameters:\n        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n        targets (numpy.ndarray): True trajectories of shape (n, 3).\n        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n\n    Returns:\n        float: The ADev metric.\n    \"\"\"\n    # Define the cube grid based on the range of the data and cube size\n    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n\n    # Create a grid of cubes\n    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n\n    # Initialize the cube occupancy arrays\n    pred_cubes = np.zeros(grid_shape, dtype=int)\n    target_cubes = np.zeros(grid_shape, dtype=int)\n\n    # Map trajectories to cubes\n    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n    target_indices = ((targets - min_coords) / cube_size).astype(int)\n\n    # Mark cubes visited by predictions and targets\n    for idx in pred_indices:\n        pred_cubes[tuple(idx)] = 1\n    for idx in target_indices:\n        target_cubes[tuple(idx)] = 1\n\n    # Compute the ADev metric\n    adev = np.sum(np.abs(pred_cubes - target_cubes))\n    return adev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:36:33.386828Z","iopub.execute_input":"2025-06-08T04:36:33.387094Z","iopub.status.idle":"2025-06-08T04:36:33.414964Z","shell.execute_reply.started":"2025-06-08T04:36:33.387072Z","shell.execute_reply":"2025-06-08T04:36:33.413781Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def scale_spectral_radius(W, target_radius=0.95):\n    \"\"\"\n    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n    \"\"\"\n    eigvals = np.linalg.eigvals(W)\n    radius = np.max(np.abs(eigvals))\n    if radius == 0:\n        return W\n    return (W / radius) * target_radius\n\ndef augment_state_with_squares(x):\n    \"\"\"\n    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n    We'll use this for both training and prediction.\n    \"\"\"\n    x_sq = x**2\n    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1\n\nclass HFRRes3D:\n    \"\"\"\n    Hierarchical Fractal Reservoir (HFR) for 3D chaotic systems.\n    \n    This novel reservoir architecture partitions the chaotic attractor at multiple\n    hierarchical scales, combining them in a fractal-like adjacency structure.\n    The method is model-free, relying solely on the observed trajectory in R^3,\n    and does not require knowledge of any system parameters such as sigma, rho, beta\n    for Lorenz63. \n    \n    Key Idea:\n     1) Define multiple 'scales' of partition of the data's bounding region.\n     2) Each scale is subdivided into a certain number of cells (regions).\n     3) Each cell at level l has links to both:\n        - other cells at the same level (horizontal adjacency),\n        - 'child' cells at the finer level l+1 (vertical adjacency).\n     4) We gather all cells across levels => a multi-level fractal graph => adjacency => W.\n     5) We build a typical ESN from this adjacency, feed data with W_in, run leaky tanh updates,\n        then do a polynomial readout for 3D next-step prediction.\n\n    This approach is suitable for chaotic systems whose attractors often exhibit fractal\n    self-similarity, thus capturing multi-scale structures in a single reservoir.\n    \"\"\"\n\n    def __init__(self,\n                 n_levels=3,             # number of hierarchical levels\n                 cells_per_level=None,   # list of number of cells at each level, e.g. [8, 32, 128]\n                 spectral_radius=0.95,\n                 input_scale=1.0,\n                 leaking_rate=1.0,\n                 ridge_alpha=1e-6,\n                 seed=42):\n        \"\"\"\n        Parameters\n        ----------\n        n_levels       : int, number of hierarchical scales\n        cells_per_level: list[int], the number of partitions/cells at each level\n                         if None, we auto-generate e.g. 2^(level+2)\n        spectral_radius: final scaling for adjacency\n        input_scale    : random input scale W_in\n        leaking_rate   : ESN leaky alpha\n        ridge_alpha    : readout ridge penalty\n        seed           : random seed\n        \"\"\"\n        self.n_levels        = n_levels\n        self.cells_per_level = cells_per_level\n        self.spectral_radius = spectral_radius\n        self.input_scale     = input_scale\n        self.leaking_rate    = leaking_rate\n        self.ridge_alpha     = ridge_alpha\n        self.seed            = seed\n\n        if self.cells_per_level is None:\n            # default scheme e.g. 8, 16, 32 for 3 levels\n            self.cells_per_level = [8*(2**i) for i in range(n_levels)]\n\n        # We'll store adjacency W, input W_in, readout W_out, reservoir state x\n        self.W     = None\n        self.W_in  = None\n        self.W_out = None\n        self.x     = None\n        self.n_levels = len(self.cells_per_level)\n\n        # We'll define a total number of nodes = sum(cells_per_level)\n        self.n_nodes = sum(self.cells_per_level)\n\n    def _build_partitions(self, data_3d):\n        \"\"\"\n        Build hierarchical partitions for each level.\n        We'll store the bounding box for data_3d, then for each level l in [0..n_levels-1]\n        run e.g. k-means with K = cells_per_level[l], each point gets a label => we track transitions.\n\n        Return: \n          partitions => list of arrays, partitions[l] => shape (N, ) cluster assignment in [0..cells_per_level[l]-1]\n        \"\"\"\n        from sklearn.cluster import KMeans\n        N = len(data_3d)\n        partitions = []\n\n        for level in range(self.n_levels):\n            k = self.cells_per_level[level]\n            # cluster\n            kmeans = KMeans(n_clusters=k, random_state=self.seed+10*level, n_init='auto')\n            kmeans.fit(data_3d)\n            labels = kmeans.predict(data_3d)\n            partitions.append(labels)\n\n        return partitions\n\n    def _build_hierarchical_adjacency(self, data_3d):\n        \"\"\"\n        Build a block adjacency with cross-level links, then scale spectral radius.\n        Steps:\n          1) Build partitions for each level => partitions[l] in [0..cells_per_level[l]-1]\n          2) For each level l, build a transition matrix T_l of shape (cells_per_level[l], cells_per_level[l]).\n          3) Link scale l to scale l+1 by figuring out which cluster i at scale l maps to which cluster j at scale l+1\n             for each sample t => link i-> j if data_3d[t] is in i at scale l and j at scale l+1.\n          4) Combine all transitions in one big adjacency W in R^(n_nodes x n_nodes).\n          5) row-normalize W => scale largest eigenvalue => spectral_radius\n        \"\"\"\n        partitions = self._build_partitions(data_3d)\n        N = len(data_3d)\n\n        # offsets for each level => to index big W\n        offsets = []\n        running = 0\n        for level in range(self.n_levels):\n            offsets.append(running)\n            running += self.cells_per_level[level]\n\n        # total nodes\n        n_tot = self.n_nodes\n        # initialize adjacency\n        A = np.zeros((n_tot, n_tot))\n\n        # 1) horizontal adjacency in each level\n        for level in range(self.n_levels):\n            k = self.cells_per_level[level]\n            labels = partitions[level]\n            # T_l => shape (k, k)\n            T_l = np.zeros((k, k))\n            for t in range(N-1):\n                i = labels[t]\n                j = labels[t+1]\n                T_l[i,j]+=1\n            # row normalize\n            row_sum = T_l.sum(axis=1, keepdims=True)\n            row_sum[row_sum==0.0] = 1.0\n            T_l /= row_sum\n            # place T_l into big A\n            off = offsets[level]\n            A[off:off+k, off:off+k] = T_l\n\n        # 2) vertical adjacency between scale l and l+1\n        for level in range(self.n_levels-1):\n            k_l   = self.cells_per_level[level]\n            k_lp1 = self.cells_per_level[level+1]\n            labels_l   = partitions[level]\n            labels_lp1 = partitions[level+1]\n            # we define adjacency from i in [0..k_l-1] to j in [0..k_lp1-1] if the same sample t belongs to i at level l and j at l+1\n            # Count how many times\n            Xvert1 = np.zeros((k_l, k_lp1))\n            for t in range(N):\n                i = labels_l[t]\n                j = labels_lp1[t]\n                Xvert1[i,j]+=1\n            # row normalize\n            row_sum = Xvert1.sum(axis=1, keepdims=True)\n            row_sum[row_sum==0.0] = 1.0\n            Xvert = Xvert1/row_sum\n            # place in big A\n            off_l   = offsets[level]\n            off_lp1 = offsets[level+1]\n            A[off_l:off_l+k_l, off_lp1:off_lp1+k_lp1] = Xvert\n            # tentative idea, we could also define adjacency from l+1 -> l (parent link), if desired\n            # we do the same for the 'child -> parent' link or skip it if we only want forward adjacency\n            # For now, let's do symmetrical\n            Yvert = Xvert1.T\n            col_sum = Yvert.sum(axis=1, keepdims=True)\n            col_sum[col_sum==0.0] = 1.0\n            Yvert /= col_sum\n            A[off_lp1:off_lp1+k_lp1, off_l:off_l+k_l] = Yvert\n\n        # now we have a big adjacency => row normalize again, then scale spectral radius\n        row_sum = A.sum(axis=1, keepdims=True)\n        row_sum[row_sum==0.0] = 1.0\n        A /= row_sum\n\n        A = scale_spectral_radius(A, self.spectral_radius)\n        return A\n\n    def fit_readout(self, train_input, train_target, discard=100):\n        \"\"\"\n        Main training routine:\n          1) Build hierarchical adjacency from fractal partition => self.W\n          2) define W_in => shape(n_nodes, 3)\n          3) teacher forcing => polynomial readout => solve => self.W_out\n        \"\"\"\n        np.random.seed(self.seed)\n        # Build adjacency\n        W_big = self._build_hierarchical_adjacency(train_input)\n        self.W = W_big\n\n        # define W_in => shape(n_nodes,3)\n        self.n_nodes = W_big.shape[0]\n        self.W_in = (np.random.rand(self.n_nodes,3)-0.5)*2.0*self.input_scale\n\n        # define reservoir state\n        self.x = np.zeros(self.n_nodes)\n\n        # gather states => teacher forcing => polynomial => readout\n        states_use, _ = self.collect_states(train_input, discard=discard)\n        target_use = train_target[discard:]\n        X_list= []\n        for s in states_use:\n            X_list.append( augment_state_with_squares(s) )\n        X_aug= np.array(X_list)\n\n        reg= Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n        reg.fit(X_aug, target_use)\n        self.W_out= reg.coef_\n\n    def collect_states(self, inputs, discard=100):\n        \"\"\"\n        Teacher forcing => feed real 3D => gather states => shape => [T-discard, n_nodes].\n        returns (states_after_discard, states_discarded).\n        \"\"\"\n        self.reset_state()\n        states= []\n        for val in inputs:\n            self._update(val)\n            states.append(self.x.copy())\n        states= np.array(states)\n        return states[discard:], states[:discard]\n\n    def reset_state(self):\n        if self.x is not None:\n            self.x.fill(0.0)\n\n    def _update(self, u):\n        \"\"\"\n        x(t+1)= (1-alpha)x(t)+ alpha tanh( W*x(t)+ W_in*u(t) ).\n        \"\"\"\n        alpha= self.leaking_rate\n        pre_acts= self.W@self.x + self.W_in@u\n        x_new= np.tanh(pre_acts)\n        self.x= (1.0- alpha)*self.x+ alpha*x_new\n\n    def predict_autoregressive(self, initial_input, n_steps):\n        \"\"\"\n        fully autonomous => feed last predicted => next input\n        \"\"\"\n        preds= []\n        #self.reset_state()\n        current_in= np.array(initial_input)\n        for _ in range(n_steps):\n            self._update(current_in)\n            big_x= augment_state_with_squares(self.x)\n            out= self.W_out@big_x\n            preds.append(out)\n            current_in= out\n        return np.array(preds)\n    \n    def predict_open_loop(self, test_input):\n        preds = []\n        for true_input in test_input:\n            self._update(true_input)\n            x_aug = augment_state_with_squares(self.x)\n            out = self.W_out @ x_aug\n            preds.append(out)\n        return np.array(preds)","metadata":{"execution":{"iopub.status.busy":"2025-06-08T04:36:33.416148Z","iopub.execute_input":"2025-06-08T04:36:33.416366Z","iopub.status.idle":"2025-06-08T04:36:33.532951Z","shell.execute_reply.started":"2025-06-08T04:36:33.416347Z","shell.execute_reply":"2025-06-08T04:36:33.531379Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class CRJ3D:\n    \"\"\"\n    Cycle Reservoir with Jumps (CRJ) for 3D->3D single-step tasks.\n    We form a ring adjacency with an extra 'jump' edge in each row.\n    This can help capture multiple timescales or delayed memory\n    while retaining the easy ring structure.\n\n    The adjacency is built as follows (reservoir_size = mod N):\n      For each i in [0..N-1]:\n        W[i, (i+1) % mod N] = 1.0\n        W[i, (i+jump) % mod N] = 1.0\n    Then we scale by 'spectral_radius.' We do an ESN update\n    with readout [ x, x^2, 1 ] -> next step in R^3.\n    \"\"\"\n\n    def __init__(self,\n                 reservoir_size=300,\n                 jump=10,                # offset for the jump\n                 spectral_radius=0.95,\n                 input_scale=1.0,\n                 leaking_rate=1.0,\n                 ridge_alpha=1e-6,\n                 seed=42):\n        \"\"\"\n        reservoir_size: how many nodes in the ring\n        jump            : the offset for the 2nd connection from node i\n        spectral_radius : scale adjacency\n        input_scale     : scale factor for W_in\n        leaking_rate    : ESN 'alpha'\n        ridge_alpha     : ridge penalty for readout\n        seed            : random seed\n        \"\"\"\n        self.reservoir_size = reservoir_size\n        self.jump = jump\n        self.spectral_radius = spectral_radius\n        self.input_scale = input_scale\n        self.leaking_rate = leaking_rate\n        self.ridge_alpha = ridge_alpha\n        self.seed = seed\n\n        # build adjacency\n        np.random.seed(self.seed)\n        W = np.zeros((reservoir_size, reservoir_size))\n        for i in range(reservoir_size):\n            # cycle edge: i -> (i+1)%N\n            W[i, (i+1) % reservoir_size] = 1.0\n            # jump edge: i -> (i+jump)%N\n            W[i, (i + self.jump) % reservoir_size] = 1.0\n\n        # scale spectral radius\n        W = scale_spectral_radius(W, self.spectral_radius)\n        self.W = W\n\n        # input weights => shape [N,3]\n        np.random.seed(self.seed+100)\n        W_in = (np.random.rand(reservoir_size, 3) - 0.5)*2.0*self.input_scale\n        self.W_in = W_in\n\n        # readout\n        self.W_out = None\n        self.x = np.zeros(self.reservoir_size)\n\n    def reset_state(self):\n        self.x = np.zeros(self.reservoir_size)\n\n    def _update(self, u):\n        \"\"\"\n        Single-step ESN update:\n          x(t+1) = (1-alpha)*x(t) + alpha*tanh( W x(t) + W_in u(t) )\n        \"\"\"\n        pre_activation = self.W @ self.x + self.W_in @ u\n        x_new = np.tanh(pre_activation)\n        alpha = self.leaking_rate\n        self.x = (1.0 - alpha)*self.x + alpha*x_new\n\n    def collect_states(self, inputs, discard=100):\n        \"\"\"\n        Teacher forcing => feed the real 3D inputs => gather states.\n        Return (states_after_discard, states_discarded).\n        \"\"\"\n        self.reset_state()\n        states = []\n        for val in inputs:\n            self._update(val)\n            states.append(self.x.copy())\n        states = np.array(states)\n        return states[discard:], states[:discard]\n\n    def fit_readout(self, train_input, train_target, discard=100):\n        \"\"\"\n        gather states => polynomial readout => solve ridge\n        \"\"\"\n        states_use, _ = self.collect_states(train_input, discard=discard)\n        target_use = train_target[discard:]\n        X_list = []\n        for s in states_use:\n            # polynomial expansion => [ x, x^2, 1 ]\n            X_list.append(augment_state_with_squares(s))\n        X_aug = np.array(X_list)\n\n        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n        reg.fit(X_aug, target_use)\n        self.W_out = reg.coef_  # shape => (3, 2N+1)\n\n    def predict_autoregressive(self, initial_input, n_steps):\n        \"\"\"\n        fully autoregressive => feed last output => next input\n        \"\"\"\n        preds = []\n        #self.reset_state()\n        current_in = np.array(initial_input)\n        for _ in range(n_steps):\n            self._update(current_in)\n            big_x = augment_state_with_squares(self.x)\n            out = self.W_out @ big_x  # shape => (3,)\n            preds.append(out)\n            current_in = out\n        return np.array(preds)\n        \n    def predict_open_loop(self, test_input):\n        preds = []\n        for true_input in test_input:\n            self._update(true_input)\n            x_aug = augment_state_with_squares(self.x)\n            out = self.W_out @ x_aug\n            preds.append(out)\n        return np.array(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:36:33.533973Z","iopub.execute_input":"2025-06-08T04:36:33.534287Z","iopub.status.idle":"2025-06-08T04:36:33.556899Z","shell.execute_reply.started":"2025-06-08T04:36:33.534263Z","shell.execute_reply":"2025-06-08T04:36:33.556152Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class SparseESN3D:\n    \"\"\"\n    Sparse random ESN for 3D->3D single-step,\n    teacher forcing for training, autoregressive for testing.\n    \"\"\"\n    def __init__(self,\n                 reservoir_size=300,\n                 spectral_radius=0.95,\n                 connectivity=0.05,\n                 input_scale=1.0,\n                 leaking_rate=1.0,\n                 ridge_alpha=1e-6,\n                 seed=42):\n        self.reservoir_size = reservoir_size\n        self.spectral_radius = spectral_radius\n        self.connectivity = connectivity\n        self.input_scale = input_scale\n        self.leaking_rate = leaking_rate\n        self.ridge_alpha = ridge_alpha\n        self.seed = seed\n\n        np.random.seed(self.seed)\n        W_full = np.random.randn(reservoir_size, reservoir_size)*0.1\n        mask = (np.random.rand(reservoir_size, reservoir_size) < self.connectivity)\n        W = W_full * mask\n        W = scale_spectral_radius(W, self.spectral_radius)\n        self.W = W\n\n        np.random.seed(self.seed+1)\n        self.W_in = (np.random.rand(reservoir_size,3) - 0.5)*2.0*self.input_scale\n\n        self.W_out = None\n        self.x = np.zeros(reservoir_size)\n\n    def reset_state(self):\n        self.x = np.zeros(self.reservoir_size)\n\n    def _update(self, u):\n        pre_activation = self.W @ self.x + self.W_in @ u\n        x_new = np.tanh(pre_activation)\n        alpha = self.leaking_rate\n        self.x = (1.0 - alpha)*self.x + alpha*x_new\n\n    def collect_states(self, inputs, discard=100):\n        self.reset_state()\n        states = []\n        for val in inputs:\n            self._update(val)\n            states.append(self.x.copy())\n        states = np.array(states)\n        return states[discard:], states[:discard]\n\n    def fit_readout(self, train_input, train_target, discard=100):\n        states_use, _ = self.collect_states(train_input, discard=discard)\n        targets_use = train_target[discard:]\n        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0],1))])\n\n        # polynomial readout\n        X_list = []\n        for s in states_use:\n            X_list.append(augment_state_with_squares(s))\n        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n\n        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n        reg.fit(X_aug, targets_use)\n        self.W_out = reg.coef_\n\n    def predict_autoregressive(self, initial_input, n_steps):\n        preds = []\n        current_in = np.array(initial_input)\n        for _ in range(n_steps):\n            self._update(current_in)\n            # x_aug = np.concatenate([self.x, [1.0]])\n            x_aug = augment_state_with_squares(self.x)\n            out = self.W_out @ x_aug\n            preds.append(out)\n            current_in = out\n        return np.array(preds)\n        \n    def predict_open_loop(self, test_input):\n        preds = []\n        for true_input in test_input:\n            self._update(true_input)\n            x_aug = augment_state_with_squares(self.x)\n            out = self.W_out @ x_aug\n            preds.append(out)\n        return np.array(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:36:33.557606Z","iopub.execute_input":"2025-06-08T04:36:33.557820Z","iopub.status.idle":"2025-06-08T04:36:33.581345Z","shell.execute_reply.started":"2025-06-08T04:36:33.557796Z","shell.execute_reply":"2025-06-08T04:36:33.580061Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class DeepESN3D:\n    \"\"\"\n    Deep Echo State Network (DeepESN) for multi-layered reservoir computing.\n    Each layer has its own reservoir, and the states are propagated through layers.\n    \"\"\"\n\n    def __init__(self,\n                 num_layers=3,\n                 reservoir_size=100,\n                 spectral_radius=0.95,\n                 connectivity=0.1,\n                 input_scale=1.0,\n                 leaking_rate=1.0,\n                 ridge_alpha=1e-6,\n                 activation_choices=('tanh','relu','sin','linear'),\n                 seed=42):\n        \"\"\"\n        Parameters:\n        - num_layers: Number of reservoir layers.\n        - reservoir_size: Number of neurons in each reservoir layer.\n        \"\"\"\n        self.num_layers = num_layers\n        self.reservoir_size = reservoir_size\n        self.spectral_radius = spectral_radius\n        self.connectivity = connectivity\n        self.input_scale = input_scale\n        self.leaking_rate = leaking_rate\n        self.ridge_alpha = ridge_alpha\n        self.activation_choices = activation_choices\n        self.seed = seed\n\n        # Initialize reservoirs and input weights for each layer\n        self.reservoirs = []\n        self.input_weights = []\n        self.states = []\n\n        np.random.seed(self.seed)\n        for layer in range(num_layers):\n            np.random.seed(seed + layer)\n            W = np.random.randn(reservoir_size, reservoir_size) * 0.1\n            mask = (np.random.rand(reservoir_size, reservoir_size) < self.connectivity)\n            W = W * mask\n            W = scale_spectral_radius(W, spectral_radius)\n            self.reservoirs.append(W)\n\n            if layer == 0 : \n                W_in = (np.random.rand(reservoir_size, 3) - 0.5) * 2.0 * input_scale\n            else:\n                W_in = (np.random.rand(reservoir_size, reservoir_size) - 0.5) * 2.0 * input_scale\n            self.input_weights.append(W_in)\n\n        np.random.seed(self.seed + 200)\n        self.node_activations = np.random.choice(self.activation_choices, size=self.reservoir_size)\n        \n        self.W_out = None\n        self.reset_state()\n\n    def reset_state(self):\n        \"\"\"\n        Reset the states of all reservoir layers.\n        \"\"\"\n        self.states = [np.zeros(self.reservoir_size) for _ in range(self.num_layers)]\n\n    def _apply_activation(self, act_type, val):\n        return np.tanh(val)\n        # if act_type=='tanh':\n        #     return np.tanh(val)\n        # elif act_type=='relu':\n        #     return max(0.0, val)\n        # elif act_type=='sin':\n        #     return np.sin(val)\n        # elif act_type=='linear':\n        #     return val\n        # else:\n        #     return np.tanh(val)\n\n    def _update_layer(self, layer_idx, u):\n        \"\"\"\n        Update a single reservoir layer.\n        \"\"\"\n        pre_activation = self.reservoirs[layer_idx] @ self.states[layer_idx]\n        if layer_idx == 0:\n            pre_activation += self.input_weights[layer_idx] @ u\n        else:\n            pre_activation += self.input_weights[layer_idx] @ self.states[layer_idx - 1]\n\n        x_new = np.zeros_like(pre_activation)\n        for i in range(self.reservoir_size):\n            activation = self.node_activations[i]\n            x_new[i] = self._apply_activation(activation, pre_activation[i])\n        alpha = self.leaking_rate\n        self.states[layer_idx] = (1.0 - alpha) * self.states[layer_idx] + alpha * x_new\n\n    def collect_states(self, inputs, discard=100):\n        self.reset_state()\n        all_states = []\n        for u in inputs:\n            for layer_idx in range(self.num_layers):\n                self._update_layer(layer_idx, u)\n            all_states.append(np.concatenate(self.states))\n        all_states = np.array(all_states)\n        return all_states[discard:], all_states[:discard]\n\n    def fit_readout(self, train_input, train_target, discard=100):\n        \"\"\"\n        Train the readout layer using ridge regression.\n        \"\"\"\n        states_use, _ = self.collect_states(train_input, discard=discard)\n        targets_use = train_target[discard:]\n\n        # Augment states with bias\n        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0], 1))])  # shape [T-discard, N*L+1]\n\n        # Quadratic readout\n        # Build augmented matrix [ x, x^2, 1 ]\n        X_list = []\n        for s in states_use:\n            X_list.append( np.concatenate([s, s**2, [1.0]]) )\n        X_aug = np.array(X_list)                                    # shape [T-discard, 2N*L+1]\n\n        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n        reg.fit(X_aug, targets_use)\n        self.W_out = reg.coef_\n\n    def predict_open_loop(self, inputs):\n        \"\"\"\n        Single-step-ahead inference on test data.\n        \"\"\"\n        preds = []\n        for u in inputs:\n            for layer_idx in range(self.num_layers):\n                self._update_layer(layer_idx, u)\n            state = np.concatenate(self.states)\n            # x_aug = np.concatenate([state, [1.0]])\n            x_aug = np.concatenate([state, (state)**2, [1.0]])  # For quadrartic readout\n            out = self.W_out @ x_aug\n            preds.append(out)\n        return np.array(preds)\n\n    def predict_autoregressive(self, initial_input, num_steps):\n        \"\"\"\n        Autoregressive multi-step forecasting for num_steps\n        \"\"\"\n        preds = []\n        current_input = initial_input.copy()\n\n        for _ in range(num_steps):\n            for layer_idx in range(self.num_layers):\n                self._update_layer(layer_idx, current_input)\n            state = np.concatenate(self.states)\n            # x_aug = np.concatenate([state, [1.0]])\n            x_aug = np.concatenate([state, (state)**2, [1.0]])  # For quadrartic readout\n            out = self.W_out @ x_aug\n            preds.append(out)\n            current_input = out\n        \n        return np.array(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:36:33.583579Z","iopub.execute_input":"2025-06-08T04:36:33.583814Z","iopub.status.idle":"2025-06-08T04:36:33.606432Z","shell.execute_reply.started":"2025-06-08T04:36:33.583794Z","shell.execute_reply":"2025-06-08T04:36:33.605251Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class MCI3D:\n    \"\"\"\n    Minimum Complexity Interaction ESN (MCI-ESN).\n\n    This class implements the approach described in:\n      \"A Minimum Complexity Interaction Echo State Network\"\n        by Jianming Liu, Xu Xu, Eric Li (2024).\n    \n    The model structure:\n      - We maintain two 'simple cycle' reservoirs (each of size N).\n      - Each reservoir is a ring with weight = l, i.e. \n            W_res[i, (i+1)%N] = l\n        plus the corner wrap from (N-1)->0, also = l. ##(unnecessary as already called for in the prev. line)\n      - The two reservoirs interact via a minimal connection matrix: \n         exactly 2 cross-connections with weight = g. \n         (One might connect x2[-1], x2[-2], ... \n          But we do where reservoir1 sees x2[-1] \n          in one location, and reservoir2 sees x1[-1] likewise.)\n      - Activation function in reservoir1 is cos(·), and in reservoir2 is sin(·).\n      - They each have a separate input weight matrix: Win1 and Win2. \n        The final state is a linear combination \n           x(t) = h*x1(t) + (1-h)*x2(t).\n      - Then we do a polynomial readout [x, x^2, 1] -> output.\n      - We feed teacher forcing in collect_states, \n        then solve readout with Ridge regression.\n\n    References:\n      - Liu, J., Xu, X., & Li, E. (2024). \n        \"A minimum complexity interaction echo state network,\" \n         Neural Computing and Applications.\n    \n    notes:\n      - The reservoir_size is N for each reservoir, \n        so total param dimension is 2*N for states, \n        but we produce a single final \"combined\" state x(t) in R^N for readout.\n      - The activation f1=cos(...) for reservoir1, f2=sin(...) for reservoir2, \n        as recommended by the paper for MCI-ESN.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        reservoir_size=500,\n        cycle_weight=0.9,      # 'l' in the paper\n        connect_weight=0.9,    # 'g' in the paper\n        input_scale=0.2,\n        leaking_rate=1.0,\n        ridge_alpha=1e-6,\n        combine_factor=0.1,    # 'h' in the paper\n        seed=47,\n        v1=0.6, v2=0.6         # fixed values for v1, v2\n    ):\n        \"\"\"\n        reservoir_size: N, size of each cycle reservoir \n        cycle_weight : l, ring adjacency weight in [0,1), ensures cycle synergy\n        connect_weight: g, cross-connection weight between the two cycle reservoirs\n        input_scale   : scale factor for input->reservoir weights\n        leaking_rate  : ESN update alpha \n        ridge_alpha   : readout ridge penalty\n        combine_factor: h in [0,1], to form x(t)= h*x1(t)+(1-h)*x2(t) as final combined state\n        seed          : random seed\n        \"\"\"\n        self.reservoir_size = reservoir_size\n        self.cycle_weight   = cycle_weight\n        self.connect_weight = connect_weight\n        self.input_scale    = input_scale\n        self.leaking_rate   = leaking_rate\n        self.ridge_alpha    = ridge_alpha\n        self.combine_factor = combine_factor\n        self.seed           = seed\n        self.v1 = v1\n        self.v2 = v2\n\n        # We'll define (and build) adjacency for each cycle, \n        # plus cross-connection for two sub-reservoirs.\n        # We'll define 2 input weight mats: Win1, Win2.\n        # We'll define states x1(t), x2(t).\n        # We'll define readout W_out after training.\n\n        self._build_mci_esn()\n\n    def _build_mci_esn(self):\n        \"\"\"\n        Build all the internal parameters: \n         - ring adjacency for each reservoir\n         - cross-reservoir connection\n         - input weights for each reservoir\n         - initial states\n        \"\"\"\n        np.random.seed(self.seed)\n\n        N = self.reservoir_size\n\n        # Build ring adjacency W_res in shape [N, N], with cycle_weight on ring\n        W_res = np.zeros((N, N))\n        for i in range(N):\n            j = (i+1) % N\n            W_res[j, i] = self.cycle_weight\n        self.W_res = W_res  # shared by both sub-reservoirs\n\n        # Build cross-connection W_cn for shape [N,N], \n        # minimal 2 nonzero elements. \n        # For the simplest approach from the paper:\n        #   W_cn[0, N-1] = g, W_cn[1, N-2] = g or similar.\n        # The paper's eq(7) suggests the last 2 elements in x(t) cross to first 2 in the other reservoir:\n        # We'll do the simplest reference: if i=0 or i=1, we connect from the other reservoir's last or second-last. \n        # We'll define a function for each sub-res to pick up from the other sub-res. \n        # We can store them in separate arrays, or define them in code. \n        # We'll just store \"We want index 0 to see x2[-1], index 1 to see x2[-2].\"\n\n        # But as done in the original code snippet from the paper:\n        #   Wcn has\n        # effectively 2 nonzero positions. We'll define that pattern:\n        W_cn = np.zeros((N, N))\n        # e.g. W_cn[0, N-1] = g, W_cn[N-1, N-2] = g or something. \n        # The paper example used W_cn = diag(0,g,...) plus the corner. We'll do the simplest:\n        # let W_cn[0, N-1]=g, W_cn[1, N-2]=g.\n        # This matches the minimal cross. \n        # For clarity we do:\n        W_cn[0, N-1] = self.connect_weight\n        if N>1:\n            # W_cn[1, N-2] = self.connect_weight\n            W_cn[N-1, 0] = self.connect_weight\n        self.W_cn = W_cn\n\n        # We'll define input weights for each sub-reservoir, shape [N, dim_input].\n        # The paper sets them as eq(10) in the snippet, with different signs. \n        # We'll define them as parted. \n        # We define V1, V2 => shape [N, dim_input], with constant magnitude t1, t2, random sign. \n        # We'll do random. Need to check this in the paper again\n        # We'll keep \"two\" separate. user can define input_scale but not two separate. \n        # We'll do the simplest approach: the absolute value is the same => input_scale, \n        # sign is random. Then we define Win1 = V1 - V2, Win2 = V1 + V2.\n        # This is consistent with eq(10) from the paper.\n\n        self.Win1 = None\n        self.Win2 = None\n\n        # We'll define states x1(t), x2(t). We'll do them after dimension known. \n        self.x1 = None\n        self.x2 = None\n\n        self.W_out = None\n\n    def _init_substates(self):\n        \"\"\"\n        Once we know reservoir_size, we define x1, x2 as zeros. \n        We'll call this in reset_state or at fit time.\n        \"\"\"\n        N = self.reservoir_size\n        self.x1 = np.zeros(N)\n        self.x2 = np.zeros(N)\n\n    def reset_state(self):\n        if self.x1 is not None:\n            self.x1[:] = 0.0\n        if self.x2 is not None:\n            self.x2[:] = 0.0\n\n    def _update(self, u):\n        \"\"\"\n        Single-step reservoir update.\n        x1(t+1) = cos( Win1*u(t+1) + W_res*x1(t) + W_cn*x2(t) )\n        x2(t+1) = sin( Win2*u(t+1) + W_res*x2(t) + W_cn*x1(t) )\n        Then x(t)= h*x1(t+1) + (1-h)* x2(t+1).\n        We'll define the leaky integration. \n        But the paper uses an approach with no leak? Be careful.\n        We'll do the approach: x1(t+1)= (1-alpha)* x1(t) + alpha*cos(...).\n        \"\"\"\n        alpha = self.leaking_rate\n\n        # pre activation for reservoir1\n        pre1 = self.Win1 @ u + self.W_res @ self.x1 + self.W_cn @ self.x2\n        # reservoir1 uses cos\n        new_x1 = np.cos(pre1)\n\n        # reservoir2 uses sin\n        pre2 = self.Win2 @ u + self.W_res @ self.x2 + self.W_cn @ self.x1\n        new_x2 = np.sin(pre2)\n\n        self.x1 = (1.0 - alpha)*self.x1 + alpha*new_x1\n        self.x2 = (1.0 - alpha)*self.x2 + alpha*new_x2\n\n    def _combine_state(self):\n        \"\"\"\n        Combine x1(t), x2(t) => x(t) = h*x1 + (1-h)*x2\n        \"\"\"\n        h = self.combine_factor\n        return h*self.x1 + (1.0 - h)*self.x2\n\n    def collect_states(self, inputs, discard=100):\n        # We reset the reservoir to zero\n        self.reset_state()\n        states = []\n        for t in range(len(inputs)):\n            self._update(inputs[t])   # feed the REAL input from the dataset\n            combined = self._combine_state()\n            states.append(combined.copy())\n        states = np.array(states)  # shape => [T, N]\n        return states[discard:], states[:discard]\n\n\n    def fit_readout(self, train_input, train_target, discard=100):\n        \"\"\"\n        Build input weights if needed, gather states on the training data (teacher forcing),\n        then solve a polynomial readout [x, x^2, 1]->train_target(t).\n\n        train_input : shape [T, d_in]\n        train_target: shape [T, d_out]\n        discard     : # of states to discard for warmup\n        \"\"\"\n        T = len(train_input)\n        if T<2:\n            raise ValueError(\"Not enough training data\")\n\n        d_in = train_input.shape[1]\n        # d_out = train_target.shape[1]\n\n        # built Win1, Win2\n        if self.Win1 is None or self.Win2 is None:\n            np.random.seed(self.seed+100)\n            # build V1, V2 in shape [N, d_in]\n            N = self.reservoir_size\n            # V1 = (np.random.rand(N, d_in)-0.5)*2.0*self.input_scale\n            # V2 = (np.random.rand(N, d_in)-0.5)*2.0*self.input_scale\n\n            sign_V1 = np.random.choice([-1, 1], size=(N, d_in))\n            sign_V2 = np.random.choice([-1, 1], size=(N, d_in))\n\n            v1, v2 = self.v1, self.v2  # fixed values for V1, V2\n\n            V1 = v1 * sign_V1 * self.input_scale\n            V2 = v2 * sign_V2 * self.input_scale\n\n            # eq(10): Win1= V1 - V2, Win2= V1 + V2\n            self.Win1 = V1 - V2\n            self.Win2 = V1 + V2\n\n        # define x1, x2\n        self._init_substates()\n\n        # gather states\n        states_use, _ = self.collect_states(train_input, discard=discard)\n        target_use = train_target[discard:]  # shape => [T-discard, d_out]\n\n        # polynomial readout\n        X_list = []\n        for s in states_use:\n            X_list.append(augment_state_with_squares(s))\n        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n\n        # Solve ridge\n        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n        reg.fit(X_aug, target_use)\n        # W_out => shape [d_out, 2N+1]\n        self.W_out = reg.coef_\n\n    def predict_autoregressive(self, initial_input, n_steps):\n        \"\"\"\n        Fully autoregressive: \n          We do not use teacher forcing, \n          we feed the model's last output as the next input \n        Typically, for MCI-ESN the paper does input(t+1) in R^d. \n        We do the test_input\n        For multi-step chaotic forecast, we feed the model's output as input? \n        That means the system dimension d_in must match d_out. \n        \"\"\"\n        preds = []\n        # re-init states\n        #self._init_substates()\n\n        # we assume initial_input => shape (d_in,)\n        current_in = np.array(initial_input)\n\n        for _ in range(n_steps):\n            self._update(current_in)\n            # read out\n            combined = self._combine_state()\n            big_x = augment_state_with_squares(combined)\n            out = self.W_out @ big_x  # shape => (d_out,)\n\n            preds.append(out)\n            current_in = out  # feed output back as next input\n\n        return np.array(preds)\n        \n    def predict_open_loop(self, test_input):\n        preds = []\n        for true_input in test_input:\n            self._update(true_input)\n            combined = self._combine_state()\n            x_aug = augment_state_with_squares(combined)\n            out = self.W_out @ x_aug\n            preds.append(out)\n        return np.array(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:36:33.607288Z","iopub.execute_input":"2025-06-08T04:36:33.607536Z","iopub.status.idle":"2025-06-08T04:36:33.634943Z","shell.execute_reply.started":"2025-06-08T04:36:33.607513Z","shell.execute_reply":"2025-06-08T04:36:33.634141Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class CR3D:\n    \"\"\"\n    Cycle (ring) reservoir for 3D->3D single-step,\n    teacher forcing for training, autoregressive for testing.\n    \"\"\"\n    def __init__(self,\n                 reservoir_size=300,\n                 spectral_radius=0.95,\n                 input_scale=1.0,\n                 leaking_rate=1.0,\n                 ridge_alpha=1e-6,\n                 seed=42):\n        self.reservoir_size = reservoir_size\n        self.spectral_radius = spectral_radius\n        self.input_scale = input_scale\n        self.leaking_rate = leaking_rate\n        self.ridge_alpha = ridge_alpha\n        self.seed = seed\n\n        np.random.seed(self.seed)\n        W = np.zeros((reservoir_size, reservoir_size))\n        for i in range(reservoir_size):\n            j = (i+1) % reservoir_size\n            W[i, j] = 1.0\n        W = scale_spectral_radius(W, self.spectral_radius)\n        self.W = W\n        \n        np.random.seed(self.seed+1)\n        self.W_in = (np.random.rand(reservoir_size,3) - 0.5)*2.0*self.input_scale\n\n        self.W_out = None\n        self.x = np.zeros(reservoir_size)\n\n    def reset_state(self):\n        self.x = np.zeros(self.reservoir_size)\n\n    def _update(self, u):\n        pre_activation = self.W @ self.x + self.W_in @ u\n        x_new = np.tanh(pre_activation)\n        alpha = self.leaking_rate\n        self.x = (1.0 - alpha)*self.x + alpha*x_new\n\n    def collect_states(self, inputs, discard=100):\n        self.reset_state()\n        states = []\n        for val in inputs:\n            self._update(val)\n            states.append(self.x.copy())\n        states = np.array(states)\n        return states[discard:], states[:discard]\n\n    def fit_readout(self, train_input, train_target, discard=100):\n        states_use, _ = self.collect_states(train_input, discard=discard)\n        targets_use = train_target[discard:]\n        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0],1))])\n\n        # polynomial readout\n        X_list = []\n        for s in states_use:\n            X_list.append(augment_state_with_squares(s))\n        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n\n        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n        reg.fit(X_aug, targets_use)\n        self.W_out = reg.coef_\n\n    def predict_autoregressive(self, initial_input, n_steps):\n        preds = []\n        current_in = np.array(initial_input)\n        for _ in range(n_steps):\n            self._update(current_in)\n            # x_aug = np.concatenate([self.x, [1.0]])\n            x_aug = augment_state_with_squares(self.x)\n            out = self.W_out @ x_aug\n            preds.append(out)\n            current_in = out\n        return np.array(preds)\n    \n    def predict_open_loop(self, test_input):\n        preds = []\n        for true_input in test_input:\n            self._update(true_input)\n            x_aug = augment_state_with_squares(self.x)\n            out = self.W_out @ x_aug\n            preds.append(out)\n        return np.array(preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T04:36:33.635579Z","iopub.execute_input":"2025-06-08T04:36:33.635817Z","iopub.status.idle":"2025-06-08T04:36:33.667147Z","shell.execute_reply.started":"2025-06-08T04:36:33.635798Z","shell.execute_reply":"2025-06-08T04:36:33.666352Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"grid = {\n    # \"cells_per_level\": [[5, 10, 15, 20, 35, 40, 45, 60, 70]],\n    \"jump\": [5],\n    # \"connectivity\": [0.2],\n    \"spectral_radius\": [0.92],\n    # \"cycle_weight\": [0.4],\n    # \"connect_weight\": [1],\n    \"input_scale\": [1],\n    \"leaking_rate\": [0.9],\n    # \"combine_factor\": [0.6],\n    \"ridge_alpha\": [1e-7],\n}","metadata":{"execution":{"iopub.status.busy":"2025-06-08T05:02:25.987458Z","iopub.execute_input":"2025-06-08T05:02:25.987751Z","iopub.status.idle":"2025-06-08T05:02:25.992800Z","shell.execute_reply.started":"2025-06-08T05:02:25.987725Z","shell.execute_reply":"2025-06-08T05:02:25.991582Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def run_grid_search(model_class, param_grid, model_name,\n                    output_path=\"grid_search_results.json\"):\n    combos = list(itertools.product(*param_grid.values()))\n    param_keys = list(param_grid.keys())\n    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n\n    results = []\n    horizons = list(range(10, 1001, 10))\n\n    for comb in tqdm(combos, desc=\"Grid Search\"):\n        params = dict(zip(param_keys, comb))\n        # seed_scores_vpt = []\n        horizon_nrmse_all = {h: [] for h in horizons}\n        # adev_scores = []\n        # ldev_scores = []\n\n        for initial_state in [[1.0, 1.0, 1.0], [1.0, 2.0, 3.0], [2.0, 1.5, 4.0]]:\n            tmax = 250\n            dt = 0.02\n            t_vals, lorenz_traj = generate_chen_data(\n                initial_state=initial_state,\n                tmax=tmax,\n                dt=dt\n            )\n\n            washout = 2000\n            t_vals = t_vals[washout:]\n            lorenz_traj = lorenz_traj[washout:]\n\n            scaler = MinMaxScaler()\n            scaler.fit(lorenz_traj)\n            lorenz_traj = scaler.transform(lorenz_traj)\n\n            T_data = len(lorenz_traj)\n            for train_frac in [0.7, 0.75, 0.8]:\n                train_end = int(train_frac * (T_data - 1))\n                train_input = lorenz_traj[:train_end]\n                train_target = lorenz_traj[1:train_end + 1]\n                test_input = lorenz_traj[train_end:-1]\n                test_target = lorenz_traj[train_end + 1:]\n                n_test_steps = len(test_input)\n                initial_in = test_input[0]\n\n                for seed in np.arange(1, 6):\n                    model = model_class(**params, seed=seed)\n                    model.fit_readout(train_input, train_target, discard=100)\n                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n\n                    # T_VPT_s, _, ratio = compute_valid_prediction_time(test_target, preds, t_vals, 0.4, 0.071, dt)\n                    # seed_scores_vpt.append(T_VPT)\n\n                    horizon_nrmse = evaluate_nrmse(preds, test_target, horizons)\n                    for h in horizons:\n                        horizon_nrmse_all[h].append(horizon_nrmse[h])\n\n                    # adev = compute_attractor_deviation(preds, test_target)\n                    # adev_scores.append(adev)\n\n                    # ldev = compute_lyapunov_exponent(\"Lorenz\", preds, dt)\n                    # ldev_scores.append(ldev)\n\n        # mean_vpt = float(np.mean(seed_scores_vpt))\n        # std_vpt = float(np.std(seed_scores_vpt))\n        mean_nrmse_dict = {str(h): float(np.mean(horizon_nrmse_all[h])) for h in horizons}\n        # std_nrmse_dict  = {str(h): float(np.std(horizon_nrmse_all[h]))  for h in horizons}\n        # mean_adev = float(np.mean(adev_scores))\n        # std_adev = float(np.std(adev_scores))\n        # mean_ldev = float(np.mean(ldev_scores))\n        # std_ldev = float(np.std(ldev_scores))\n\n        results.append({\n            # \"params\": params,\n            # \"seed_scores_T_VPT\": seed_scores_vpt,\n            # \"mean_T_VPT\": mean_vpt,\n            # \"std_T_VPT\": std_vpt,\n            \"mean_NRMSEs\": mean_nrmse_dict,\n            # \"std_NRMSEs\": std_nrmse_dict,\n            # \"mean_ADev\": mean_adev,\n            # \"std_ADev\": std_adev,\n            # \"mean_LDev\": mean_ldev,\n            # \"std_LDev\": std_ldev\n        })\n\n    with open(output_path, \"w\") as f:\n        json.dump(results, f, indent=2)\n    print(f\"\\nAll results saved to `{output_path}`\")\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2025-06-08T05:02:26.577250Z","iopub.execute_input":"2025-06-08T05:02:26.577517Z","iopub.status.idle":"2025-06-08T05:02:26.587497Z","shell.execute_reply.started":"2025-06-08T05:02:26.577497Z","shell.execute_reply":"2025-06-08T05:02:26.586330Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"run_grid_search(CRJ3D, grid, \"crj\", output_path=\"crj.json\")","metadata":{"execution":{"iopub.status.busy":"2025-06-08T05:02:27.157948Z","iopub.execute_input":"2025-06-08T05:02:27.158262Z","iopub.status.idle":"2025-06-08T05:02:53.279932Z","shell.execute_reply.started":"2025-06-08T05:02:27.158241Z","shell.execute_reply":"2025-06-08T05:02:53.278939Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n== Initial grid search for crj with 1 combinations ==\n","output_type":"stream"},{"name":"stderr","text":"Grid Search: 100%|██████████| 1/1 [00:26<00:00, 26.11s/it]","output_type":"stream"},{"name":"stdout","text":"\nAll results saved to `crj.json`\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[{'mean_NRMSEs': {'10': 0.0001067103266162546,\n   '20': 0.00015367684035856737,\n   '30': 0.0002734460090864755,\n   '40': 0.0003822483894050464,\n   '50': 0.0005170662716024251,\n   '60': 0.000985919181399445,\n   '70': 0.0011789542073478514,\n   '80': 0.0039031268019375977,\n   '90': 0.005632166581280571,\n   '100': 0.005948044873524257,\n   '110': 0.006865584914653936,\n   '120': 0.008646819559374862,\n   '130': 0.01778224900561981,\n   '140': 0.03140122690487811,\n   '150': 0.035852893823591116,\n   '160': 0.04363087468696096,\n   '170': 0.061510761096770936,\n   '180': 0.08299261378890087,\n   '190': 0.11322166541884658,\n   '200': 0.14748561655126427,\n   '210': 0.1780174215673384,\n   '220': 0.2117422849190264,\n   '230': 0.24028086770647966,\n   '240': 0.27373932210970525,\n   '250': 0.30155881828106745,\n   '260': 0.35823984331796,\n   '270': 0.39401975799271194,\n   '280': 0.4263209264424268,\n   '290': 0.47589512267689726,\n   '300': 0.5263343648727764,\n   '310': 0.5573701944620415,\n   '320': 0.5960368933899088,\n   '330': 0.6241568839771834,\n   '340': 0.6555725580765898,\n   '350': 0.6827452205947598,\n   '360': 0.7186971845589446,\n   '370': 0.7474878928972709,\n   '380': 0.7709292152535296,\n   '390': 0.8003941946743056,\n   '400': 0.825637948543711,\n   '410': 0.8512671173689031,\n   '420': 0.8704710765676744,\n   '430': 0.8871474676832259,\n   '440': 0.9020799044854823,\n   '450': 0.9228179766717378,\n   '460': 0.9353551710990182,\n   '470': 0.9484268974580738,\n   '480': 0.9603876640572762,\n   '490': 0.9770394944611491,\n   '500': 0.9900265448077484,\n   '510': 0.9974951634397938,\n   '520': 1.0033852139128732,\n   '530': 1.0109935335089415,\n   '540': 1.0196252441314462,\n   '550': 1.0288511107499545,\n   '560': 1.0351880462566772,\n   '570': 1.0416183565522308,\n   '580': 1.0483943702587937,\n   '590': 1.0563006082148907,\n   '600': 1.064733164852617,\n   '610': 1.0696936488915236,\n   '620': 1.0750598658436177,\n   '630': 1.0800995219277927,\n   '640': 1.0865744475911383,\n   '650': 1.0923619340916597,\n   '660': 1.0986639841158883,\n   '670': 1.102822721512543,\n   '680': 1.1077477759770338,\n   '690': 1.1127194626464745,\n   '700': 1.118023190927539,\n   '710': 1.1226810293485898,\n   '720': 1.128037034667369,\n   '730': 1.1306608992001792,\n   '740': 1.1339054401730562,\n   '750': 1.138506544563565,\n   '760': 1.14448907467417,\n   '770': 1.1487793992092519,\n   '780': 1.1524546689683706,\n   '790': 1.1578526620314789,\n   '800': 1.161388611689598,\n   '810': 1.1657030223240827,\n   '820': 1.1683292034004973,\n   '830': 1.172503307759215,\n   '840': 1.178025811783798,\n   '850': 1.1805842069127717,\n   '860': 1.1840374884300573,\n   '870': 1.1870491089429256,\n   '880': 1.1906699635199265,\n   '890': 1.1929219467450076,\n   '900': 1.1963799130365615,\n   '910': 1.198802245570927,\n   '920': 1.2018750576658572,\n   '930': 1.2052659467428946,\n   '940': 1.2075301410813164,\n   '950': 1.20935777968275,\n   '960': 1.2116353459136877,\n   '970': 1.2148509013321047,\n   '980': 1.2186545600120036,\n   '990': 1.22125706261071,\n   '1000': 1.2224517300977678}}]"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}