{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4f1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef868780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rossler_derivatives(state, t, a=0.2, b=0.2, c=5.7):\n",
    "    \"\"\"Compute time derivatives [dx/dt, dy/dt, dz/dt] for the Rössler system.\"\"\"\n",
    "    x, y, z = state\n",
    "    dxdt = -y - z\n",
    "    dydt = x + a * y\n",
    "    dzdt = b + z * (x - c)\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_rossler_data(\n",
    "    initial_state=[1.0, 0.0, 0.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    a=0.2,\n",
    "    b=0.2,\n",
    "    c=5.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Numerically integrate Rössler equations x'(t), y'(t), z'(t) using odeint.\n",
    "    Returns:\n",
    "       t_vals: array of time points\n",
    "       sol   : array shape [num_steps, 3] of [x(t), y(t), z(t)]\n",
    "    \"\"\"\n",
    "    num_steps = int(tmax / dt)\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(rossler_derivatives, initial_state, t_vals, args=(a, b, c))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a856f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n",
    "    \"\"\"\n",
    "    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (N, dim)\n",
    "        True trajectory over time.\n",
    "    y_pred : ndarray of shape (N, dim)\n",
    "        Model's predicted trajectory over time (closed-loop).\n",
    "    t_vals : ndarray of shape (N,)\n",
    "        Time values corresponding to the trajectory steps.\n",
    "    threshold : float, optional\n",
    "        The error threshold, default is 0.4 as in your snippet.\n",
    "    lambda_max : float, optional\n",
    "        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    T_VPT : float\n",
    "        Valid prediction time. The earliest time at which normalized error surpasses threshold\n",
    "        (or the last time if never surpassed).\n",
    "    T_lambda : float\n",
    "        Lyapunov time = 1 / lambda_max\n",
    "    ratio : float\n",
    "        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n",
    "    \"\"\"\n",
    "    # 1) Average of y_true\n",
    "    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n",
    "    \n",
    "    # 2) Time-averaged norm^2 of (y_true - y_mean)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n",
    "    \n",
    "    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n",
    "    diff = y_true - y_pred\n",
    "    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n",
    "    delta_gamma = err_sq / denom      # shape (N,)\n",
    "    \n",
    "    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n",
    "    idx_exceed = np.where(delta_gamma > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = t_vals[-1]\n",
    "    else:\n",
    "        T_VPT = t_vals[idx_exceed[0]]\n",
    "    \n",
    "    # 5) Compute T_lambda and ratio\n",
    "    T_lambda = 1.0 / lambda_max\n",
    "\n",
    "    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n",
    "\n",
    "    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n",
    "    ratio = T_VPT / T_lambda\n",
    "\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdb5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "def scale_spectral_radius(W, target_radius=0.95):\n",
    "    \"\"\"\n",
    "    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(W)\n",
    "    radius = np.max(np.abs(eigvals))\n",
    "    if radius == 0:\n",
    "        return W\n",
    "    return (W / radius) * target_radius\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"\n",
    "    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n",
    "    We'll use this for both training and prediction.\n",
    "    \"\"\"\n",
    "    x_sq = x**2\n",
    "    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1\n",
    "\n",
    "class MCI3D:\n",
    "    \"\"\"\n",
    "    Minimum Complexity Interaction ESN (MCI-ESN).\n",
    "\n",
    "    This class implements the approach described in:\n",
    "      \"A Minimum Complexity Interaction Echo State Network\"\n",
    "        by Jianming Liu, Xu Xu, Eric Li (2024).\n",
    "    \n",
    "    The model structure:\n",
    "      - We maintain two 'simple cycle' reservoirs (each of size N).\n",
    "      - Each reservoir is a ring with weight = l, i.e. \n",
    "            W_res[i, (i+1)%N] = l\n",
    "        plus the corner wrap from (N-1)->0, also = l. ##(unnecessary as already called for in the prev. line)\n",
    "      - The two reservoirs interact via a minimal connection matrix: \n",
    "         exactly 2 cross-connections with weight = g. \n",
    "         (One might connect x2[-1], x2[-2], ... \n",
    "          But we do where reservoir1 sees x2[-1] \n",
    "          in one location, and reservoir2 sees x1[-1] likewise.)\n",
    "      - Activation function in reservoir1 is cos(·), and in reservoir2 is sin(·).\n",
    "      - They each have a separate input weight matrix: Win1 and Win2. \n",
    "        The final state is a linear combination \n",
    "           x(t) = h*x1(t) + (1-h)*x2(t).\n",
    "      - Then we do a polynomial readout [x, x^2, 1] -> output.\n",
    "      - We feed teacher forcing in collect_states, \n",
    "        then solve readout with Ridge regression.\n",
    "\n",
    "    References:\n",
    "      - Liu, J., Xu, X., & Li, E. (2024). \n",
    "        \"A minimum complexity interaction echo state network,\" \n",
    "         Neural Computing and Applications.\n",
    "    \n",
    "    notes:\n",
    "      - The reservoir_size is N for each reservoir, \n",
    "        so total param dimension is 2*N for states, \n",
    "        but we produce a single final \"combined\" state x(t) in R^N for readout.\n",
    "      - The activation f1=cos(...) for reservoir1, f2=sin(...) for reservoir2, \n",
    "        as recommended by the paper for MCI-ESN.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        reservoir_size=500,\n",
    "        cycle_weight=0.9,      # 'l' in the paper\n",
    "        connect_weight=0.9,    # 'g' in the paper\n",
    "        input_scale=0.2,\n",
    "        leaking_rate=1.0,\n",
    "        ridge_alpha=1e-6,\n",
    "        combine_factor=0.1,    # 'h' in the paper\n",
    "        seed=47,\n",
    "        v1=0.4, v2=0.4         # fixed values for v1, v2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        reservoir_size: N, size of each cycle reservoir \n",
    "        cycle_weight : l, ring adjacency weight in [0,1), ensures cycle synergy\n",
    "        connect_weight: g, cross-connection weight between the two cycle reservoirs\n",
    "        input_scale   : scale factor for input->reservoir weights\n",
    "        leaking_rate  : ESN update alpha \n",
    "        ridge_alpha   : readout ridge penalty\n",
    "        combine_factor: h in [0,1], to form x(t)= h*x1(t)+(1-h)*x2(t) as final combined state\n",
    "        seed          : random seed\n",
    "        \"\"\"\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.cycle_weight   = cycle_weight\n",
    "        self.connect_weight = connect_weight\n",
    "        self.input_scale    = input_scale\n",
    "        self.leaking_rate   = leaking_rate\n",
    "        self.ridge_alpha    = ridge_alpha\n",
    "        self.combine_factor = combine_factor\n",
    "        self.seed           = seed\n",
    "        self.v1 = v1\n",
    "        self.v2 = v2\n",
    "\n",
    "        # We'll define (and build) adjacency for each cycle, \n",
    "        # plus cross-connection for two sub-reservoirs.\n",
    "        # We'll define 2 input weight mats: Win1, Win2.\n",
    "        # We'll define states x1(t), x2(t).\n",
    "        # We'll define readout W_out after training.\n",
    "\n",
    "        self._build_mci_esn()\n",
    "\n",
    "    def _build_mci_esn(self):\n",
    "        \"\"\"\n",
    "        Build all the internal parameters: \n",
    "         - ring adjacency for each reservoir\n",
    "         - cross-reservoir connection\n",
    "         - input weights for each reservoir\n",
    "         - initial states\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        N = self.reservoir_size\n",
    "\n",
    "        # Build ring adjacency W_res in shape [N, N], with cycle_weight on ring\n",
    "        W_res = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            j = (i+1) % N\n",
    "            W_res[j, i] = self.cycle_weight\n",
    "        self.W_res = W_res  # shared by both sub-reservoirs\n",
    "\n",
    "        # Build cross-connection W_cn for shape [N,N], \n",
    "        # minimal 2 nonzero elements. \n",
    "        # For the simplest approach from the paper:\n",
    "        #   W_cn[0, N-1] = g, W_cn[1, N-2] = g or similar.\n",
    "        # The paper's eq(7) suggests the last 2 elements in x(t) cross to first 2 in the other reservoir:\n",
    "        # We'll do the simplest reference: if i=0 or i=1, we connect from the other reservoir's last or second-last. \n",
    "        # We'll define a function for each sub-res to pick up from the other sub-res. \n",
    "        # We can store them in separate arrays, or define them in code. \n",
    "        # We'll just store \"We want index 0 to see x2[-1], index 1 to see x2[-2].\"\n",
    "\n",
    "        # But as done in the original code snippet from the paper:\n",
    "        #   Wcn has\n",
    "        # effectively 2 nonzero positions. We'll define that pattern:\n",
    "        W_cn = np.zeros((N, N))\n",
    "        # e.g. W_cn[0, N-1] = g, W_cn[N-1, N-2] = g or something. \n",
    "        # The paper example used W_cn = diag(0,g,...) plus the corner. We'll do the simplest:\n",
    "        # let W_cn[0, N-1]=g, W_cn[1, N-2]=g.\n",
    "        # This matches the minimal cross. \n",
    "        # For clarity we do:\n",
    "        W_cn[0, N-1] = self.connect_weight\n",
    "        if N>1:\n",
    "            # W_cn[1, N-2] = self.connect_weight\n",
    "            W_cn[N-1, 0] = self.connect_weight\n",
    "        self.W_cn = W_cn\n",
    "\n",
    "        # We'll define input weights for each sub-reservoir, shape [N, dim_input].\n",
    "        # The paper sets them as eq(10) in the snippet, with different signs. \n",
    "        # We'll define them as parted. \n",
    "        # We define V1, V2 => shape [N, dim_input], with constant magnitude t1, t2, random sign. \n",
    "        # We'll do random. Need to check this in the paper again\n",
    "        # We'll keep \"two\" separate. user can define input_scale but not two separate. \n",
    "        # We'll do the simplest approach: the absolute value is the same => input_scale, \n",
    "        # sign is random. Then we define Win1 = V1 - V2, Win2 = V1 + V2.\n",
    "        # This is consistent with eq(10) from the paper.\n",
    "\n",
    "        self.Win1 = None\n",
    "        self.Win2 = None\n",
    "\n",
    "        # We'll define states x1(t), x2(t). We'll do them after dimension known. \n",
    "        self.x1 = None\n",
    "        self.x2 = None\n",
    "\n",
    "        self.W_out = None\n",
    "\n",
    "    def _init_substates(self):\n",
    "        \"\"\"\n",
    "        Once we know reservoir_size, we define x1, x2 as zeros. \n",
    "        We'll call this in reset_state or at fit time.\n",
    "        \"\"\"\n",
    "        N = self.reservoir_size\n",
    "        self.x1 = np.zeros(N)\n",
    "        self.x2 = np.zeros(N)\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.x1 is not None:\n",
    "            self.x1[:] = 0.0\n",
    "        if self.x2 is not None:\n",
    "            self.x2[:] = 0.0\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        Single-step reservoir update.\n",
    "        x1(t+1) = cos( Win1*u(t+1) + W_res*x1(t) + W_cn*x2(t) )\n",
    "        x2(t+1) = sin( Win2*u(t+1) + W_res*x2(t) + W_cn*x1(t) )\n",
    "        Then x(t)= h*x1(t+1) + (1-h)* x2(t+1).\n",
    "        We'll define the leaky integration. \n",
    "        But the paper uses an approach with no leak? Be careful.\n",
    "        We'll do the approach: x1(t+1)= (1-alpha)* x1(t) + alpha*cos(...).\n",
    "        \"\"\"\n",
    "        alpha = self.leaking_rate\n",
    "\n",
    "        # pre activation for reservoir1\n",
    "        pre1 = self.Win1 @ u + self.W_res @ self.x1 + self.W_cn @ self.x2\n",
    "        # reservoir1 uses cos\n",
    "        new_x1 = np.cos(pre1)\n",
    "\n",
    "        # reservoir2 uses sin\n",
    "        pre2 = self.Win2 @ u + self.W_res @ self.x2 + self.W_cn @ self.x1\n",
    "        new_x2 = np.sin(pre2)\n",
    "\n",
    "        self.x1 = (1.0 - alpha)*self.x1 + alpha*new_x1\n",
    "        self.x2 = (1.0 - alpha)*self.x2 + alpha*new_x2\n",
    "\n",
    "    def _combine_state(self):\n",
    "        \"\"\"\n",
    "        Combine x1(t), x2(t) => x(t) = h*x1 + (1-h)*x2\n",
    "        \"\"\"\n",
    "        h = self.combine_factor\n",
    "        return h*self.x1 + (1.0 - h)*self.x2\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        # We reset the reservoir to zero\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for t in range(len(inputs)):\n",
    "            self._update(inputs[t])   # feed the REAL input from the dataset\n",
    "            combined = self._combine_state()\n",
    "            states.append(combined.copy())\n",
    "        states = np.array(states)  # shape => [T, N]\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Build input weights if needed, gather states on the training data (teacher forcing),\n",
    "        then solve a polynomial readout [x, x^2, 1]->train_target(t).\n",
    "\n",
    "        train_input : shape [T, d_in]\n",
    "        train_target: shape [T, d_out]\n",
    "        discard     : # of states to discard for warmup\n",
    "        \"\"\"\n",
    "        T = len(train_input)\n",
    "        if T<2:\n",
    "            raise ValueError(\"Not enough training data\")\n",
    "\n",
    "        d_in = train_input.shape[1]\n",
    "        # d_out = train_target.shape[1]\n",
    "\n",
    "        # built Win1, Win2\n",
    "        if self.Win1 is None or self.Win2 is None:\n",
    "            np.random.seed(self.seed+100)\n",
    "            # build V1, V2 in shape [N, d_in]\n",
    "            N = self.reservoir_size\n",
    "            # V1 = (np.random.rand(N, d_in)-0.5)*2.0*self.input_scale\n",
    "            # V2 = (np.random.rand(N, d_in)-0.5)*2.0*self.input_scale\n",
    "\n",
    "            sign_V1 = np.random.choice([-1, 1], size=(N, d_in))\n",
    "            sign_V2 = np.random.choice([-1, 1], size=(N, d_in))\n",
    "\n",
    "            v1, v2 = self.v1, self.v2  # fixed values for V1, V2\n",
    "\n",
    "            V1 = v1 * sign_V1 * self.input_scale\n",
    "            V2 = v2 * sign_V2 * self.input_scale\n",
    "\n",
    "            # eq(10): Win1= V1 - V2, Win2= V1 + V2\n",
    "            self.Win1 = V1 - V2\n",
    "            self.Win2 = V1 + V2\n",
    "\n",
    "        # define x1, x2\n",
    "        self._init_substates()\n",
    "\n",
    "        # gather states\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]  # shape => [T-discard, d_out]\n",
    "\n",
    "        # polynomial readout\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append(augment_state_with_squares(s))\n",
    "        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n",
    "\n",
    "        # Solve ridge\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        # W_out => shape [d_out, 2N+1]\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        Fully autoregressive: \n",
    "          We do not use teacher forcing, \n",
    "          we feed the model's last output as the next input \n",
    "        Typically, for MCI-ESN the paper does input(t+1) in R^d. \n",
    "        We do the test_input\n",
    "        For multi-step chaotic forecast, we feed the model's output as input? \n",
    "        That means the system dimension d_in must match d_out. \n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        # re-init states\n",
    "        #self._init_substates()\n",
    "\n",
    "        # we assume initial_input => shape (d_in,)\n",
    "        current_in = np.array(initial_input)\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            # read out\n",
    "            combined = self._combine_state()\n",
    "            big_x = augment_state_with_squares(combined)\n",
    "            out = self.W_out @ big_x  # shape => (d_out,)\n",
    "\n",
    "            preds.append(out)\n",
    "            current_in = out  # feed output back as next input\n",
    "\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48747df",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"cycle_weight\": [0.4,0.6,0.8,1],\n",
    "    \"connect_weight\":[0.6,0.8,1],\n",
    "    \"input_scale\": [0.1,0.3,0.5,0.7],\n",
    "    \"leaking_rate\": [0.3,0.5,0.7,0.9],\n",
    "    \"combine_factor\":[0.2,0.4,0.6,0.8],\n",
    "    \"ridge_alpha\": [1e-7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    # Precompute param combinations\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    # tqdm adds a progress bar for better visualization\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores = []\n",
    "        \n",
    "        # Run all 20 seeds\n",
    "        for initial_state in [[1.0,1.0,1.0],[1.0,2.0,3.0],[2.0,1.5,4.0]]:\n",
    "            tmax = 250\n",
    "            dt   = 0.02\n",
    "            t_vals, rossler_traj = generate_rossler_data(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "            \n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            rossler_traj = rossler_traj[washout:]\n",
    "            \n",
    "            # normalize\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(rossler_traj)\n",
    "            rossler_traj = scaler.transform(rossler_traj)\n",
    "            \n",
    "            T_data = len(rossler_traj)\n",
    "            for train_frac in [0.3,0.35,0.4]:\n",
    "                train_end = int(train_frac*(T_data-1))\n",
    "                train_input  = rossler_traj[:train_end]\n",
    "                train_target = rossler_traj[1:train_end+1]\n",
    "                test_input   = rossler_traj[train_end:-1]\n",
    "                test_target  = rossler_traj[train_end+1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "                for seed in np.arange(1,6):\n",
    "                    model = model_class(**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "                    T_VPT_s, _, _ = compute_valid_prediction_time(test_target,preds,t_vals,0.4,0.9,dt)\n",
    "                    seed_scores.append(T_VPT_s)\n",
    "        mean_score = float(np.mean(seed_scores))\n",
    "        std_dev    = float(np.std(seed_scores))\n",
    "        # is_stable  = std_dev < 1.5\n",
    "        # status     = \"Stable\" if is_stable else \"Unstable\"\n",
    "        \n",
    "        # print(f\"Params: {params} → Avg T_VPT={mean_score:.3f}, \"\n",
    "        #       f\"Std Dev={std_dev:.3f} → {status}\")\n",
    "\n",
    "        results.append({\n",
    "            \"params\":      params,\n",
    "            \"seed_scores\": seed_scores,\n",
    "            \"mean_T_VPT\":  mean_score,\n",
    "            \"std_dev\":     std_dev,\n",
    "            # \"stable\":      is_stable\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mci_results = run_grid_search(MCI3D, grid, \"MCI\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
