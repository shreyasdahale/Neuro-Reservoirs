{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdede0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.linear_model import Ridge\n",
    "from matplotlib.colors import Normalize\n",
    "import networkx as nx\n",
    "from scipy.signal import welch\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52baae13",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ecb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_derivatives(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    \"\"\"Compute time derivatives [dx/dt, dy/dt, dz/dt] for the Lorenz system.\"\"\"\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x * (rho - z) - y\n",
    "    dzdt = x * y - beta * z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Numerically integrate Lorenz equations x'(t), y'(t), z'(t) using odeint.\n",
    "    Returns:\n",
    "       t_vals: array of time points\n",
    "       sol   : array shape [num_steps, 3] of [x(t), y(t), z(t)]\n",
    "    \"\"\"\n",
    "    num_steps = int(tmax / dt) + 1\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(lorenz_derivatives, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b88e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 12501, from t=0..250.0 with dt=0.02.\n",
      "Train size: 8750  Test size: 3750\n"
     ]
    }
   ],
   "source": [
    "chosen_system = \"Lorenz\"  # Options: \"lorenz\", \"rossler\", \"chen\", \"chua\"\n",
    "tmax = 250.0\n",
    "dt = 0.02\n",
    "t_vals, lorenz_traj = generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=tmax,\n",
    "    dt=dt,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    ")\n",
    "\n",
    "# Discard first 2,000 points as washout\n",
    "washout = 2000\n",
    "t_vals = t_vals[washout:]\n",
    "lorenz_trajectory = lorenz_traj[washout:]\n",
    "\n",
    "T_data = len(lorenz_traj)\n",
    "print(f\"Data length: {T_data}, from t=0..{tmax} with dt={dt}.\")\n",
    "\n",
    "train_frac = 0.7\n",
    "train_end = int(train_frac*(T_data-1))\n",
    "# train_end = 4500\n",
    "train_input  = lorenz_traj[:train_end]\n",
    "train_target = lorenz_traj[1:train_end+1]\n",
    "test_input   = lorenz_traj[train_end:-1]\n",
    "test_target  = lorenz_traj[train_end+1:]\n",
    "\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps)*dt\n",
    "print(f\"Train size: {len(train_input)}  Test size: {len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b9b6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline3D:\n",
    "    \"\"\"\n",
    "    Lightweight single-layer LSTM for 3-dim Lorenz forecasting.\n",
    "    * hidden_size=32 → ~4.8k trainable parameters\n",
    "    * fit() trains in teacher-forcing mode\n",
    "    * predict() produces autoregressive roll-out\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim:  int = 3,\n",
    "                 hidden_size: int = 32,\n",
    "                 output_dim: int = 3,\n",
    "                 lr: float = 1e-3,\n",
    "                 epochs: int = 30,\n",
    "                 device: str = 'cpu',\n",
    "                 seed: int = 0):\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "        self.device  = torch.device(device)\n",
    "        self.epochs  = epochs\n",
    "        self.model   = nn.LSTM(input_dim, hidden_size,\n",
    "                               batch_first=True).to(self.device)\n",
    "        self.head    = nn.Linear(hidden_size, output_dim).to(self.device)\n",
    "        self.crit    = nn.MSELoss()\n",
    "        self.optim   = Adam(list(self.model.parameters())+\n",
    "                            list(self.head.parameters()), lr=lr)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def _init_hidden(self, batch_sz=1):\n",
    "        h0 = torch.zeros(1, batch_sz,\n",
    "                         self.model.hidden_size,\n",
    "                         device=self.device)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        return (h0, c0)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, x_np: np.ndarray, y_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        x_np shape [T, 3]  (input  at t)\n",
    "        y_np shape [T, 3]  (target at t)\n",
    "        \"\"\"\n",
    "        x = torch.tensor(x_np, dtype=torch.float32,\n",
    "                         device=self.device).unsqueeze(0)  # [1,T,3]\n",
    "        y = torch.tensor(y_np, dtype=torch.float32,\n",
    "                         device=self.device).unsqueeze(0)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            self.optim.zero_grad()\n",
    "            out, _ = self.model(x, self._init_hidden())\n",
    "            pred   = self.head(out)\n",
    "            loss   = self.crit(pred, y)\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict(self, init_u: np.ndarray, n_steps: int):\n",
    "        \"\"\"\n",
    "        Autoregressive roll-out.\n",
    "        init_u : initial 3-vector (last known sample)\n",
    "        Returns array of shape [n_steps, 3].\n",
    "        \"\"\"\n",
    "        self.model.eval(); self.head.eval()\n",
    "\n",
    "        inp     = torch.tensor(init_u[None, None, :],\n",
    "                               dtype=torch.float32, device=self.device)\n",
    "        h, c    = self._init_hidden()\n",
    "        preds   = np.empty((n_steps, 3), dtype=np.float32)\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            out, (h, c) = self.model(inp, (h, c))\n",
    "            y           = self.head(out)\n",
    "            preds[t]    = y.squeeze(0).cpu().numpy()\n",
    "            inp         = y.detach()    # feed prediction back\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_open_loop(self, x_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        Open-loop prediction using teacher-forced inputs (like during training).\n",
    "        x_np shape: [T, 3] – input sequence\n",
    "        Returns:\n",
    "            preds: [T, 3] – predicted output sequence\n",
    "        \"\"\"\n",
    "        self.model.eval(); self.head.eval()\n",
    "\n",
    "        x = torch.tensor(x_np, dtype=torch.float32,\n",
    "                         device=self.device).unsqueeze(0)  # [1, T, 3]\n",
    "        out, _ = self.model(x, self._init_hidden())\n",
    "        preds = self.head(out).squeeze(0).cpu().numpy()  # [T, 3]\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "lstm_baseline = LSTMBaseline3D(\n",
    "                    hidden_size=38,         # parameter budget ~ 4800\n",
    "                    lr=1e-3,\n",
    "                    epochs=100,\n",
    "                    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                    seed=45)\n",
    "lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "# one-step roll-out to build an initial vector for auto-regressive mode\n",
    "init_vec = train_target[-1]                # last teacher-forced target\n",
    "lstm_preds = lstm_baseline.predict(init_vec,\n",
    "                                   n_steps=len(test_input))\n",
    "lstm_preds_open_loop = lstm_baseline.predict_open_loop(test_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878d0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons for Teacher-forced Single-step Forecasting\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets)**2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * variance))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "705faf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse = evaluate_nrmse(lstm_preds_open_loop, y_test, horizons=[200, 400, 600, 800, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ec2f419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{200: array([2.99048437, 2.59969958, 2.61917507]),\n",
       " 400: array([2.9221503 , 2.52039873, 2.36095988]),\n",
       " 600: array([3.07108201, 2.71649147, 2.7338271 ]),\n",
       " 800: array([3.09763912, 2.71336986, 2.70645885]),\n",
       " 1000: array([3.08192524, 2.69861583, 2.72102465])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00486cc",
   "metadata": {},
   "source": [
    "### MIT-BIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "826976d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 25000, train size: 15000, test size: 9999\n"
     ]
    }
   ],
   "source": [
    "# MIT-BIH Dataset\n",
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb\n",
    "\n",
    "import wfdb\n",
    "\n",
    "# Download and load record and annotations for patient #100\n",
    "record = wfdb.rdrecord('100', sampfrom=0, sampto=25002, pn_dir='mitdb')  # first 20,000 samples\n",
    "annotation = wfdb.rdann('100', 'atr', sampfrom=0, sampto=25002, pn_dir='mitdb')\n",
    "# Get input signal u(t) from the first channel\n",
    "u = record.p_signal[:, 0] \n",
    "u\n",
    "# Normalize input\n",
    "u_min = np.min(u)\n",
    "u_max = np.max(u)\n",
    "u_norm = (u - u_min) / (u_max - u_min)\n",
    "fs = record.fs  # sampling frequency (should be 360 Hz)\n",
    "t_vals = np.arange(len(u_norm)) / fs\n",
    "emb_dim = 3\n",
    "# inputs = u_norm\n",
    "inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "\n",
    "# Create target array (heartbeat locations)\n",
    "targets = np.zeros(len(u_norm))\n",
    "targets[annotation.sample] = 1  # mark annotations as 1 (heartbeat)\n",
    "targets = create_delay_embedding(targets, emb_dim)\n",
    "data_size = len(inputs)\n",
    "train_size = 15000\n",
    "train_input = inputs[:train_size]\n",
    "train_target = targets[:train_size]\n",
    "test_input = inputs[train_size+1:]\n",
    "test_target = targets[train_size+1:]\n",
    "test_size = len(test_input)\n",
    "print(f\"Total samples: {data_size}, train size: {train_size}, test size: {test_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d135c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_baseline = LSTMBaseline3D(\n",
    "                    hidden_size=32,         # parameter budget ~ 4800\n",
    "                    lr=1e-3,\n",
    "                    epochs=400,\n",
    "                    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                    seed=42)\n",
    "lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "# one-step roll-out to build an initial vector for auto-regressive mode\n",
    "init_vec = train_target[-1]                # last teacher-forced target\n",
    "lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "nrmse = evaluate_nrmse(lstm_preds, test_target, horizons=[200, 400, 600, 800, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea2bbb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{200: array([1.26855248, 1.26855248, 1.26855248]),\n",
       " 400: array([1.19724515, 1.19724515, 1.19724515]),\n",
       " 600: array([1.199403, 1.199403, 1.199403]),\n",
       " 800: array([1.18188347, 1.18188347, 1.18188347]),\n",
       " 1000: array([1.22432953, 1.22432953, 1.22432953])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1baaadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    lstm_baseline = LSTMBaseline3D(\n",
    "                        hidden_size=32,         # parameter budget ~ 4800\n",
    "                        lr=1e-3,\n",
    "                        epochs=400,\n",
    "                        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        seed=seed)\n",
    "    lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "    # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "    init_vec = train_target[-1]                # last teacher-forced target\n",
    "    lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "    nrmse = evaluate_nrmse(lstm_preds, test_target, horizons)\n",
    "    nrmse_dict['LSTM'].append(nrmse)\n",
    "    # for horizon, value in nrmse.items():\n",
    "    #     nrmse_dict[horizon].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cefd294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LSTM             \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        1.403689251708608 ± 0.11595694894078283\n",
      "600        1.3572118807998188 ± 0.11991708060885164\n",
      "1000       1.3587061311672828 ± 0.11065892082379176\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'LSTM':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['LSTM']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b5ba0",
   "metadata": {},
   "source": [
    "### Sunspot (Monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7161afa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3315\n",
      "Train size: 2000\n",
      "Test size: 1312\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'RealWorld/datasets/SN_m_tot_V2.0.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, sep=';', header = None)\n",
    "df\n",
    "data = df.iloc[:, 3].values\n",
    "dt = 1\n",
    "dataset_size = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 2000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}\\nTest size: {len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "931fe915",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_baseline = LSTMBaseline3D(\n",
    "                    hidden_size=32,         # parameter budget ~ 4800\n",
    "                    lr=1e-3,\n",
    "                    epochs=400,\n",
    "                    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                    seed=42)\n",
    "lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "# one-step roll-out to build an initial vector for auto-regressive mode\n",
    "init_vec = train_target[-1]                # last teacher-forced target\n",
    "lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "nrmse = evaluate_nrmse(lstm_preds, test_target, horizons=[200, 400, 600, 800, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bb23aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{200: array([2.77286289, 2.76831341, 2.75809817]),\n",
       " 400: array([2.64045184, 2.61904885, 2.60626304]),\n",
       " 600: array([2.5336566 , 2.53202015, 2.5302861 ]),\n",
       " 800: array([2.62882725, 2.62769681, 2.62751743]),\n",
       " 1000: array([2.5902262, 2.5902443, 2.5904081])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b5dc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    lstm_baseline = LSTMBaseline3D(\n",
    "                        hidden_size=32,         # parameter budget ~ 4800\n",
    "                        lr=1e-3,\n",
    "                        epochs=400,\n",
    "                        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        seed=seed)\n",
    "    lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "    # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "    init_vec = train_target[-1]                # last teacher-forced target\n",
    "    lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "    nrmse = evaluate_nrmse(lstm_preds, test_target, horizons)\n",
    "    nrmse_dict['LSTM'].append(nrmse)\n",
    "    # for horizon, value in nrmse.items():\n",
    "    #     nrmse_dict[horizon].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db726d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LSTM             \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        2.675008492187419 ± 0.012923192834715486\n",
      "600        2.537752244792846 ± 0.009405482742044077\n",
      "1000       2.5961442773004455 ± 0.009656027200575885\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'LSTM':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['LSTM']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad54a6",
   "metadata": {},
   "source": [
    "### Santa Fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b15ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 17000.\n",
      "Train size: 7000  \n",
      "Test size: 9997\n"
     ]
    }
   ],
   "source": [
    "file_path = 'RealWorld/datasets/santa-fe-time-series-competition-data-set-b-1.0.0/b1.txt'\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, sep=' ')\n",
    "df\n",
    "# Normalize the first column (column 0) of the DataFrame\n",
    "df[0] = (df[0] - df[0].min()) / (df[0].max() - df[0].min())\n",
    "data = df.iloc[:, 0].values\n",
    "chosen_system = \"SantaFe\"\n",
    "dt = 1\n",
    "T_data = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Data length: {T_data}.\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 7000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}  \\nTest size: {len(test_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e2434c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:\n",
    "    lstm_baseline = LSTMBaseline3D(\n",
    "                        hidden_size=32,         # parameter budget ~ 4800\n",
    "                        lr=1e-3,\n",
    "                        epochs=400,\n",
    "                        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        seed=seed)\n",
    "    lstm_baseline.fit(train_input, train_target)\n",
    "\n",
    "    # one-step roll-out to build an initial vector for auto-regressive mode\n",
    "    init_vec = train_target[-1]                # last teacher-forced target\n",
    "    lstm_preds = lstm_baseline.predict_open_loop(test_input)\n",
    "\n",
    "    nrmse = evaluate_nrmse(lstm_preds, test_target, horizons)\n",
    "    nrmse_dict['LSTM'].append(nrmse)\n",
    "    # for horizon, value in nrmse.items():\n",
    "    #     nrmse_dict[horizon].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eed92cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "LSTM             \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        1.0224924845401087 ± 0.14157896135439874\n",
      "600        0.7022846945297287 ± 0.08675457054383437\n",
      "1000       0.7800291094963395 ± 0.09463118835634546\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'LSTM':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['LSTM']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643beaa",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27eb2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBaseline3D(nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer causal TCN       (kernel=3, dilation=1 & 2, padding chosen\n",
    "    so receptive field = 5 time-steps).\n",
    "    ----------------------\n",
    "    • input_dim  = 3\n",
    "    • hidden_dim = 32  → total ≈ 4.9 k parameters\n",
    "    • output_dim = 3    (one-step prediction)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim:  int = 3,\n",
    "                 hidden_dim: int = 32,\n",
    "                 output_dim: int = 3,\n",
    "                 lr: float = 1e-3,\n",
    "                 epochs: int = 40,\n",
    "                 device: str = \"cpu\",\n",
    "                 seed: int = 0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "        k = 3  # kernel\n",
    "        # layer 1: dilation 1  → pad 2 to keep length\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim,\n",
    "                               kernel_size=k,\n",
    "                               dilation=1,\n",
    "                               padding=2,\n",
    "                               bias=True)\n",
    "        # layer 2: dilation 2  → pad 4\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim,\n",
    "                               kernel_size=k,\n",
    "                               dilation=2,\n",
    "                               padding=4,\n",
    "                               bias=True)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.head  = nn.Conv1d(hidden_dim, output_dim,\n",
    "                               kernel_size=1, bias=True)\n",
    "\n",
    "        self.lr, self.epochs = lr, epochs\n",
    "        self.to(device)\n",
    "        self.optim = Adam(self.parameters(), lr=lr)\n",
    "        self.crit  = nn.MSELoss()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape  [B, T, 3]  (batch, time, channels)\n",
    "        return  [B, T, 3]\n",
    "        \"\"\"\n",
    "        # reshape to Conv1d convention: (B, C, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        y = self.conv1(x); y = self.relu(y[:, :, :-2])     # remove look-ahead pad\n",
    "        y = self.conv2(y); y = self.relu(y[:, :, :-4])     # remove look-ahead pad\n",
    "        out = self.head(y).permute(0, 2, 1)                # back to (B,T,C)\n",
    "        return out\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def fit(self, x_np: np.ndarray, y_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        Teacher-forcing on entire sequence (batch size = 1).\n",
    "        x_np, y_np shape [T, 3]\n",
    "        \"\"\"\n",
    "        x = torch.tensor(x_np[None], dtype=torch.float32, device=next(self.parameters()).device)\n",
    "        y = torch.tensor(y_np[None], dtype=torch.float32, device=next(self.parameters()).device)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            self.optim.zero_grad()\n",
    "            pred = self.forward(x)\n",
    "            loss = self.crit(pred[:, :-1], y[:, 1:])  # predict next step\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    @torch.no_grad()\n",
    "    def predict(self, init_window: np.ndarray, n_steps: int):\n",
    "        \"\"\"\n",
    "        Autoregressive roll-out.\n",
    "        init_window : length L≥5, shape [L,3] (latest samples, earliest first)\n",
    "        Returns      : [n_steps,3]\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        window = init_window.copy()\n",
    "        preds  = np.empty((n_steps, 3), dtype=np.float32)\n",
    "\n",
    "        for t in range(n_steps):\n",
    "            inp = torch.tensor(window[None], dtype=torch.float32, device=device)\n",
    "            y   = self.forward(inp)[0, -1].cpu().numpy()\n",
    "            preds[t] = y\n",
    "            window   = np.vstack([window[1:], y])  # slide window\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_open_loop(self, x_np: np.ndarray):\n",
    "        \"\"\"\n",
    "        Open-loop prediction (teacher-forced inputs).\n",
    "        x_np shape: [T, 3]\n",
    "        Returns:\n",
    "            preds: [T - 1, 3] – one-step-ahead predictions\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        x = torch.tensor(x_np[None], dtype=torch.float32, device=device)  # [1, T, 3]\n",
    "        preds = self.forward(x)  # [1, T, 3]\n",
    "        preds = preds[:, :-1]    # predict from t=0 to t=T-2 for target t=1 to t=T-1\n",
    "\n",
    "        return preds.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "tcn = TCNBaseline3D(hidden_dim=32, epochs=50, lr=1e-3, device=\"cpu\", seed=46)\n",
    "tcn.fit(train_input, train_target)\n",
    "\n",
    "# initial window must be >4 samples:\n",
    "init_win = test_input[:5].copy()\n",
    "tcn_preds = tcn.predict(init_win, n_steps=len(test_target))\n",
    "tcn_preds_open_loop = tcn.predict_open_loop(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d932f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse = evaluate_nrmse(tcn_preds, y_test, horizons=[200, 400, 600, 800, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34050567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{200: array([3.3622266 , 2.92286399, 2.94476045]),\n",
       " 400: array([3.35475998, 2.8935311 , 2.71048812]),\n",
       " 600: array([3.59476628, 3.17971057, 3.2000023 ]),\n",
       " 800: array([3.6408971 , 3.18923543, 3.18111239]),\n",
       " 1000: array([3.62186296, 3.17139968, 3.19773441])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddeff9",
   "metadata": {},
   "source": [
    "### MIT-BIH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9be1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 25000, train size: 15000, test size: 9999\n"
     ]
    }
   ],
   "source": [
    "# MIT-BIH Dataset\n",
    "def create_delay_embedding(signal, embed_dim):\n",
    "    L = len(signal) - embed_dim + 1\n",
    "    emb = np.zeros((L, embed_dim))\n",
    "    for i in range(L):\n",
    "        emb[i, :] = signal[i:i+embed_dim]\n",
    "    return emb\n",
    "\n",
    "import wfdb\n",
    "\n",
    "# Download and load record and annotations for patient #100\n",
    "record = wfdb.rdrecord('100', sampfrom=0, sampto=25002, pn_dir='mitdb')  # first 20,000 samples\n",
    "annotation = wfdb.rdann('100', 'atr', sampfrom=0, sampto=25002, pn_dir='mitdb')\n",
    "# Get input signal u(t) from the first channel\n",
    "u = record.p_signal[:, 0] \n",
    "u\n",
    "# Normalize input\n",
    "u_min = np.min(u)\n",
    "u_max = np.max(u)\n",
    "u_norm = (u - u_min) / (u_max - u_min)\n",
    "fs = record.fs  # sampling frequency (should be 360 Hz)\n",
    "t_vals = np.arange(len(u_norm)) / fs\n",
    "emb_dim = 3\n",
    "# inputs = u_norm\n",
    "inputs = create_delay_embedding(u_norm, emb_dim)\n",
    "\n",
    "# Create target array (heartbeat locations)\n",
    "targets = np.zeros(len(u_norm))\n",
    "targets[annotation.sample] = 1  # mark annotations as 1 (heartbeat)\n",
    "targets = create_delay_embedding(targets, emb_dim)\n",
    "data_size = len(inputs)\n",
    "train_size = 15000\n",
    "train_input = inputs[:train_size]\n",
    "train_target = targets[:train_size]\n",
    "test_input = inputs[train_size+1:]\n",
    "test_target = targets[train_size+1:]\n",
    "test_size = len(test_input)\n",
    "print(f\"Total samples: {data_size}, train size: {train_size}, test size: {test_size}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a0cac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:  \n",
    "    tcn = TCNBaseline3D(hidden_dim=32, epochs=50, lr=1e-3, device=\"cpu\", seed=seed)\n",
    "    tcn.fit(train_input, train_target)\n",
    "\n",
    "    # initial window must be >4 samples:\n",
    "    # init_win = test_input[:5].copy()    \n",
    "    tcn_preds = tcn.predict_open_loop(test_input)\n",
    "    nrmse = evaluate_nrmse(tcn_preds, test_target, horizons)\n",
    "    nrmse_dict['TCN'].append(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d36b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TCN              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        1.620596824802384 ± 0.04332837192484439\n",
      "600        1.6186667098390075 ± 0.04171223835818487\n",
      "1000       1.6169760364489023 ± 0.04015689973379543\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'TCN':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['TCN']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "783ddd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3315\n",
      "Train size: 2000\n",
      "Test size: 1312\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = 'RealWorld/datasets/SN_m_tot_V2.0.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, sep=';', header = None)\n",
    "df\n",
    "data = df.iloc[:, 3].values\n",
    "dt = 1\n",
    "dataset_size = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Dataset size: {dataset_size}\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 2000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}\\nTest size: {len(test_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a46acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:  \n",
    "    tcn = TCNBaseline3D(hidden_dim=32, epochs=40, lr=1e-3, device=\"cpu\", seed=seed)\n",
    "    tcn.fit(train_input, train_target)\n",
    "\n",
    "    # initial window must be >4 samples:\n",
    "    # init_win = test_input[:5].copy()    \n",
    "    tcn_preds = tcn.predict_open_loop(test_input)\n",
    "    nrmse = evaluate_nrmse(tcn_preds, test_target, horizons)\n",
    "    nrmse_dict['TCN'].append(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db20e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TCN              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        0.920251233826814 ± 0.051167505977109125\n",
      "600        0.7670735717539632 ± 0.05024783059948199\n",
      "1000       0.7532650172790911 ± 0.0496774645230772\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'TCN':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['TCN']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f11a803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 17000.\n",
      "Train size: 7000  \n",
      "Test size: 9997\n"
     ]
    }
   ],
   "source": [
    "file_path = 'RealWorld/datasets/santa-fe-time-series-competition-data-set-b-1.0.0/b1.txt'\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, sep=' ')\n",
    "df\n",
    "# Normalize the first column (column 0) of the DataFrame\n",
    "df[0] = (df[0] - df[0].min()) / (df[0].max() - df[0].min())\n",
    "data = df.iloc[:, 0].values\n",
    "chosen_system = \"SantaFe\"\n",
    "dt = 1\n",
    "T_data = len(data)\n",
    "data = create_delay_embedding(data, 3)\n",
    "print(f\"Data length: {T_data}.\")\n",
    "\n",
    "# Train/Test Split\n",
    "train_end = 7000\n",
    "train_input  = data[:train_end]\n",
    "train_target = data[1:train_end+1]\n",
    "test_input   = data[train_end:-1]\n",
    "test_target  = data[train_end+1:]\n",
    "y_test = test_target\n",
    "n_test_steps = len(test_target)\n",
    "time_test = np.arange(n_test_steps) * dt\n",
    "\n",
    "print(f\"Train size: {len(train_input)}  \\nTest size: {len(test_input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "827b543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [300, 600, 1000]\n",
    "\n",
    "nrmse_dict = defaultdict(list)\n",
    "seeds = range(995, 1025)\n",
    "\n",
    "for seed in seeds:  \n",
    "    tcn = TCNBaseline3D(hidden_dim=32, epochs=100, lr=1e-3, device=\"cpu\", seed=seed)\n",
    "    tcn.fit(train_input, train_target)\n",
    "\n",
    "    # initial window must be >4 samples:\n",
    "    # init_win = test_input[:5].copy()    \n",
    "    tcn_preds = tcn.predict_open_loop(test_input)\n",
    "    nrmse = evaluate_nrmse(tcn_preds, test_target, horizons)\n",
    "    nrmse_dict['TCN'].append(nrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e81c93c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NRMSE for Different Prediction Horizons:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "TCN              \n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "300        1.2064525351486612 ± 0.18056236093445113\n",
      "600        0.8929744613975146 ± 0.12518798413065568\n",
      "1000       0.9639359904495735 ± 0.13378513242802026\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNRMSE for Different Prediction Horizons:\")\n",
    "print(\"-\" * 140)\n",
    "print(f\"{'TCN':<17}\")\n",
    "print(\"-\" * 140)\n",
    "\n",
    "for horizon in horizons:\n",
    "    hfr_vals = [np.mean(hfr_nrmse[horizon]) for hfr_nrmse in nrmse_dict['TCN']]\n",
    "\n",
    "    print(f\"{horizon:<10}\", end=\" \")\n",
    "    for vals in [hfr_vals]:\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        print(f\"{mean} ± {std}\".ljust(18), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f4662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
