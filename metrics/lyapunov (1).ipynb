{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:46:40.219898Z",
     "iopub.status.busy": "2025-06-04T15:46:40.219547Z",
     "iopub.status.idle": "2025-06-04T15:46:42.536578Z",
     "shell.execute_reply": "2025-06-04T15:46:42.535130Z",
     "shell.execute_reply.started": "2025-06-04T15:46:40.219855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.signal import welch, periodogram\n",
    "from scipy.spatial.distance import cdist\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import lyapynov\n",
    "import neurokit2 as nk\n",
    "from scipy.signal import welch, periodogram\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T15:46:42.538311Z",
     "iopub.status.busy": "2025-06-04T15:46:42.537750Z",
     "iopub.status.idle": "2025-06-04T15:46:42.545736Z",
     "shell.execute_reply": "2025-06-04T15:46:42.544632Z",
     "shell.execute_reply.started": "2025-06-04T15:46:42.538278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x*(rho - z) - y\n",
    "    dzdt = x*y - beta*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    num_steps = int(tmax / dt) + 1 # +1 to include t=0\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rossler_derivatives(state, t, a=0.2, b=0.2, c=5.7):\n",
    "    \"\"\"Compute time derivatives [dx/dt, dy/dt, dz/dt] for the Rössler system.\"\"\"\n",
    "    x, y, z = state\n",
    "    dxdt = -y - z\n",
    "    dydt = x + a * y\n",
    "    dzdt = b + z * (x - c)\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_rossler_data(\n",
    "    initial_state=[1.0, 0.0, 0.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    a=0.2,\n",
    "    b=0.2,\n",
    "    c=5.7\n",
    "):\n",
    "    \"\"\"\n",
    "    Numerically integrate Rössler equations x'(t), y'(t), z'(t) using odeint.\n",
    "    Returns:\n",
    "       t_vals: array of time points\n",
    "       sol   : array shape [num_steps, 3] of [x(t), y(t), z(t)]\n",
    "    \"\"\"\n",
    "    num_steps = int(tmax / dt)\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(rossler_derivatives, initial_state, t_vals, args=(a, b, c))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "# Euclidean distance between two vectors\n",
    "def distance(xe: np.array, xi: np.array) -> float:\n",
    "    return np.sqrt(np.sum((xi - xe) ** 2))\n",
    "\n",
    "# Find nearest neighbor for one point\n",
    "def get_nearest_neighbour(xi: np.array, X: np.ndarray, mu: float, time_steps: int) -> int:\n",
    "    xes = np.arange(len(X) - time_steps)\n",
    "    ds = np.array([distance(X[xe], xi) for xe in xes])\n",
    "    ds = np.where(ds == 0, np.inf, ds)\n",
    "    index = np.where((X == xi).all(axis=1))[0][0]\n",
    "    xes_aux = np.abs(xes - index)\n",
    "    ds = np.where(xes_aux < mu, np.inf, ds)\n",
    "    return np.argmin(ds)\n",
    "\n",
    "# Get nearest neighbors for all points\n",
    "def get_nearest_neighbours(X: np.ndarray, mu: float, time_steps: int) -> list:\n",
    "    return [get_nearest_neighbour(xi, X, mu, time_steps) for xi in X]\n",
    "\n",
    "# Mean period estimation using Welch\n",
    "def mp_welch(ts: np.array) -> float:\n",
    "    f, Pxx = welch(ts)\n",
    "    w = Pxx / np.sum(Pxx)\n",
    "    mean_frequency = np.average(f, weights=w)\n",
    "    return 1 / mean_frequency\n",
    "\n",
    "# Mean period estimation using Periodogram\n",
    "def mp_periodogram(ts: np.array) -> float:\n",
    "    f, Pxx = periodogram(ts)\n",
    "    w = Pxx / np.sum(Pxx)\n",
    "    mean_frequency = np.average(f, weights=w)\n",
    "    return 1 / mean_frequency\n",
    "\n",
    "# Log distance divergence for each time step\n",
    "def expected_log_distance(i: int, X: np.ndarray, j: list) -> float:\n",
    "    d_ji = np.array([distance(X[j[k] + i], X[k + i]) for k in range(len(X) - i)])\n",
    "    return np.mean(np.log(d_ji))\n",
    "\n",
    "#  MAIN callable function\n",
    "def compute_lyapunov_exponent(\n",
    "    ts: np.ndarray,\n",
    "    lag: int = 11,\n",
    "    emb_dim: int = 9,\n",
    "    t_0: int = 80,\n",
    "    t_f: int = 150,\n",
    "    delta_t: float = 0.02,\n",
    "    method: str = \"welch\"\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Estimate the largest Lyapunov exponent using Rosenstein's algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts : np.ndarray\n",
    "        1D time-series data.\n",
    "    lag : int\n",
    "        Time delay (τ).\n",
    "    emb_dim : int\n",
    "        Embedding dimension (m).\n",
    "    t_0 : int\n",
    "        Start time index for fitting.\n",
    "    t_f : int\n",
    "        End time index for fitting.\n",
    "    delta_t : float\n",
    "        Time interval between samples.\n",
    "    method : str\n",
    "        'welch' or 'periodogram' for estimating mean period.\n",
    "    show : bool\n",
    "        If True, plot the divergence graph.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lle : float\n",
    "        Largest Lyapunov Exponent.\n",
    "    corr_dim : float\n",
    "        Estimated correlation dimension (placeholder).\n",
    "    \"\"\"\n",
    "\n",
    "    if len(ts.shape) != 1:\n",
    "        raise ValueError(\"Time series ts must be 1D (e.g., x(t) from Lorenz system).\")\n",
    "\n",
    "    # Mean period\n",
    "    if method == \"welch\":\n",
    "        mu = mp_welch(ts)\n",
    "    elif method == \"periodogram\":\n",
    "        mu = mp_periodogram(ts)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}\")\n",
    "\n",
    "    # Time-delay embedding\n",
    "    J = lag\n",
    "    m = emb_dim\n",
    "    N = len(ts)\n",
    "    M = N - (m - 1) * J\n",
    "    X = np.empty((M, m))\n",
    "\n",
    "    for i in range(M):\n",
    "        idx = np.arange(i, i + (m - 1) * J + 1, J)\n",
    "        X[i] = ts[idx]\n",
    "\n",
    "    j = get_nearest_neighbours(X, mu=mu, time_steps=t_f)\n",
    "\n",
    "    mean_log_distance = np.array([\n",
    "        expected_log_distance(i, X, j) for i in range(t_0, t_f)\n",
    "    ])\n",
    "\n",
    "    time = np.arange(t_0, t_f) * delta_t\n",
    "\n",
    "    # Linear fit to <ln divergence>\n",
    "    A = np.vstack([time, np.ones(len(time))]).T\n",
    "    slope, intercept = np.linalg.lstsq(A, mean_log_distance, rcond=None)[0]\n",
    "\n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_mle(pred, true):\n",
    "    le_p = [compute_lyapunov_exponent(ts=pred[:, i]) for i in range(pred.shape[1])]\n",
    "    le_t = [compute_lyapunov_exponent(ts=true[:, i]) for i in range(true.shape[1])]\n",
    "    mle_t = max(le_t)\n",
    "    mle_p = max(le_p)\n",
    "    return mle_p, mle_t, abs(mle_p - mle_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:01:21.989245Z",
     "iopub.status.busy": "2025-06-03T13:01:21.988874Z",
     "iopub.status.idle": "2025-06-03T13:01:21.996516Z",
     "shell.execute_reply": "2025-06-03T13:01:21.995391Z",
     "shell.execute_reply.started": "2025-06-03T13:01:21.989220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lambda_max = {\n",
    "        \"Mackey\": 0.006100,\n",
    "        \"Lorenz\": 0.905600,\n",
    "        \"Rossler\": 0.071400,\n",
    "        \"Chen\": 0.829600,\n",
    "        \"Chua\": 0.428400\n",
    "        }\n",
    "# def compute_lyapunov_exponent(chosen_system, trajectory, ground_truth, dt):\n",
    "#     \"\"\"\n",
    "#     Compute the Lyapunov Exponent for a given trajectory using NeuroKit2.\n",
    "\n",
    "#     Parameters:\n",
    "#         chosen_system (str): Name of the dynamical system (for comparison with true LEs).\n",
    "#         trajectory (np.ndarray): Shape (N,) or (N, 3) trajectory.\n",
    "#         dt (float): Time step used in the simulation.\n",
    "\n",
    "#     Returns:\n",
    "#         lyapunov_exponent (float): Estimated max Lyapunov exponent.\n",
    "#         diff (float): Absolute error from ground truth value.\n",
    "#     \"\"\"\n",
    "#     trajectory = np.asarray(trajectory)\n",
    "\n",
    "#     # Ensure it's 2D for consistency\n",
    "#     if trajectory.ndim == 1:\n",
    "#         trajectory = trajectory.reshape(-1, 1)\n",
    "\n",
    "#     # # Compute delay and dimension from the first component\n",
    "#     # delay, _ = nk.complexity_delay(signal=trajectory[:, 0])\n",
    "#     # dimension, _ = nk.complexity_dimension(signal=trajectory[:, 0], delay=delay)\n",
    "\n",
    "#     # Compute LE for each dimension\n",
    "#     les = []\n",
    "#     for i in range(trajectory.shape[1]):\n",
    "#         le, _ = nk.complexity_lyapunov(signal=trajectory[:, i])\n",
    "#         les.append(le)\n",
    "\n",
    "#     les1 = []\n",
    "#     for i in range(ground_truth.shape[1]):\n",
    "#         le, _ = nk.complexity_lyapunov(signal=ground_truth[:, i])\n",
    "#         les1.append(le)\n",
    "\n",
    "#     lyapunov_exponent = max(les)\n",
    "#     lyapunov_exponent1 = max(les1)\n",
    "#     diff = np.abs(lyapunov_exponent - lyapunov_exponent1)\n",
    "\n",
    "#     return lyapunov_exponent1, lyapunov_exponent, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:01:30.215796Z",
     "iopub.status.busy": "2025-06-03T13:01:30.215469Z",
     "iopub.status.idle": "2025-06-03T13:01:30.232619Z",
     "shell.execute_reply": "2025-06-03T13:01:30.231213Z",
     "shell.execute_reply.started": "2025-06-03T13:01:30.215770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scale_spectral_radius(W, target_radius=0.95):\n",
    "    \"\"\"\n",
    "    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(W)\n",
    "    radius = np.max(np.abs(eigvals))\n",
    "    if radius == 0:\n",
    "        return W\n",
    "    return (W / radius) * target_radius\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"\n",
    "    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n",
    "    We'll use this for both training and prediction.\n",
    "    \"\"\"\n",
    "    x_sq = x**2\n",
    "    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1\n",
    "\n",
    "class CR3D:\n",
    "    \"\"\"\n",
    "    Cycle (ring) reservoir for 3D->3D single-step,\n",
    "    teacher forcing for training, autoregressive for testing.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 reservoir_size=300,\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale = input_scale\n",
    "        self.leaking_rate = leaking_rate\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.zeros((reservoir_size, reservoir_size))\n",
    "        for i in range(reservoir_size):\n",
    "            j = (i+1) % reservoir_size\n",
    "            W[i, j] = 1.0\n",
    "        W = scale_spectral_radius(W, self.spectral_radius)\n",
    "        self.W = W\n",
    "        \n",
    "        np.random.seed(self.seed+1)\n",
    "        self.W_in = (np.random.rand(reservoir_size,3) - 0.5)*2.0*self.input_scale\n",
    "\n",
    "        self.W_out = None\n",
    "        self.x = np.zeros(reservoir_size)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.x = np.zeros(self.reservoir_size)\n",
    "\n",
    "    def _update(self, u):\n",
    "        pre_activation = self.W @ self.x + self.W_in @ u\n",
    "        x_new = np.tanh(pre_activation)\n",
    "        alpha = self.leaking_rate\n",
    "        self.x = (1.0 - alpha)*self.x + alpha*x_new\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        self.reset_state()\n",
    "        states = []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states = np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        targets_use = train_target[discard:]\n",
    "        # X_aug = np.hstack([states_use, np.ones((states_use.shape[0],1))])\n",
    "\n",
    "        # polynomial readout\n",
    "        X_list = []\n",
    "        for s in states_use:\n",
    "            X_list.append(augment_state_with_squares(s))\n",
    "        X_aug = np.array(X_list)  # shape => [T-discard, 2N+1]\n",
    "\n",
    "        reg = Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, targets_use)\n",
    "        self.W_out = reg.coef_\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        preds = []\n",
    "        current_in = np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            # x_aug = np.concatenate([self.x, [1.0]])\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFRRes3D:\n",
    "    \"\"\"\n",
    "    Hierarchical Fractal Reservoir (HFR) for 3D chaotic systems.\n",
    "    \n",
    "    This novel reservoir architecture partitions the chaotic attractor at multiple\n",
    "    hierarchical scales, combining them in a fractal-like adjacency structure.\n",
    "    The method is model-free, relying solely on the observed trajectory in R^3,\n",
    "    and does not require knowledge of any system parameters such as sigma, rho, beta\n",
    "    for Lorenz63. \n",
    "    \n",
    "    Key Idea:\n",
    "     1) Define multiple 'scales' of partition of the data's bounding region.\n",
    "     2) Each scale is subdivided into a certain number of cells (regions).\n",
    "     3) Each cell at level l has links to both:\n",
    "        - other cells at the same level (horizontal adjacency),\n",
    "        - 'child' cells at the finer level l+1 (vertical adjacency).\n",
    "     4) We gather all cells across levels => a multi-level fractal graph => adjacency => W.\n",
    "     5) We build a typical ESN from this adjacency, feed data with W_in, run leaky tanh updates,\n",
    "        then do a polynomial readout for 3D next-step prediction.\n",
    "\n",
    "    This approach is suitable for chaotic systems whose attractors often exhibit fractal\n",
    "    self-similarity, thus capturing multi-scale structures in a single reservoir.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_levels=3,             # number of hierarchical levels\n",
    "                 cells_per_level=None,   # list of number of cells at each level, e.g. [8, 32, 128]\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_levels       : int, number of hierarchical scales\n",
    "        cells_per_level: list[int], the number of partitions/cells at each level\n",
    "                         if None, we auto-generate e.g. 2^(level+2)\n",
    "        spectral_radius: final scaling for adjacency\n",
    "        input_scale    : random input scale W_in\n",
    "        leaking_rate   : ESN leaky alpha\n",
    "        ridge_alpha    : readout ridge penalty\n",
    "        seed           : random seed\n",
    "        \"\"\"\n",
    "        self.n_levels        = n_levels\n",
    "        self.cells_per_level = cells_per_level\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale     = input_scale\n",
    "        self.leaking_rate    = leaking_rate\n",
    "        self.ridge_alpha     = ridge_alpha\n",
    "        self.seed            = seed\n",
    "\n",
    "        if self.cells_per_level is None:\n",
    "            # default scheme e.g. 8, 16, 32 for 3 levels\n",
    "            self.cells_per_level = [8*(2**i) for i in range(n_levels)]\n",
    "\n",
    "        # We'll store adjacency W, input W_in, readout W_out, reservoir state x\n",
    "        self.W     = None\n",
    "        self.W_in  = None\n",
    "        self.W_out = None\n",
    "        self.x     = None\n",
    "        self.n_levels = len(self.cells_per_level)\n",
    "\n",
    "        # We'll define a total number of nodes = sum(cells_per_level)\n",
    "        self.n_nodes = sum(self.cells_per_level)\n",
    "\n",
    "    def _build_partitions(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build hierarchical partitions for each level.\n",
    "        We'll store the bounding box for data_3d, then for each level l in [0..n_levels-1]\n",
    "        run e.g. k-means with K = cells_per_level[l], each point gets a label => we track transitions.\n",
    "\n",
    "        Return: \n",
    "          partitions => list of arrays, partitions[l] => shape (N, ) cluster assignment in [0..cells_per_level[l]-1]\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        N = len(data_3d)\n",
    "        partitions = []\n",
    "\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            # cluster\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.seed+10*level, n_init='auto')\n",
    "            kmeans.fit(data_3d)\n",
    "            labels = kmeans.predict(data_3d)\n",
    "            partitions.append(labels)\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def _build_hierarchical_adjacency(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build a block adjacency with cross-level links, then scale spectral radius.\n",
    "        Steps:\n",
    "          1) Build partitions for each level => partitions[l] in [0..cells_per_level[l]-1]\n",
    "          2) For each level l, build a transition matrix T_l of shape (cells_per_level[l], cells_per_level[l]).\n",
    "          3) Link scale l to scale l+1 by figuring out which cluster i at scale l maps to which cluster j at scale l+1\n",
    "             for each sample t => link i-> j if data_3d[t] is in i at scale l and j at scale l+1.\n",
    "          4) Combine all transitions in one big adjacency W in R^(n_nodes x n_nodes).\n",
    "          5) row-normalize W => scale largest eigenvalue => spectral_radius\n",
    "        \"\"\"\n",
    "        partitions = self._build_partitions(data_3d)\n",
    "        N = len(data_3d)\n",
    "\n",
    "        # offsets for each level => to index big W\n",
    "        offsets = []\n",
    "        running = 0\n",
    "        for level in range(self.n_levels):\n",
    "            offsets.append(running)\n",
    "            running += self.cells_per_level[level]\n",
    "\n",
    "        # total nodes\n",
    "        n_tot = self.n_nodes\n",
    "        # initialize adjacency\n",
    "        A = np.zeros((n_tot, n_tot))\n",
    "\n",
    "        # 1) horizontal adjacency in each level\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            labels = partitions[level]\n",
    "            # T_l => shape (k, k)\n",
    "            T_l = np.zeros((k, k))\n",
    "            for t in range(N-1):\n",
    "                i = labels[t]\n",
    "                j = labels[t+1]\n",
    "                T_l[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = T_l.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            T_l /= row_sum\n",
    "            # place T_l into big A\n",
    "            off = offsets[level]\n",
    "            A[off:off+k, off:off+k] = T_l\n",
    "\n",
    "        # 2) vertical adjacency between scale l and l+1\n",
    "        for level in range(self.n_levels-1):\n",
    "            k_l   = self.cells_per_level[level]\n",
    "            k_lp1 = self.cells_per_level[level+1]\n",
    "            labels_l   = partitions[level]\n",
    "            labels_lp1 = partitions[level+1]\n",
    "            # we define adjacency from i in [0..k_l-1] to j in [0..k_lp1-1] if the same sample t belongs to i at level l and j at l+1\n",
    "            # Count how many times\n",
    "            Xvert1 = np.zeros((k_l, k_lp1))\n",
    "            for t in range(N):\n",
    "                i = labels_l[t]\n",
    "                j = labels_lp1[t]\n",
    "                Xvert1[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = Xvert1.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            Xvert = Xvert1/row_sum\n",
    "            # place in big A\n",
    "            off_l   = offsets[level]\n",
    "            off_lp1 = offsets[level+1]\n",
    "            A[off_l:off_l+k_l, off_lp1:off_lp1+k_lp1] = Xvert\n",
    "            # tentative idea, we could also define adjacency from l+1 -> l (parent link), if desired\n",
    "            # we do the same for the 'child -> parent' link or skip it if we only want forward adjacency\n",
    "            # For now, let's do symmetrical\n",
    "            Yvert = Xvert1.T\n",
    "            col_sum = Yvert.sum(axis=1, keepdims=True)\n",
    "            col_sum[col_sum==0.0] = 1.0\n",
    "            Yvert /= col_sum\n",
    "            A[off_lp1:off_lp1+k_lp1, off_l:off_l+k_l] = Yvert\n",
    "\n",
    "        # now we have a big adjacency => row normalize again, then scale spectral radius\n",
    "        row_sum = A.sum(axis=1, keepdims=True)\n",
    "        row_sum[row_sum==0.0] = 1.0\n",
    "        A /= row_sum\n",
    "\n",
    "        A = scale_spectral_radius(A, self.spectral_radius)\n",
    "        return A\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Main training routine:\n",
    "          1) Build hierarchical adjacency from fractal partition => self.W\n",
    "          2) define W_in => shape(n_nodes, 3)\n",
    "          3) teacher forcing => polynomial readout => solve => self.W_out\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Build adjacency\n",
    "        W_big = self._build_hierarchical_adjacency(train_input)\n",
    "        self.W = W_big\n",
    "\n",
    "        # define W_in => shape(n_nodes,3)\n",
    "        self.n_nodes = W_big.shape[0]\n",
    "        self.W_in = (np.random.rand(self.n_nodes,3)-0.5)*2.0*self.input_scale\n",
    "\n",
    "        # define reservoir state\n",
    "        self.x = np.zeros(self.n_nodes)\n",
    "\n",
    "        # gather states => teacher forcing => polynomial => readout\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]\n",
    "        X_list= []\n",
    "        for s in states_use:\n",
    "            X_list.append( augment_state_with_squares(s) )\n",
    "        X_aug= np.array(X_list)\n",
    "\n",
    "        reg= Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        self.W_out= reg.coef_\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Teacher forcing => feed real 3D => gather states => shape => [T-discard, n_nodes].\n",
    "        returns (states_after_discard, states_discarded).\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states= []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states= np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.x is not None:\n",
    "            self.x.fill(0.0)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        x(t+1)= (1-alpha)x(t)+ alpha tanh( W*x(t)+ W_in*u(t) ).\n",
    "        \"\"\"\n",
    "        alpha= self.leaking_rate\n",
    "        pre_acts= self.W@self.x + self.W_in@u\n",
    "        x_new= np.tanh(pre_acts)\n",
    "        self.x= (1.0- alpha)*self.x+ alpha*x_new\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        fully autonomous => feed last predicted => next input\n",
    "        \"\"\"\n",
    "        preds= []\n",
    "        #self.reset_state()\n",
    "        current_in= np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            big_x= augment_state_with_squares(self.x)\n",
    "            out= self.W_out@big_x\n",
    "            preds.append(out)\n",
    "            current_in = out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:01:32.738360Z",
     "iopub.status.busy": "2025-06-03T13:01:32.737470Z",
     "iopub.status.idle": "2025-06-03T13:01:32.743586Z",
     "shell.execute_reply": "2025-06-03T13:01:32.742526Z",
     "shell.execute_reply.started": "2025-06-03T13:01:32.738323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"cells_per_level\": [[5, 10, 15, 20, 25, 35, 45, 50, 95]],\n",
    "    \"spectral_radius\": [0.92],\n",
    "    \"input_scale\": [1],\n",
    "    \"leaking_rate\": [0.9],\n",
    "    \"ridge_alpha\": [1e-8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:04:57.918107Z",
     "iopub.status.busy": "2025-06-03T13:04:57.917751Z",
     "iopub.status.idle": "2025-06-03T13:04:57.930501Z",
     "shell.execute_reply": "2025-06-03T13:04:57.929653Z",
     "shell.execute_reply.started": "2025-06-03T13:04:57.918084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        ldev_scores = []\n",
    "        l_max = []\n",
    "        diff_list = []\n",
    "        ldev_scores_open_loop = []\n",
    "        diff_list_open_loop = []\n",
    "        l_max_open_loop = []\n",
    "\n",
    "        for initial_state in [[1.0, 1.0, 1.0], [1.0, 2.0, 3.0], [2.0, 1.5, 4.0]]:\n",
    "            tmax = 250\n",
    "            dt = 0.02\n",
    "            t_vals, lorenz_traj = generate_lorenz_data(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "\n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            # scaler = StandardScaler()\n",
    "            scaler.fit(lorenz_traj)\n",
    "            lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "            T_data = len(lorenz_traj)\n",
    "            for train_frac in [0.7, 0.75, 0.8]:\n",
    "                train_end = int(train_frac * (T_data - 1))\n",
    "                train_input = lorenz_traj[:train_end]\n",
    "                train_target = lorenz_traj[1:train_end + 1]\n",
    "                test_input = lorenz_traj[train_end:-1]\n",
    "                test_target = lorenz_traj[train_end + 1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "\n",
    "                for seed in np.arange(1, 6):\n",
    "                    model = model_class(**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "                    preds_open_loop = model.predict_open_loop(test_input)\n",
    "                    l_max_true, l_max_pred, diff = compute_lyapunov_deviation(test_target, preds)\n",
    "                    l_max_true_open_loop, l_max_pred_open_loop, diff_open_loop = compute_lyapunov_deviation(test_target, preds_open_loop)\n",
    "                    ldev_scores.append(l_max_pred)\n",
    "                    l_max.append(l_max_true)\n",
    "                    ldev_scores_open_loop.append(l_max_pred_open_loop)\n",
    "                    diff_list.append(diff)\n",
    "                    l_max_open_loop.append(l_max_true_open_loop)\n",
    "                    diff_list_open_loop.append(diff_open_loop)\n",
    "\n",
    "        mean_ldev = float(np.mean(ldev_scores))\n",
    "        std_ldev = float(np.std(ldev_scores))\n",
    "        mean_diff = float(np.mean(diff_list))\n",
    "        std_diff = float(np.std(diff_list))\n",
    "        mean_ldev_open_loop = float(np.mean(ldev_scores_open_loop))\n",
    "        std_ldev_open_loop = float(np.std(ldev_scores_open_loop))\n",
    "        mean_diff_open_loop = float(np.mean(diff_list_open_loop))\n",
    "        std_diff_open_loop = float(np.std(diff_list_open_loop))\n",
    "\n",
    "        results.append({\n",
    "            \"params\": params,\n",
    "            \"seed_scores_L_max_calculated\": ldev_scores,\n",
    "            \"lambda_max_system\": l_max,\n",
    "            \"diff_from_L_max_true\": diff_list,\n",
    "            \"mean_diff\": mean_diff,\n",
    "            \"std_diff\": std_diff,\n",
    "            \"mean_L_calc\": mean_ldev,\n",
    "            \"std_L_calc\": std_ldev,\n",
    "            \"seed_scores_L_max_calculated_open_loop\": ldev_scores_open_loop,\n",
    "            \"mean_L_calc_open_loop\": mean_ldev_open_loop,\n",
    "            \"std_L_calc_open_loop\": std_ldev_open_loop,\n",
    "            \"seed_scores_diff_from_lambda_max_open_loop\": diff_list_open_loop,\n",
    "            \"mean_diff_open_loop\": mean_diff_open_loop,\n",
    "            \"std_diff_open_loop\": std_diff_open_loop,\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        ldev_scores = []\n",
    "        l_max = []\n",
    "        diff_list = []\n",
    "\n",
    "        for initial_state in [[1.0, 1.0, 1.0], [1.0, 2.0, 3.0], [2.0, 1.5, 4.0]]:\n",
    "            tmax = 250\n",
    "            dt = 0.02\n",
    "            t_vals, lorenz_traj = generate_lorenz_data(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "\n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(lorenz_traj)\n",
    "            lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "            T_data = len(lorenz_traj)\n",
    "            for train_frac in [0.7, 0.75, 0.8]:\n",
    "                train_end = int(train_frac * (T_data - 1))\n",
    "                train_input = lorenz_traj[:train_end]\n",
    "                train_target = lorenz_traj[1:train_end + 1]\n",
    "                test_input = lorenz_traj[train_end:-1]\n",
    "                test_target = lorenz_traj[train_end + 1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "\n",
    "                for seed in np.arange(1, 6):\n",
    "                    model = model_class(**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "                    l_max_true, l_max_pred, diff = compute_lyapunov_deviation(test_target, preds)\n",
    "                    ldev_scores.append(l_max_pred)\n",
    "                    l_max.append(l_max_true)\n",
    "                    diff_list.append(diff)\n",
    "\n",
    "        mean_ldev = float(np.mean(ldev_scores))\n",
    "        std_ldev = float(np.std(ldev_scores))\n",
    "        mean_diff = float(np.mean(diff_list))\n",
    "        std_diff = float(np.std(diff_list))\n",
    "\n",
    "        results.append({\n",
    "            \"params\": params,\n",
    "            \"seed_scores_L_max_calculated\": ldev_scores,\n",
    "            \"lambda_max_system\": l_max,\n",
    "            \"diff_from_L_max_true\": diff_list,\n",
    "            \"mean_diff\": mean_diff,\n",
    "            \"std_diff\": std_diff,\n",
    "            \"mean_L_calc\": mean_ldev,\n",
    "            \"std_L_calc\": std_ldev,\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_case(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    \"\"\"\n",
    "    Runs one train/test split on the Lorenz dataset, trains the given model_class\n",
    "    with the first combination of param_grid, and computes the max-Lyapunov exponents\n",
    "    for the true vs. predicted time series along the second coordinate (index 1).\n",
    "    The result dictionary is written to `output_path` and also returned.\n",
    "    \"\"\"\n",
    "    # 1) Take the first parameter combination\n",
    "    comb = list(itertools.product(*param_grid.values()))[0]\n",
    "    params = dict(zip(param_grid.keys(), comb))\n",
    "    print(f\"\\n== Running single test case for {model_name} with parameters: {params} ==\")\n",
    "\n",
    "    # 2) Fixed initial condition, train/test split, and seed\n",
    "    initial_state = [1.0, 1.0, 1.0]\n",
    "    train_frac = 0.2\n",
    "    seed = 1\n",
    "\n",
    "    # 3) Generate Lorenz data (you already have this function elsewhere)\n",
    "    tmax = 250\n",
    "    dt = 0.02\n",
    "    t_vals, lorenz_traj = generate_lorenz_data(\n",
    "        initial_state=initial_state,\n",
    "        tmax=tmax,\n",
    "        dt=dt\n",
    "    )\n",
    "\n",
    "    # 4) Drop washout, then scale to [0, 1]\n",
    "    washout = 2000\n",
    "    t_vals = t_vals[washout:]\n",
    "    lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(lorenz_traj)\n",
    "    lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "    # 5) Split into train vs. test\n",
    "    T_data = len(lorenz_traj)\n",
    "    train_end = int(train_frac * (T_data - 1))\n",
    "    train_input = lorenz_traj[:train_end]\n",
    "    train_target = lorenz_traj[1 : train_end + 1]\n",
    "    test_input = lorenz_traj[train_end : -1]\n",
    "    test_target = lorenz_traj[train_end + 1 : ]\n",
    "    n_test_steps = len(test_input)\n",
    "    initial_in = test_input[0]\n",
    "\n",
    "    # 6) Instantiate, train, and predict with the model\n",
    "    model = model_class(**params, seed=seed)\n",
    "    model.fit_readout(train_input, train_target, discard=100)\n",
    "    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "\n",
    "    # 7) Compute Lyapunov exponents along the second coordinate (index 1):\n",
    "    #    We pass dt=0.02 to match how the dataset was generated.\n",
    "    l_max_true, l_max_pred, ldev = report_mle(pred=preds, true=test_target)\n",
    "\n",
    "    # 8) Save results\n",
    "    result = {\n",
    "        \"params\": params,\n",
    "        \"lambda_max_system\":    l_max_true,\n",
    "        \"lambda_max_prediction\": l_max_pred,\n",
    "        \"lyapunov_deviation\":   ldev\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump([result], f, indent=2)\n",
    "\n",
    "    print(f\"\\nResult saved to `{output_path}`\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:05:12.996327Z",
     "iopub.status.busy": "2025-06-03T13:05:12.995863Z",
     "iopub.status.idle": "2025-06-03T13:05:14.607442Z",
     "shell.execute_reply": "2025-06-03T13:05:14.606191Z",
     "shell.execute_reply.started": "2025-06-03T13:05:12.996295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Running single test case for HFR with parameters: {'cells_per_level': [5, 10, 15, 20, 25, 35, 45, 50, 95], 'spectral_radius': 0.92, 'input_scale': 1, 'leaking_rate': 0.9, 'ridge_alpha': 1e-08} ==\n"
     ]
    }
   ],
   "source": [
    "run_single_case(HFRRes3D, grid, \"HFR\", output_path=\"hfr_single_case.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
