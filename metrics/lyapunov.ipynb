{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:08.388536Z",
     "iopub.status.busy": "2025-06-03T11:23:08.388230Z",
     "iopub.status.idle": "2025-06-03T11:23:14.170936Z",
     "shell.execute_reply": "2025-06-03T11:23:14.169676Z",
     "shell.execute_reply.started": "2025-06-03T11:23:08.388513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neurokit2\n",
      "  Downloading neurokit2-0.2.11-py2.py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.32.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->neurokit2) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->neurokit2) (2.4.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->neurokit2) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->neurokit2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->neurokit2) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->neurokit2) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->neurokit2) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->neurokit2) (2024.2.0)\n",
      "Downloading neurokit2-0.2.11-py2.py3-none-any.whl (696 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696.5/696.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: neurokit2\n",
      "Successfully installed neurokit2-0.2.11\n"
     ]
    }
   ],
   "source": [
    "!pip install neurokit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:14.173101Z",
     "iopub.status.busy": "2025-06-03T11:23:14.172860Z",
     "iopub.status.idle": "2025-06-03T11:23:16.242189Z",
     "shell.execute_reply": "2025-06-03T11:23:16.241079Z",
     "shell.execute_reply.started": "2025-06-03T11:23:14.173077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "import json\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:16.243516Z",
     "iopub.status.busy": "2025-06-03T11:23:16.243144Z",
     "iopub.status.idle": "2025-06-03T11:23:16.249089Z",
     "shell.execute_reply": "2025-06-03T11:23:16.248354Z",
     "shell.execute_reply.started": "2025-06-03T11:23:16.243497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lorenz_deriv(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    x, y, z = state\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x*(rho - z) - y\n",
    "    dzdt = x*y - beta*z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz_data(\n",
    "    initial_state=[1.0, 1.0, 1.0],\n",
    "    tmax=25.0,\n",
    "    dt=0.01,\n",
    "    sigma=10.0,\n",
    "    rho=28.0,\n",
    "    beta=8.0/3.0\n",
    "):\n",
    "    num_steps = int(tmax / dt) + 1 # +1 to include t=0\n",
    "    t_vals = np.linspace(0, tmax, num_steps)\n",
    "    sol = odeint(lorenz_deriv, initial_state, t_vals, args=(sigma, rho, beta))\n",
    "    return t_vals, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:16.250878Z",
     "iopub.status.busy": "2025-06-03T11:23:16.250645Z",
     "iopub.status.idle": "2025-06-03T11:23:16.278305Z",
     "shell.execute_reply": "2025-06-03T11:23:16.277391Z",
     "shell.execute_reply.started": "2025-06-03T11:23:16.250859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_valid_prediction_time(y_true, y_pred, t_vals, threshold, lambda_max, dt):\n",
    "    \"\"\"\n",
    "    Compute the Valid Prediction Time (VPT) and compare it to Lyapunov time T_lambda = 1 / lambda_max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (N, dim)\n",
    "        True trajectory over time.\n",
    "    y_pred : ndarray of shape (N, dim)\n",
    "        Model's predicted trajectory over time (closed-loop).\n",
    "    t_vals : ndarray of shape (N,)\n",
    "        Time values corresponding to the trajectory steps.\n",
    "    threshold : float, optional\n",
    "        The error threshold, default is 0.4 as in your snippet.\n",
    "    lambda_max : float, optional\n",
    "        Largest Lyapunov exponent. Default=0.9 for Lorenz.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    T_VPT : float\n",
    "        Valid prediction time. The earliest time at which normalized error surpasses threshold\n",
    "        (or the last time if never surpassed).\n",
    "    T_lambda : float\n",
    "        Lyapunov time = 1 / lambda_max\n",
    "    ratio : float\n",
    "        How many Lyapunov times the model prediction remains valid, i.e. T_VPT / T_lambda.\n",
    "    \"\"\"\n",
    "    # 1) Average of y_true\n",
    "    y_mean = np.mean(y_true, axis=0)  # shape (dim,)\n",
    "    \n",
    "    # 2) Time-averaged norm^2 of (y_true - y_mean)\n",
    "    y_centered = y_true - y_mean\n",
    "    denom = np.mean(np.sum(y_centered**2, axis=1))  # scalar\n",
    "    \n",
    "    # 3) Compute the normalized error delta_gamma(t) = ||y_true - y_pred||^2 / denom\n",
    "    diff = y_true - y_pred\n",
    "    err_sq = np.sum(diff**2, axis=1)  # shape (N,)\n",
    "    delta_gamma = err_sq / denom      # shape (N,)\n",
    "    \n",
    "    # 4) Find the first time index where delta_gamma(t) exceeds threshold\n",
    "    idx_exceed = np.where(delta_gamma > threshold)[0]\n",
    "    if len(idx_exceed) == 0:\n",
    "        # never exceeds threshold => set T_VPT to the final time\n",
    "        T_VPT = t_vals[-1]\n",
    "    else:\n",
    "        T_VPT = t_vals[idx_exceed[0]]\n",
    "    \n",
    "    # 5) Compute T_lambda and ratio\n",
    "    T_lambda = 1.0 / lambda_max\n",
    "\n",
    "    # print(f\"\\n--- Valid Prediction Time (VPT) with threshold={threshold}, lambda_max={lambda_max} ---\")\n",
    "\n",
    "    T_VPT = (T_VPT - t_vals[0])  # Adjust T_VPT to be relative to the start time\n",
    "    ratio = T_VPT / T_lambda\n",
    "\n",
    "    return T_VPT, T_lambda, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:16.279431Z",
     "iopub.status.busy": "2025-06-03T11:23:16.279163Z",
     "iopub.status.idle": "2025-06-03T11:23:16.307994Z",
     "shell.execute_reply": "2025-06-03T11:23:16.306964Z",
     "shell.execute_reply.started": "2025-06-03T11:23:16.279408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_nrmse(all_preds, test_target, horizons):\n",
    "    \"\"\"\n",
    "    Evaluate model performance over multiple prediction horizons\n",
    "    for teacher-forced single-step forecasting or autoregressive rollout.\n",
    "    \"\"\"\n",
    "    horizon_nrmse = {}\n",
    "    for horizon in horizons:\n",
    "        preds = all_preds[:horizon]\n",
    "        targets = test_target[:horizon]\n",
    "        squared_errors = (preds - targets) ** 2\n",
    "        variance = np.var(targets, axis=0)\n",
    "        variance[variance == 0] = 1e-8  # avoid divide-by-zero\n",
    "        nrmse = np.sqrt(np.sum(squared_errors) / (horizon * np.sum(variance)))\n",
    "        horizon_nrmse[horizon] = nrmse\n",
    "    return horizon_nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:16.309733Z",
     "iopub.status.busy": "2025-06-03T11:23:16.309370Z",
     "iopub.status.idle": "2025-06-03T11:23:16.334814Z",
     "shell.execute_reply": "2025-06-03T11:23:16.334075Z",
     "shell.execute_reply.started": "2025-06-03T11:23:16.309703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_attractor_deviation(predictions, targets, cube_size=(0.1, 0.1, 0.1)):\n",
    "    \"\"\"\n",
    "    Compute the Attractor Deviation (ADev) metric.\n",
    "\n",
    "    Parameters:\n",
    "        predictions (numpy.ndarray): Predicted trajectories of shape (n, 3).\n",
    "        targets (numpy.ndarray): True trajectories of shape (n, 3).\n",
    "        cube_size (tuple): Dimensions of the cube (dx, dy, dz).\n",
    "\n",
    "    Returns:\n",
    "        float: The ADev metric.\n",
    "    \"\"\"\n",
    "    # Define the cube grid based on the range of the data and cube size\n",
    "    min_coords = np.min(np.vstack((predictions, targets)), axis=0)\n",
    "    max_coords = np.max(np.vstack((predictions, targets)), axis=0)\n",
    "\n",
    "    # Create a grid of cubes\n",
    "    grid_shape = ((max_coords - min_coords) / cube_size).astype(int) + 1\n",
    "\n",
    "    # Initialize the cube occupancy arrays\n",
    "    pred_cubes = np.zeros(grid_shape, dtype=int)\n",
    "    target_cubes = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Map trajectories to cubes\n",
    "    pred_indices = ((predictions - min_coords) / cube_size).astype(int)\n",
    "    target_indices = ((targets - min_coords) / cube_size).astype(int)\n",
    "\n",
    "    # Mark cubes visited by predictions and targets\n",
    "    for idx in pred_indices:\n",
    "        pred_cubes[tuple(idx)] = 1\n",
    "    for idx in target_indices:\n",
    "        target_cubes[tuple(idx)] = 1\n",
    "\n",
    "    # Compute the ADev metric\n",
    "    adev = np.sum(np.abs(pred_cubes - target_cubes))\n",
    "    return adev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:16.830076Z",
     "iopub.status.busy": "2025-06-03T11:23:16.829773Z",
     "iopub.status.idle": "2025-06-03T11:23:16.835124Z",
     "shell.execute_reply": "2025-06-03T11:23:16.833825Z",
     "shell.execute_reply.started": "2025-06-03T11:23:16.830056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lambda_max = {\n",
    "    \"lorenz\": 0.9056  # Example ground truth LE for the Lorenz system\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T12:03:27.653573Z",
     "iopub.status.busy": "2025-06-03T12:03:27.653269Z",
     "iopub.status.idle": "2025-06-03T12:03:27.660489Z",
     "shell.execute_reply": "2025-06-03T12:03:27.659701Z",
     "shell.execute_reply.started": "2025-06-03T12:03:27.653554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "lambda_max = {\n",
    "        \"Mackey\": 0.006100,\n",
    "        \"Lorenz\": 0.905600,\n",
    "        \"Rossler\": 0.071400,\n",
    "        \"Chen\": 0.829600,\n",
    "        \"Chua\": 0.428400\n",
    "        }\n",
    "def compute_lyapunov_exponent(chosen_system, trajectory, dt):\n",
    "    \"\"\"\n",
    "    Compute the Lyapunov Exponent for a given trajectory using NeuroKit2.\n",
    "\n",
    "    Parameters:\n",
    "        chosen_system (str): Name of the dynamical system (for comparison with true LEs).\n",
    "        trajectory (np.ndarray): Shape (N,) or (N, 3) trajectory.\n",
    "        dt (float): Time step used in the simulation.\n",
    "\n",
    "    Returns:\n",
    "        lyapunov_exponent (float): Estimated max Lyapunov exponent.\n",
    "        diff (float): Absolute error from ground truth value.\n",
    "    \"\"\"\n",
    "    trajectory = np.asarray(trajectory)\n",
    "\n",
    "    # Ensure it's 2D for consistency\n",
    "    if trajectory.ndim == 1:\n",
    "        trajectory = trajectory.reshape(-1, 1)\n",
    "\n",
    "    # # Compute delay and dimension from the first component\n",
    "    # delay, _ = nk.complexity_delay(signal=trajectory[:, 0])\n",
    "    # dimension, _ = nk.complexity_dimension(signal=trajectory[:, 0], delay=delay)\n",
    "\n",
    "    # Compute LE for each dimension\n",
    "    les = []\n",
    "    for i in range(trajectory.shape[1]):\n",
    "        le, _ = nk.complexity_lyapunov(signal=trajectory[:, i])\n",
    "        les.append(le)\n",
    "\n",
    "    lyapunov_exponent = max(les)\n",
    "    diff = np.abs(lyapunov_exponent - lambda_max[chosen_system])\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T11:23:24.125023Z",
     "iopub.status.busy": "2025-06-03T11:23:24.124757Z",
     "iopub.status.idle": "2025-06-03T11:23:24.249883Z",
     "shell.execute_reply": "2025-06-03T11:23:24.248491Z",
     "shell.execute_reply.started": "2025-06-03T11:23:24.125005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scale_spectral_radius(W, target_radius=0.95):\n",
    "    \"\"\"\n",
    "    Scales a matrix W so that its largest eigenvalue magnitude = target_radius.\n",
    "    \"\"\"\n",
    "    eigvals = np.linalg.eigvals(W)\n",
    "    radius = np.max(np.abs(eigvals))\n",
    "    if radius == 0:\n",
    "        return W\n",
    "    return (W / radius) * target_radius\n",
    "\n",
    "def augment_state_with_squares(x):\n",
    "    \"\"\"\n",
    "    Given state vector x in R^N, return [ x, x^2, 1 ] in R^(2N+1).\n",
    "    We'll use this for both training and prediction.\n",
    "    \"\"\"\n",
    "    x_sq = x**2\n",
    "    return np.concatenate([x, x_sq, [1.0]])  # shape: 2N+1\n",
    "\n",
    "class HFRRes3D:\n",
    "    \"\"\"\n",
    "    Hierarchical Fractal Reservoir (HFR) for 3D chaotic systems.\n",
    "    \n",
    "    This novel reservoir architecture partitions the chaotic attractor at multiple\n",
    "    hierarchical scales, combining them in a fractal-like adjacency structure.\n",
    "    The method is model-free, relying solely on the observed trajectory in R^3,\n",
    "    and does not require knowledge of any system parameters such as sigma, rho, beta\n",
    "    for Lorenz63. \n",
    "    \n",
    "    Key Idea:\n",
    "     1) Define multiple 'scales' of partition of the data's bounding region.\n",
    "     2) Each scale is subdivided into a certain number of cells (regions).\n",
    "     3) Each cell at level l has links to both:\n",
    "        - other cells at the same level (horizontal adjacency),\n",
    "        - 'child' cells at the finer level l+1 (vertical adjacency).\n",
    "     4) We gather all cells across levels => a multi-level fractal graph => adjacency => W.\n",
    "     5) We build a typical ESN from this adjacency, feed data with W_in, run leaky tanh updates,\n",
    "        then do a polynomial readout for 3D next-step prediction.\n",
    "\n",
    "    This approach is suitable for chaotic systems whose attractors often exhibit fractal\n",
    "    self-similarity, thus capturing multi-scale structures in a single reservoir.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_levels=3,             # number of hierarchical levels\n",
    "                 cells_per_level=None,   # list of number of cells at each level, e.g. [8, 32, 128]\n",
    "                 spectral_radius=0.95,\n",
    "                 input_scale=1.0,\n",
    "                 leaking_rate=1.0,\n",
    "                 ridge_alpha=1e-6,\n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_levels       : int, number of hierarchical scales\n",
    "        cells_per_level: list[int], the number of partitions/cells at each level\n",
    "                         if None, we auto-generate e.g. 2^(level+2)\n",
    "        spectral_radius: final scaling for adjacency\n",
    "        input_scale    : random input scale W_in\n",
    "        leaking_rate   : ESN leaky alpha\n",
    "        ridge_alpha    : readout ridge penalty\n",
    "        seed           : random seed\n",
    "        \"\"\"\n",
    "        self.n_levels        = n_levels\n",
    "        self.cells_per_level = cells_per_level\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.input_scale     = input_scale\n",
    "        self.leaking_rate    = leaking_rate\n",
    "        self.ridge_alpha     = ridge_alpha\n",
    "        self.seed            = seed\n",
    "\n",
    "        if self.cells_per_level is None:\n",
    "            # default scheme e.g. 8, 16, 32 for 3 levels\n",
    "            self.cells_per_level = [8*(2**i) for i in range(n_levels)]\n",
    "\n",
    "        # We'll store adjacency W, input W_in, readout W_out, reservoir state x\n",
    "        self.W     = None\n",
    "        self.W_in  = None\n",
    "        self.W_out = None\n",
    "        self.x     = None\n",
    "        self.n_levels = len(self.cells_per_level)\n",
    "\n",
    "        # We'll define a total number of nodes = sum(cells_per_level)\n",
    "        self.n_nodes = sum(self.cells_per_level)\n",
    "\n",
    "    def _build_partitions(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build hierarchical partitions for each level.\n",
    "        We'll store the bounding box for data_3d, then for each level l in [0..n_levels-1]\n",
    "        run e.g. k-means with K = cells_per_level[l], each point gets a label => we track transitions.\n",
    "\n",
    "        Return: \n",
    "          partitions => list of arrays, partitions[l] => shape (N, ) cluster assignment in [0..cells_per_level[l]-1]\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        N = len(data_3d)\n",
    "        partitions = []\n",
    "\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            # cluster\n",
    "            kmeans = KMeans(n_clusters=k, random_state=self.seed+10*level, n_init='auto')\n",
    "            kmeans.fit(data_3d)\n",
    "            labels = kmeans.predict(data_3d)\n",
    "            partitions.append(labels)\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def _build_hierarchical_adjacency(self, data_3d):\n",
    "        \"\"\"\n",
    "        Build a block adjacency with cross-level links, then scale spectral radius.\n",
    "        Steps:\n",
    "          1) Build partitions for each level => partitions[l] in [0..cells_per_level[l]-1]\n",
    "          2) For each level l, build a transition matrix T_l of shape (cells_per_level[l], cells_per_level[l]).\n",
    "          3) Link scale l to scale l+1 by figuring out which cluster i at scale l maps to which cluster j at scale l+1\n",
    "             for each sample t => link i-> j if data_3d[t] is in i at scale l and j at scale l+1.\n",
    "          4) Combine all transitions in one big adjacency W in R^(n_nodes x n_nodes).\n",
    "          5) row-normalize W => scale largest eigenvalue => spectral_radius\n",
    "        \"\"\"\n",
    "        partitions = self._build_partitions(data_3d)\n",
    "        N = len(data_3d)\n",
    "\n",
    "        # offsets for each level => to index big W\n",
    "        offsets = []\n",
    "        running = 0\n",
    "        for level in range(self.n_levels):\n",
    "            offsets.append(running)\n",
    "            running += self.cells_per_level[level]\n",
    "\n",
    "        # total nodes\n",
    "        n_tot = self.n_nodes\n",
    "        # initialize adjacency\n",
    "        A = np.zeros((n_tot, n_tot))\n",
    "\n",
    "        # 1) horizontal adjacency in each level\n",
    "        for level in range(self.n_levels):\n",
    "            k = self.cells_per_level[level]\n",
    "            labels = partitions[level]\n",
    "            # T_l => shape (k, k)\n",
    "            T_l = np.zeros((k, k))\n",
    "            for t in range(N-1):\n",
    "                i = labels[t]\n",
    "                j = labels[t+1]\n",
    "                T_l[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = T_l.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            T_l /= row_sum\n",
    "            # place T_l into big A\n",
    "            off = offsets[level]\n",
    "            A[off:off+k, off:off+k] = T_l\n",
    "\n",
    "        # 2) vertical adjacency between scale l and l+1\n",
    "        for level in range(self.n_levels-1):\n",
    "            k_l   = self.cells_per_level[level]\n",
    "            k_lp1 = self.cells_per_level[level+1]\n",
    "            labels_l   = partitions[level]\n",
    "            labels_lp1 = partitions[level+1]\n",
    "            # we define adjacency from i in [0..k_l-1] to j in [0..k_lp1-1] if the same sample t belongs to i at level l and j at l+1\n",
    "            # Count how many times\n",
    "            Xvert1 = np.zeros((k_l, k_lp1))\n",
    "            for t in range(N):\n",
    "                i = labels_l[t]\n",
    "                j = labels_lp1[t]\n",
    "                Xvert1[i,j]+=1\n",
    "            # row normalize\n",
    "            row_sum = Xvert1.sum(axis=1, keepdims=True)\n",
    "            row_sum[row_sum==0.0] = 1.0\n",
    "            Xvert = Xvert1/row_sum\n",
    "            # place in big A\n",
    "            off_l   = offsets[level]\n",
    "            off_lp1 = offsets[level+1]\n",
    "            A[off_l:off_l+k_l, off_lp1:off_lp1+k_lp1] = Xvert\n",
    "            # tentative idea, we could also define adjacency from l+1 -> l (parent link), if desired\n",
    "            # we do the same for the 'child -> parent' link or skip it if we only want forward adjacency\n",
    "            # For now, let's do symmetrical\n",
    "            Yvert = Xvert1.T\n",
    "            col_sum = Yvert.sum(axis=1, keepdims=True)\n",
    "            col_sum[col_sum==0.0] = 1.0\n",
    "            Yvert /= col_sum\n",
    "            A[off_lp1:off_lp1+k_lp1, off_l:off_l+k_l] = Yvert\n",
    "\n",
    "        # now we have a big adjacency => row normalize again, then scale spectral radius\n",
    "        row_sum = A.sum(axis=1, keepdims=True)\n",
    "        row_sum[row_sum==0.0] = 1.0\n",
    "        A /= row_sum\n",
    "\n",
    "        A = scale_spectral_radius(A, self.spectral_radius)\n",
    "        return A\n",
    "\n",
    "    def fit_readout(self, train_input, train_target, discard=100):\n",
    "        \"\"\"\n",
    "        Main training routine:\n",
    "          1) Build hierarchical adjacency from fractal partition => self.W\n",
    "          2) define W_in => shape(n_nodes, 3)\n",
    "          3) teacher forcing => polynomial readout => solve => self.W_out\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        # Build adjacency\n",
    "        W_big = self._build_hierarchical_adjacency(train_input)\n",
    "        self.W = W_big\n",
    "\n",
    "        # define W_in => shape(n_nodes,3)\n",
    "        self.n_nodes = W_big.shape[0]\n",
    "        self.W_in = (np.random.rand(self.n_nodes,3)-0.5)*2.0*self.input_scale\n",
    "\n",
    "        # define reservoir state\n",
    "        self.x = np.zeros(self.n_nodes)\n",
    "\n",
    "        # gather states => teacher forcing => polynomial => readout\n",
    "        states_use, _ = self.collect_states(train_input, discard=discard)\n",
    "        target_use = train_target[discard:]\n",
    "        X_list= []\n",
    "        for s in states_use:\n",
    "            X_list.append( augment_state_with_squares(s) )\n",
    "        X_aug= np.array(X_list)\n",
    "\n",
    "        reg= Ridge(alpha=self.ridge_alpha, fit_intercept=False)\n",
    "        reg.fit(X_aug, target_use)\n",
    "        self.W_out= reg.coef_\n",
    "\n",
    "    def collect_states(self, inputs, discard=100):\n",
    "        \"\"\"\n",
    "        Teacher forcing => feed real 3D => gather states => shape => [T-discard, n_nodes].\n",
    "        returns (states_after_discard, states_discarded).\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        states= []\n",
    "        for val in inputs:\n",
    "            self._update(val)\n",
    "            states.append(self.x.copy())\n",
    "        states= np.array(states)\n",
    "        return states[discard:], states[:discard]\n",
    "\n",
    "    def reset_state(self):\n",
    "        if self.x is not None:\n",
    "            self.x.fill(0.0)\n",
    "\n",
    "    def _update(self, u):\n",
    "        \"\"\"\n",
    "        x(t+1)= (1-alpha)x(t)+ alpha tanh( W*x(t)+ W_in*u(t) ).\n",
    "        \"\"\"\n",
    "        alpha= self.leaking_rate\n",
    "        pre_acts= self.W@self.x + self.W_in@u\n",
    "        x_new= np.tanh(pre_acts)\n",
    "        self.x= (1.0- alpha)*self.x+ alpha*x_new\n",
    "\n",
    "    def predict_autoregressive(self, initial_input, n_steps):\n",
    "        \"\"\"\n",
    "        fully autonomous => feed last predicted => next input\n",
    "        \"\"\"\n",
    "        preds= []\n",
    "        #self.reset_state()\n",
    "        current_in= np.array(initial_input)\n",
    "        for _ in range(n_steps):\n",
    "            self._update(current_in)\n",
    "            big_x= augment_state_with_squares(self.x)\n",
    "            out= self.W_out@big_x\n",
    "            preds.append(out)\n",
    "            current_in= out\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_open_loop(self, test_input):\n",
    "        preds = []\n",
    "        for true_input in test_input:\n",
    "            self._update(true_input)\n",
    "            x_aug = augment_state_with_squares(self.x)\n",
    "            out = self.W_out @ x_aug\n",
    "            preds.append(out)\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T12:01:30.451794Z",
     "iopub.status.busy": "2025-06-03T12:01:30.451542Z",
     "iopub.status.idle": "2025-06-03T12:01:30.457771Z",
     "shell.execute_reply": "2025-06-03T12:01:30.456644Z",
     "shell.execute_reply.started": "2025-06-03T12:01:30.451775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"cells_per_level\": [[5, 15, 20, 30, 45, 50, 55, 80]],\n",
    "    \"spectral_radius\": [0.92],\n",
    "    \"input_scale\": [1],\n",
    "    \"leaking_rate\": [0.9],\n",
    "    \"ridge_alpha\": [1e-8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T12:01:30.960221Z",
     "iopub.status.busy": "2025-06-03T12:01:30.959925Z",
     "iopub.status.idle": "2025-06-03T12:01:30.971430Z",
     "shell.execute_reply": "2025-06-03T12:01:30.970590Z",
     "shell.execute_reply.started": "2025-06-03T12:01:30.960201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    # horizons = [200, 400, 600, 800, 1000]\n",
    "\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores_vpt = []\n",
    "        # horizon_nrmse_all = {h: [] for h in horizons}\n",
    "        adev_scores = []\n",
    "        # ldev_scores = []\n",
    "\n",
    "        for initial_state in [[1.0, 1.0, 1.0], [1.0, 2.0, 3.0], [2.0, 1.5, 4.0]]:\n",
    "            tmax = 250\n",
    "            dt = 0.02\n",
    "            t_vals, lorenz_traj = generate_lorenz_data(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "\n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            lorenz_traj = lorenz_traj[washout:]\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(lorenz_traj)\n",
    "            lorenz_traj = scaler.transform(lorenz_traj)\n",
    "\n",
    "            T_data = len(lorenz_traj)\n",
    "            for train_frac in [0.7, 0.75, 0.8]:\n",
    "                train_end = int(train_frac * (T_data - 1))\n",
    "                train_input = lorenz_traj[:train_end]\n",
    "                train_target = lorenz_traj[1:train_end + 1]\n",
    "                test_input = lorenz_traj[train_end:-1]\n",
    "                test_target = lorenz_traj[train_end + 1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "\n",
    "                for seed in np.arange(1, 6):\n",
    "                    model = model_class(**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "\n",
    "                    T_VPT_s, _, _ = compute_valid_prediction_time(test_target, preds, t_vals, 0.4, 0.9, dt)\n",
    "                    seed_scores_vpt.append(T_VPT_s)\n",
    "\n",
    "                    # horizon_nrmse = evaluate_nrmse(preds, test_target, horizons)\n",
    "                    # for h in horizons:\n",
    "                    #     horizon_nrmse_all[h].append(horizon_nrmse[h])\n",
    "\n",
    "                    adev = compute_attractor_deviation(preds, test_target)\n",
    "                    adev_scores.append(adev)\n",
    "\n",
    "                    # ldev = compute_lyapunov_exponent(\"Lorenz\", preds, dt)\n",
    "                    # ldev_scores.append(ldev)\n",
    "\n",
    "        mean_vpt = float(np.mean(seed_scores_vpt))\n",
    "        std_vpt = float(np.std(seed_scores_vpt))\n",
    "        # mean_nrmse_dict = {str(h): float(np.mean(horizon_nrmse_all[h])) for h in horizons}\n",
    "        # std_nrmse_dict  = {str(h): float(np.std(horizon_nrmse_all[h]))  for h in horizons}\n",
    "        mean_adev = float(np.mean(adev_scores))\n",
    "        std_adev = float(np.std(adev_scores))\n",
    "        # mean_ldev = float(np.mean(ldev_scores))\n",
    "        # std_ldev = float(np.std(ldev_scores))\n",
    "\n",
    "        results.append({\n",
    "            \"params\": params,\n",
    "            \"seed_scores_T_VPT\": seed_scores_vpt,\n",
    "            \"mean_T_VPT\": mean_vpt,\n",
    "            \"std_T_VPT\": std_vpt,\n",
    "            # \"mean_NRMSEs\": mean_nrmse_dict,\n",
    "            # \"std_NRMSEs\": std_nrmse_dict,\n",
    "            \"mean_ADev\": mean_adev,\n",
    "            \"std_ADev\": std_adev,\n",
    "            # \"mean_LDev\": mean_ldev,\n",
    "            # \"std_LDev\": std_ldev\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to `{output_path}`\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T12:03:34.799593Z",
     "iopub.status.busy": "2025-06-03T12:03:34.799240Z",
     "iopub.status.idle": "2025-06-03T12:03:34.811727Z",
     "shell.execute_reply": "2025-06-03T12:03:34.809827Z",
     "shell.execute_reply.started": "2025-06-03T12:03:34.799571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_grid_search(model_class, param_grid, model_name,\n",
    "                    output_path=\"grid_search_results.json\"):\n",
    "    # Precompute param combinations\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    param_keys = list(param_grid.keys())\n",
    "    print(f\"\\n== Initial grid search for {model_name} with {len(combos)} combinations ==\")\n",
    "\n",
    "    results = []\n",
    "    # tqdm adds a progress bar for better visualization\n",
    "    for comb in tqdm(combos, desc=\"Grid Search\"):\n",
    "        params = dict(zip(param_keys, comb))\n",
    "        seed_scores = []\n",
    "        adevs=[]\n",
    "        lyups=[]\n",
    "        \n",
    "        # Run all 20 seeds\n",
    "        for initial_state in [[1.0,1.0,1.0],[1.0,2.0,3.0],[2.0,1.5,4.0]]:\n",
    "            tmax = 250\n",
    "            dt   = 0.02\n",
    "            t_vals, lorenz_traj = generate_lorenz_data(\n",
    "                initial_state=initial_state,\n",
    "                tmax=tmax,\n",
    "                dt=dt\n",
    "            )\n",
    "            \n",
    "            washout = 2000\n",
    "            t_vals = t_vals[washout:]\n",
    "            lorenz_traj = lorenz_traj[washout:]\n",
    "            \n",
    "            # normalize\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(lorenz_traj)\n",
    "            lorenz_traj = scaler.transform(lorenz_traj)\n",
    "            \n",
    "            T_data = len(lorenz_traj)\n",
    "            for train_frac in [0.7,0.75,0.8]:\n",
    "                train_end = int(train_frac*(T_data-1))\n",
    "                train_input  = lorenz_traj[:train_end]\n",
    "                train_target = lorenz_traj[1:train_end+1]\n",
    "                test_input   = lorenz_traj[train_end:-1]\n",
    "                test_target  = lorenz_traj[train_end+1:]\n",
    "                n_test_steps = len(test_input)\n",
    "                initial_in = test_input[0]\n",
    "                for seed in np.arange(1,6):\n",
    "                    model = model_class(**params, seed=seed)\n",
    "                    model.fit_readout(train_input, train_target, discard=100)\n",
    "                    preds = model.predict_autoregressive(initial_in, n_test_steps)\n",
    "                    T_VPT_s, _, _ = compute_valid_prediction_time(test_target,preds,t_vals,0.4,0.9,dt)\n",
    "                    seed_scores.append(T_VPT_s)\n",
    "                    adev= compute_attractor_deviation(preds, test_target, (0.1,0.1,0.1))\n",
    "                    adevs.append(adev)\n",
    "                    lyup = compute_lyapunov_exponent(\"Lorenz\", preds, dt)\n",
    "                    lyups.append(lyup)\n",
    "        mean_score = float(np.mean(seed_scores))\n",
    "        std_dev    = float(np.std(seed_scores))\n",
    "        is_stable  = std_dev < 1.5\n",
    "        status     = \"Stable\" if is_stable else \"Unstable\"\n",
    "        mean_adev= float(np.mean(adevs))\n",
    "        mean_lyup=float(np.mean(lyups))\n",
    "        std_dev_adev = float(np.std(adevs))\n",
    "        std_dev_lyup = float(np.std(lyups))\n",
    "        \n",
    "        # print(f\"Params: {params} → Avg T_VPT={mean_score:.3f}, \"\n",
    "        #       f\"Std Dev={std_dev:.3f} → {status}\")\n",
    "\n",
    "        print(f\"Mean VPT: {mean_score} Std Dev VPT: {std_dev}\" \n",
    "        f\"Adev: {mean_adev} Std Dev Adev: {std_dev_adev}\" \n",
    "        f\"Lyupanov: {mean_lyup} Std Dev Lyupanov: {std_dev_lyup}\")\n",
    "\n",
    "        results.append({\n",
    "            \"params\":      params,\n",
    "            \"seed_scores\": seed_scores,\n",
    "            \"mean_T_VPT\":  mean_score,\n",
    "            \"std_dev\":     std_dev,\n",
    "            \"LDev\": mean_lyup + std_dev_lyup\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nAll results saved to {output_path}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T12:03:35.340520Z",
     "iopub.status.busy": "2025-06-03T12:03:35.340200Z",
     "iopub.status.idle": "2025-06-03T12:04:26.743983Z",
     "shell.execute_reply": "2025-06-03T12:04:26.743274Z",
     "shell.execute_reply.started": "2025-06-03T12:03:35.340499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Initial grid search for HFR with 1 combinations ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search: 100%|██████████| 1/1 [00:51<00:00, 51.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean VPT: 12.831555555555555 Std Dev VPT: 1.6508825324057634Adev: 26.844444444444445 Std Dev Adev: 11.191178183542718Lyupanov: 0.8010989481322953 Std Dev Lyupanov: 0.005840453690493894\n",
      "\n",
      "All results saved to hfr_grid_search_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'params': {'cells_per_level': [5, 15, 20, 30, 45, 50, 55, 80],\n",
       "   'spectral_radius': 0.92,\n",
       "   'input_scale': 1,\n",
       "   'leaking_rate': 0.9,\n",
       "   'ridge_alpha': 1e-08},\n",
       "  'seed_scores': [12.759999999999998,\n",
       "   14.880000000000003,\n",
       "   12.660000000000004,\n",
       "   12.64,\n",
       "   15.660000000000004,\n",
       "   11.259999999999998,\n",
       "   12.800000000000004,\n",
       "   11.14,\n",
       "   12.800000000000004,\n",
       "   14.32,\n",
       "   11.420000000000002,\n",
       "   13.0,\n",
       "   13.740000000000002,\n",
       "   12.899999999999999,\n",
       "   12.800000000000004,\n",
       "   11.480000000000004,\n",
       "   13.780000000000001,\n",
       "   11.520000000000003,\n",
       "   16.08,\n",
       "   11.480000000000004,\n",
       "   12.280000000000001,\n",
       "   12.340000000000003,\n",
       "   13.82,\n",
       "   14.0,\n",
       "   12.240000000000002,\n",
       "   12.160000000000004,\n",
       "   12.259999999999998,\n",
       "   12.18,\n",
       "   16.92,\n",
       "   12.18,\n",
       "   13.259999999999998,\n",
       "   13.020000000000003,\n",
       "   13.04,\n",
       "   13.200000000000003,\n",
       "   18.08,\n",
       "   12.219999999999999,\n",
       "   12.120000000000005,\n",
       "   12.14,\n",
       "   12.020000000000003,\n",
       "   15.600000000000001,\n",
       "   10.560000000000002,\n",
       "   10.660000000000004,\n",
       "   10.64,\n",
       "   10.740000000000002,\n",
       "   10.620000000000005],\n",
       "  'mean_T_VPT': 12.831555555555555,\n",
       "  'std_dev': 1.6508825324057634,\n",
       "  'LDev': 0.8069394018227892}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_grid_search(HFRRes3D, grid, \"HFR\", output_path=\"hfr_grid_search_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
